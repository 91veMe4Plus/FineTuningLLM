{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bbf116",
   "metadata": {},
   "source": [
    "# HyperCLOVAX 모델 배치 크기(Batch Size) 비교 분석\n",
    "\n",
    "이 노트북은 원본 모델과 서로 다른 배치 크기로 미세조정된 HyperCLOVAX 모델의 성능을 비교합니다.\n",
    "\n",
    "## 분석 목표\n",
    "- 세 모델 간 정량적 성능 비교 (BLEU, ROUGE, 문자 정확도)\n",
    "- 정성적 분석 (실제 출력 예시 비교)\n",
    "- 카테고리별 성능 분석\n",
    "- 추론 시간 및 효율성 비교\n",
    "- 배치 크기 조정 효과 분석\n",
    "- 결과 시각화\n",
    "\n",
    "## 모델 정보\n",
    "- **원본 모델**: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- **배치 크기 1 모델**: `hyperclova-deobfuscation-lora-with-1-batch-size`\n",
    "- **배치 크기 2 모델**: `hyperclova-deobfuscation-lora-with-2-batch-size`\n",
    "- **배치 크기 4 모델**: `hyperclova-deobfuscation-lora-with-4-batch-size`\n",
    "- **테스트 데이터**: `testdata.csv` (1,002 샘플)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb29ec6",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"패키지 설치 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib 및 seaborn 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 이미지 저장 폴더 생성\n",
    "image_save_dir = '/content/drive/MyDrive/Colab Notebooks/analysis_images'\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5b03f",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 연결 (Colab에서 실행 시)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    \n",
    "    # 기존 마운트 포인트가 있으면 정리\n",
    "    mount_point = '/content/drive'\n",
    "    if os.path.exists(mount_point):\n",
    "        try:\n",
    "            # 마운트 해제 시도\n",
    "            print(\"기존 마운트 포인트 정리 중...\")\n",
    "            os.system(f'fusermount -u {mount_point} 2>/dev/null || true')\n",
    "            shutil.rmtree(mount_point, ignore_errors=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Google Drive 마운트\n",
    "    drive.mount(mount_point, force_remount=True)\n",
    "    \n",
    "    # 경로 설정\n",
    "    BASE_PATH = '/content/drive/MyDrive/'\n",
    "    MODEL_BATCH_1_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-1-batch-size'\n",
    "    MODEL_BATCH_2_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-2-batch-size'\n",
    "    MODEL_BATCH_4_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-4-batch-size'\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv'\n",
    "    \n",
    "    # Google Drive 루트에 전용 분석 결과 폴더 생성\n",
    "    analysis_root_dir = os.path.join(BASE_PATH, 'HyperCLOVAX_BatchSize_Analysis_Results')\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # 로컬 실행 시\n",
    "    BASE_PATH = './'\n",
    "    MODEL_BATCH_1_PATH = './hyperclova-deobfuscation-lora-with-1-batch-size'\n",
    "    MODEL_BATCH_2_PATH = './hyperclova-deobfuscation-lora-with-2-batch-size'\n",
    "    MODEL_BATCH_4_PATH = './hyperclova-deobfuscation-lora-with-4-batch-size'\n",
    "    TEST_DATA_PATH = './testdata.csv'\n",
    "    \n",
    "    # 로컬용 분석 결과 폴더\n",
    "    analysis_root_dir = './HyperCLOVAX_BatchSize_Analysis_Results'\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "\n",
    "# 이미지 저장 폴더 생성 (분석 결과 폴더 내에)\n",
    "image_save_dir = os.path.join(analysis_root_dir, 'visualization_images')\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "print(f\"\\n경로 설정 완료:\")\n",
    "print(f\"배치 크기 1 모델 경로: {MODEL_BATCH_1_PATH}\")\n",
    "print(f\"배치 크기 2 모델 경로: {MODEL_BATCH_2_PATH}\")\n",
    "print(f\"배치 크기 4 모델 경로: {MODEL_BATCH_4_PATH}\")\n",
    "print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")\n",
    "print(f\"분석 결과 저장 경로: {analysis_root_dir}\")\n",
    "print(f\"이미지 저장 경로: {image_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"테스트 데이터 크기: {len(test_df)} 샘플\")\n",
    "print(f\"컬럼 목록: {test_df.columns.tolist()}\")\n",
    "print(\"\\n첫 5개 샘플:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 데이터 통계\n",
    "print(\"\\n데이터 통계:\")\n",
    "print(f\"- 총 샘플 수: {len(test_df)}\")\n",
    "print(f\"- 원본 텍스트 평균 길이: {test_df['original'].str.len().mean():.1f}\")\n",
    "print(f\"- 난독화 텍스트 평균 길이: {test_df['obfuscated'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c660f8",
   "metadata": {},
   "source": [
    "## 3. 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 이름 설정\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# 토크나이저 로드 (배치 크기 1 모델에서)\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_BATCH_1_PATH)\n",
    "print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83913712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name):\n",
    "    \"\"\"로라 모델을 로드합니다\"\"\"\n",
    "    print(f\"\\n{model_name} 모델 로딩 중...\")\n",
    "    \n",
    "    # 베이스 모델 로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 어댑터 적용\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"{model_name} 모델 로딩 완료\")\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"원본 베이스 모델을 로드합니다\"\"\"\n",
    "    print(\"\\n원본 모델 로딩 중...\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    print(\"원본 모델 로딩 완료\")\n",
    "    return base_model\n",
    "\n",
    "# 모델 로드\n",
    "base_model = load_base_model()\n",
    "model_batch_1 = load_model(MODEL_BATCH_1_PATH, \"배치 크기 1 모델\")\n",
    "model_batch_2 = load_model(MODEL_BATCH_2_PATH, \"배치 크기 2 모델\")\n",
    "model_batch_4 = load_model(MODEL_BATCH_4_PATH, \"배치 크기 4 모델\")\n",
    "\n",
    "print(\"모든 모델 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb373",
   "metadata": {},
   "source": [
    "## 4. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, obfuscated_text, max_length=256):\n",
    "    \"\"\"난독화된 텍스트를 입력받아 원본 텍스트 생성\"\"\"\n",
    "    prompt = f\"\"\"### 지시사항:\n",
    "다음 난독화된 한국어 텍스트를 원래 텍스트로 복원해주세요.\n",
    "\n",
    "난독화된 텍스트: {obfuscated_text}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 응답 부분만 추출\n",
    "    if \"### 응답:\" in response:\n",
    "        response = response.split(\"### 응답:\")[1].strip()\n",
    "        # 불필요한 부분 제거\n",
    "        if \"<|endoftext|>\" in response:\n",
    "            response = response.split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "print(\"추론 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaac22",
   "metadata": {},
   "source": [
    "## 5. 성능 평가 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 메트릭 로드\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def calculate_character_accuracy(pred, ref):\n",
    "    \"\"\"문자 단위 정확도 계산\"\"\"\n",
    "    if len(ref) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    \n",
    "    # 정확히 일치하는 문자 수 계산\n",
    "    matches = sum(1 for i, char in enumerate(pred) if i < len(ref) and char == ref[i])\n",
    "    return matches / len(ref)\n",
    "\n",
    "def calculate_exact_match(pred, ref):\n",
    "    \"\"\"완전 일치 여부\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"모든 메트릭 계산\"\"\"\n",
    "    # BLEU 계산\n",
    "    try:\n",
    "        bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    except:\n",
    "        rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    \n",
    "    # 문자 정확도 계산\n",
    "    char_accuracies = [calculate_character_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    avg_char_accuracy = np.mean(char_accuracies)\n",
    "    \n",
    "    # 완전 일치율 계산\n",
    "    exact_matches = [calculate_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    exact_match_rate = np.mean(exact_matches)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'char_accuracy': avg_char_accuracy,\n",
    "        'exact_match': exact_match_rate,\n",
    "        'char_accuracies': char_accuracies,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "print(\"평가 메트릭 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3322",
   "metadata": {},
   "source": [
    "## 6. 모델 성능 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_df, sample_size=None):\n",
    "    \"\"\"모델 성능 평가\"\"\"\n",
    "    if sample_size:\n",
    "        test_data = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        test_data = test_df.copy()\n",
    "    \n",
    "    print(f\"\\n{model_name} 평가 시작 ({len(test_data)}개 샘플)\")\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"{model_name} 평가\"):\n",
    "        obfuscated = row['obfuscated']\n",
    "        pred, inf_time = generate_deobfuscated_text(model, obfuscated)\n",
    "        predictions.append(pred)\n",
    "        inference_times.append(inf_time)\n",
    "    \n",
    "    # 참조 텍스트\n",
    "    references = test_data['original'].tolist()\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # 추론 시간 통계\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    total_inference_time = np.sum(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'inference_times': inference_times,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'total_inference_time': total_inference_time,\n",
    "        'test_data': test_data,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} 평가 완료\")\n",
    "    print(f\"평균 추론 시간: {avg_inference_time:.3f}초\")\n",
    "    print(f\"총 추론 시간: {total_inference_time:.1f}초\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 평가 실행 (전체 데이터셋 또는 샘플)\n",
    "SAMPLE_SIZE = 200  # 전체 평가를 원하면 None으로 설정\n",
    "\n",
    "print(\"모델 성능 평가를 시작합니다...\")\n",
    "results_base = evaluate_model(base_model, \"원본 모델\", test_df, SAMPLE_SIZE)\n",
    "results_batch_1 = evaluate_model(model_batch_1, \"배치 크기 1 모델\", test_df, SAMPLE_SIZE)\n",
    "results_batch_2 = evaluate_model(model_batch_2, \"배치 크기 2 모델\", test_df, SAMPLE_SIZE)\n",
    "results_batch_4 = evaluate_model(model_batch_4, \"배치 크기 4 모델\", test_df, SAMPLE_SIZE)\n",
    "\n",
    "print(\"\\n모든 모델 평가 완료!\")\n",
    "\n",
    "## 13. 결과 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18542a41",
   "metadata": {},
   "source": [
    "## 7. 성능 비교 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 표 생성\n",
    "comparison_df = pd.DataFrame({\n",
    "    '메트릭': ['BLEU 점수', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
    "               '문자 정확도', '정확 일치율', '평균 추론 시간(초)'],\n",
    "    '원본 모델': [\n",
    "        f\"{results_base['bleu']:.4f}\",\n",
    "        f\"{results_base['rouge1']:.4f}\",\n",
    "        f\"{results_base['rouge2']:.4f}\",\n",
    "        f\"{results_base['rougeL']:.4f}\",\n",
    "        f\"{results_base['char_accuracy']:.4f}\",\n",
    "        f\"{results_base['exact_match']:.4f}\",\n",
    "        f\"{results_base['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '배치 크기 1 모델': [\n",
    "        f\"{results_batch_1['bleu']:.4f}\",\n",
    "        f\"{results_batch_1['rouge1']:.4f}\",\n",
    "        f\"{results_batch_1['rouge2']:.4f}\",\n",
    "        f\"{results_batch_1['rougeL']:.4f}\",\n",
    "        f\"{results_batch_1['char_accuracy']:.4f}\",\n",
    "        f\"{results_batch_1['exact_match']:.4f}\",\n",
    "        f\"{results_batch_1['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '배치 크기 2 모델': [\n",
    "        f\"{results_batch_2['bleu']:.4f}\",\n",
    "        f\"{results_batch_2['rouge1']:.4f}\",\n",
    "        f\"{results_batch_2['rouge2']:.4f}\",\n",
    "        f\"{results_batch_2['rougeL']:.4f}\",\n",
    "        f\"{results_batch_2['char_accuracy']:.4f}\",\n",
    "        f\"{results_batch_2['exact_match']:.4f}\",\n",
    "        f\"{results_batch_2['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '배치 크기 4 모델': [\n",
    "        f\"{results_batch_4['bleu']:.4f}\",\n",
    "        f\"{results_batch_4['rouge1']:.4f}\",\n",
    "        f\"{results_batch_4['rouge2']:.4f}\",\n",
    "        f\"{results_batch_4['rougeL']:.4f}\",\n",
    "        f\"{results_batch_4['char_accuracy']:.4f}\",\n",
    "        f\"{results_batch_4['exact_match']:.4f}\",\n",
    "        f\"{results_batch_4['avg_inference_time']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== 모델 성능 비교 결과 ===\\n(원본 vs 배치 크기 1 vs 2 vs 4)\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 원본 모델 대비 미세조정 모델 성능 개선률 계산\n",
    "print(\"\\n=== 원본 모델 대비 미세조정 모델 성능 개선율 ===\")\n",
    "metrics_to_compare = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy', 'exact_match']\n",
    "print(f\"{'Metric':<15} {'배치 1 vs 원본':<20} {'배치 2 vs 원본':<20} {'배치 4 vs 원본':<20}\")\n",
    "print(\"-\" * 80)\n",
    "for metric in metrics_to_compare:\n",
    "    improvement_batch_1 = ((results_batch_1[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    improvement_batch_2 = ((results_batch_2[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    improvement_batch_4 = ((results_batch_4[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    print(f\"{metric.upper():<15} {improvement_batch_1:+7.2f}%          {improvement_batch_2:+7.2f}%          {improvement_batch_4:+7.2f}%\")\n",
    "\n",
    "# 배치 크기별 모델 간 비교\n",
    "print(\"\\n=== 배치 크기별 모델 간 성능 비교 ===\")\n",
    "print(f\"{'Metric':<15} {'배치 2 vs 1':<20} {'배치 4 vs 1':<20} {'배치 4 vs 2':<20}\")\n",
    "print(\"-\" * 80)\n",
    "for metric in metrics_to_compare:\n",
    "    improvement_2_vs_1 = ((results_batch_2[metric] - results_batch_1[metric]) / results_batch_1[metric]) * 100 if results_batch_1[metric] > 0 else 0\n",
    "    improvement_4_vs_1 = ((results_batch_4[metric] - results_batch_1[metric]) / results_batch_1[metric]) * 100 if results_batch_1[metric] > 0 else 0\n",
    "    improvement_4_vs_2 = ((results_batch_4[metric] - results_batch_2[metric]) / results_batch_2[metric]) * 100 if results_batch_2[metric] > 0 else 0\n",
    "    print(f\"{metric.upper():<15} {improvement_2_vs_1:+7.2f}%          {improvement_4_vs_1:+7.2f}%          {improvement_4_vs_2:+7.2f}%\")\n",
    "\n",
    "# 추론 시간 비율 비교\n",
    "time_ratio_base_batch1 = results_batch_1['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_base_batch2 = results_batch_2['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_base_batch4 = results_batch_4['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_batch2_batch1 = results_batch_2['avg_inference_time'] / results_batch_1['avg_inference_time']\n",
    "time_ratio_batch4_batch1 = results_batch_4['avg_inference_time'] / results_batch_1['avg_inference_time']\n",
    "time_ratio_batch4_batch2 = results_batch_4['avg_inference_time'] / results_batch_2['avg_inference_time']\n",
    "\n",
    "print(f\"\\n=== 추론 시간 비율 비교 ===\")\n",
    "print(f\"추론 시간 비율 (배치 1/원본): {time_ratio_base_batch1:.2f}x\")\n",
    "print(f\"추론 시간 비율 (배치 2/원본): {time_ratio_base_batch2:.2f}x\")\n",
    "print(f\"추론 시간 비율 (배치 4/원본): {time_ratio_base_batch4:.2f}x\")\n",
    "print(f\"추론 시간 비율 (배치 2/배치 1): {time_ratio_batch2_batch1:.2f}x\")\n",
    "print(f\"추론 시간 비율 (배치 4/배치 1): {time_ratio_batch4_batch1:.2f}x\")\n",
    "print(f\"추론 시간 비율 (배치 4/배치 2): {time_ratio_batch4_batch2:.2f}x\")\n",
    "\n",
    "# 전반적인 성능 요약\n",
    "batch_accuracies = [results_batch_1['char_accuracy'], results_batch_2['char_accuracy'], results_batch_4['char_accuracy']]\n",
    "best_batch_accuracy = max(batch_accuracies)\n",
    "best_batch_idx = batch_accuracies.index(best_batch_accuracy)\n",
    "batch_names = [\"1\", \"2\", \"4\"]\n",
    "best_batch_name = batch_names[best_batch_idx]\n",
    "accuracy_improvement = ((best_batch_accuracy - results_base['char_accuracy']) / results_base['char_accuracy'] * 100)\n",
    "\n",
    "print(f\"\\n=== 전반적 성능 요약 ===\")\n",
    "print(f\"가장 우수한 모델: 배치 크기 {best_batch_name} 모델\")\n",
    "print(f\"원본 모델 대비 최대 성능 향상: {accuracy_improvement:.2f}%\")\n",
    "print(f\"배치 크기 조정 효과: {'significant' if accuracy_improvement > 10 else 'moderate' if accuracy_improvement > 5 else 'limited'}\")\n",
    "\n",
    "# 배치 크기별 성능 트렌드 분석\n",
    "print(f\"\\n=== 배치 크기별 성능 트렌드 ===\")\n",
    "for metric in ['char_accuracy', 'bleu', 'rouge1']:\n",
    "    values = [results_batch_1[metric], results_batch_2[metric], results_batch_4[metric]]\n",
    "    if values[1] > values[0] and values[2] > values[1]:\n",
    "        trend = \"지속적 향상\"\n",
    "    elif values[1] > values[0] or values[2] > values[1]:\n",
    "        trend = \"부분적 향상\"\n",
    "    else:\n",
    "        trend = \"불규칙적\"\n",
    "    print(f\"{metric.upper()} 트렌드: {trend}\")\n",
    "    print(f\"  배치 1: {values[0]:.4f}, 배치 2: {values[1]:.4f}, 배치 4: {values[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540b5d9",
   "metadata": {},
   "source": [
    "## 8. 배치 크기별 성능 시각화 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Batch Size Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Analysis (Base vs Fine-tuned with Different Batch Sizes)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_data = {\n",
    "    'BLEU': [results_base['bleu'], results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_batch_1['rouge1'], results_batch_2['rouge1'], results_batch_4['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_batch_1['rouge2'], results_batch_2['rouge2'], results_batch_4['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_batch_1['rougeL'], results_batch_2['rougeL'], results_batch_4['rougeL']],\n",
    "    'Character Accuracy': [results_base['char_accuracy'], results_batch_1['char_accuracy'], results_batch_2['char_accuracy'], results_batch_4['char_accuracy']],\n",
    "    'Exact Match': [results_base['exact_match'], results_batch_1['exact_match'], results_batch_2['exact_match'], results_batch_4['exact_match']]\n",
    "}\n",
    "\n",
    "models = ['Base Model', 'Batch Size 1', 'Batch Size 2', 'Batch Size 4']\n",
    "colors = ['lightgray', 'skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "for idx, (metric, values) in enumerate(metrics_data.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    bars = axes[row, col].bar(models, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{metric}', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Score')\n",
    "    axes[row, col].set_ylim(0, max(values) * 1.1)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Display values\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '01_batch_size_performance_comparison.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Character Accuracy Distribution Comparison for Batch Sizes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Character Accuracy Distribution by Batch Size', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 원본 모델\n",
    "axes[0, 0].hist(results_base['char_accuracies'], bins=20, alpha=0.7, color='lightgray', edgecolor='black')\n",
    "axes[0, 0].set_title('Base Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Character Accuracy')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(results_base['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_base[\"char_accuracy\"]:.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 배치 크기 1 모델\n",
    "axes[0, 1].hist(results_batch_1['char_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Batch Size 1 Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Character Accuracy')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(results_batch_1['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_batch_1[\"char_accuracy\"]:.3f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 배치 크기 2 모델\n",
    "axes[1, 0].hist(results_batch_2['char_accuracies'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Batch Size 2 Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Character Accuracy')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(results_batch_2['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_batch_2[\"char_accuracy\"]:.3f}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 배치 크기 4 모델\n",
    "axes[1, 1].hist(results_batch_4['char_accuracies'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1, 1].set_title('Batch Size 4 Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Character Accuracy')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(results_batch_4['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_batch_4[\"char_accuracy\"]:.3f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '02_character_accuracy_distribution.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference Time Comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 평균 추론 시간 비교\n",
    "inference_times = [results_base['avg_inference_time'], \n",
    "                  results_batch_1['avg_inference_time'],\n",
    "                  results_batch_2['avg_inference_time'], \n",
    "                  results_batch_4['avg_inference_time']]\n",
    "\n",
    "bars1 = ax1.bar(models, inference_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Average Inference Time Comparison', fontweight='bold')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars1, inference_times):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(inference_times)*0.01,\n",
    "             f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 추론 시간 비율 (원본 모델 대비)\n",
    "time_ratios = [1.0,  # Base model ratio\n",
    "               results_batch_1['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "               results_batch_2['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "               results_batch_4['avg_inference_time'] / results_base['avg_inference_time']]\n",
    "\n",
    "bars2 = ax2.bar(models, time_ratios, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Inference Time Ratio (vs Base Model)', fontweight='bold')\n",
    "ax2.set_ylabel('Ratio')\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Base Model')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "for bar, ratio in zip(bars2, time_ratios):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(time_ratios)*0.01,\n",
    "             f'{ratio:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "img_path = os.path.join(image_save_dir, '03_inference_time_comparison.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Performance by Text Length Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Performance Analysis by Text Length', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 텍스트 길이 계산\n",
    "text_lengths = [len(text) for text in results_base['references']]\n",
    "\n",
    "# 길이 범위별 그룹화 (4개 그룹)\n",
    "length_percentiles = np.percentile(text_lengths, [25, 50, 75])\n",
    "length_groups = []\n",
    "group_labels = ['Short (0-25%)', 'Medium-Short (25-50%)', 'Medium-Long (50-75%)', 'Long (75-100%)']\n",
    "\n",
    "for i, length in enumerate(text_lengths):\n",
    "    if length <= length_percentiles[0]:\n",
    "        length_groups.append(0)\n",
    "    elif length <= length_percentiles[1]:\n",
    "        length_groups.append(1)\n",
    "    elif length <= length_percentiles[2]:\n",
    "        length_groups.append(2)\n",
    "    else:\n",
    "        length_groups.append(3)\n",
    "\n",
    "# 각 모델의 길이별 성능 계산\n",
    "models_results = [results_base, results_batch_1, results_batch_2, results_batch_4]\n",
    "model_names_short = ['Base', 'Batch 1', 'Batch 2', 'Batch 4']\n",
    "\n",
    "for idx, (model_result, model_name) in enumerate(zip(models_results, model_names_short)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    group_accuracies = []\n",
    "    for group in range(4):\n",
    "        group_indices = [i for i, g in enumerate(length_groups) if g == group]\n",
    "        if group_indices:\n",
    "            group_acc = np.mean([model_result['char_accuracies'][i] for i in group_indices])\n",
    "            group_accuracies.append(group_acc)\n",
    "        else:\n",
    "            group_accuracies.append(0)\n",
    "    \n",
    "    bars = axes[row, col].bar(group_labels, group_accuracies, \n",
    "                             color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{model_name} Model - Performance by Text Length', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Character Accuracy')\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].set_ylim(0, max(group_accuracies) * 1.1 if max(group_accuracies) > 0 else 1)\n",
    "    \n",
    "    # 값 표시\n",
    "    for bar, acc in zip(bars, group_accuracies):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(group_accuracies)*0.01,\n",
    "                           f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "img_path = os.path.join(image_save_dir, '04_performance_by_text_length.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inference Time Comparison by Batch Size\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Inference time distribution\n",
    "inference_data = [results_base['inference_times'], results_batch_1['inference_times'], \n",
    "                 results_batch_2['inference_times'], results_batch_4['inference_times']]\n",
    "labels = ['Base Model', 'Batch Size 1', 'Batch Size 2', 'Batch Size 4']\n",
    "\n",
    "axes[0].boxplot(inference_data, labels=labels, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[0].set_title('Inference Time Distribution Comparison by Batch Size', fontweight='bold')\n",
    "axes[0].set_ylabel('Inference Time (seconds)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average inference time bar chart\n",
    "avg_times = [results_base['avg_inference_time'], results_batch_1['avg_inference_time'], \n",
    "            results_batch_2['avg_inference_time'], results_batch_4['avg_inference_time']]\n",
    "colors = ['lightgray', 'skyblue', 'lightgreen', 'lightcoral']\n",
    "bars = axes[1].bar(labels, avg_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Inference Time Comparison by Batch Size', fontweight='bold')\n",
    "axes[1].set_ylabel('Average Inference Time (seconds)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, avg_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '05_batch_size_inference_time_comparison.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c6546",
   "metadata": {},
   "source": [
    "## 9. 질적 분석 - 예시 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 성능 차이가 큰 샘플들 찾기\n",
    "def find_performance_difference_samples(results_base, results_batch_1, results_batch_2, results_batch_4, n_samples=5):\n",
    "    \"\"\"네 모델 간 성능 차이가 큰 샘플들 찾기\"\"\"\n",
    "    char_acc_base = np.array(results_base['char_accuracies'])\n",
    "    char_acc_1 = np.array(results_batch_1['char_accuracies'])\n",
    "    char_acc_2 = np.array(results_batch_2['char_accuracies'])\n",
    "    char_acc_4 = np.array(results_batch_4['char_accuracies'])\n",
    "    \n",
    "    # 미세조정 효과 (최고 성능 모델 vs 원본)\n",
    "    max_finetuned = np.maximum(np.maximum(char_acc_1, char_acc_2), char_acc_4)\n",
    "    finetuning_improvement = max_finetuned - char_acc_base\n",
    "    \n",
    "    # 배치 크기 4 vs 1 비교\n",
    "    diff_4_vs_1 = char_acc_4 - char_acc_1\n",
    "    \n",
    "    # 배치 크기 2 vs 1 비교\n",
    "    diff_2_vs_1 = char_acc_2 - char_acc_1\n",
    "    \n",
    "    # 가장 미세조정 효과가 큰 인덱스들\n",
    "    best_finetuning_idx = np.argsort(finetuning_improvement)[-n_samples:][::-1]\n",
    "    worst_finetuning_idx = np.argsort(finetuning_improvement)[:n_samples]\n",
    "    \n",
    "    # 배치 4가 배치 1보다 훨씬 좋은/나쁜 경우\n",
    "    best_4_vs_1_idx = np.argsort(diff_4_vs_1)[-n_samples:][::-1]\n",
    "    worst_4_vs_1_idx = np.argsort(diff_4_vs_1)[:n_samples]\n",
    "    \n",
    "    return best_finetuning_idx, worst_finetuning_idx, best_4_vs_1_idx, worst_4_vs_1_idx\n",
    "\n",
    "best_ft_idx, worst_ft_idx, best_4_idx, worst_4_idx = find_performance_difference_samples(\n",
    "    results_base, results_batch_1, results_batch_2, results_batch_4)\n",
    "\n",
    "print(\"=== 미세조정이 원본 모델 대비 가장 효과적이었던 예시 ===\")\n",
    "for i, idx in enumerate(best_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"배치 1 모델 예측: {results_batch_1['predictions'][idx]}\")\n",
    "    print(f\"배치 2 모델 예측: {results_batch_2['predictions'][idx]}\")\n",
    "    print(f\"배치 4 모델 예측: {results_batch_4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 배치 1: {results_batch_1['char_accuracies'][idx]:.3f}, 배치 2: {results_batch_2['char_accuracies'][idx]:.3f}, 배치 4: {results_batch_4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 배치 4 모델이 배치 1 모델 대비 크게 우수했던 예시 ===\")\n",
    "for i, idx in enumerate(best_4_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_batch_1['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_batch_1['references'][idx]}\")\n",
    "    print(f\"배치 1 모델 예측: {results_batch_1['predictions'][idx]}\")\n",
    "    print(f\"배치 2 모델 예측: {results_batch_2['predictions'][idx]}\")\n",
    "    print(f\"배치 4 모델 예측: {results_batch_4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 배치 1: {results_batch_1['char_accuracies'][idx]:.3f}, 배치 2: {results_batch_2['char_accuracies'][idx]:.3f}, 배치 4: {results_batch_4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 미세조정 효과가 제한적이었던 예시 ===\")\n",
    "for i, idx in enumerate(worst_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"배치 1 모델 예측: {results_batch_1['predictions'][idx]}\")\n",
    "    print(f\"배치 2 모델 예측: {results_batch_2['predictions'][idx]}\")\n",
    "    print(f\"배치 4 모델 예측: {results_batch_4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 배치 1: {results_batch_1['char_accuracies'][idx]:.3f}, 배치 2: {results_batch_2['char_accuracies'][idx]:.3f}, 배치 4: {results_batch_4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "# 전반적인 성능 비교 통계\n",
    "print(\"\\n=== 전반적인 성능 비교 통계 ===\")\n",
    "char_acc_base = np.array(results_base['char_accuracies'])\n",
    "char_acc_1 = np.array(results_batch_1['char_accuracies'])\n",
    "char_acc_2 = np.array(results_batch_2['char_accuracies'])\n",
    "char_acc_4 = np.array(results_batch_4['char_accuracies'])\n",
    "\n",
    "# 원본 대비 미세조정 모델 성능\n",
    "better_1_vs_base = np.sum(char_acc_1 > char_acc_base)\n",
    "better_2_vs_base = np.sum(char_acc_2 > char_acc_base)\n",
    "better_4_vs_base = np.sum(char_acc_4 > char_acc_base)\n",
    "\n",
    "# 배치 크기 간 비교\n",
    "better_2_vs_1 = np.sum(char_acc_2 > char_acc_1)\n",
    "better_4_vs_1 = np.sum(char_acc_4 > char_acc_1)\n",
    "better_4_vs_2 = np.sum(char_acc_4 > char_acc_2)\n",
    "\n",
    "print(f\"배치 1 모델 > 원본 모델: {better_1_vs_base}개 ({better_1_vs_base/len(char_acc_base)*100:.1f}%)\")\n",
    "print(f\"배치 2 모델 > 원본 모델: {better_2_vs_base}개 ({better_2_vs_base/len(char_acc_base)*100:.1f}%)\")\n",
    "print(f\"배치 4 모델 > 원본 모델: {better_4_vs_base}개 ({better_4_vs_base/len(char_acc_base)*100:.1f}%)\")\n",
    "print(f\"배치 2 모델 > 배치 1 모델: {better_2_vs_1}개 ({better_2_vs_1/len(char_acc_1)*100:.1f}%)\")\n",
    "print(f\"배치 4 모델 > 배치 1 모델: {better_4_vs_1}개 ({better_4_vs_1/len(char_acc_1)*100:.1f}%)\")\n",
    "print(f\"배치 4 모델 > 배치 2 모델: {better_4_vs_2}개 ({better_4_vs_2/len(char_acc_2)*100:.1f}%)\")\n",
    "\n",
    "avg_improvement_1 = np.mean(char_acc_1 - char_acc_base)\n",
    "avg_improvement_2 = np.mean(char_acc_2 - char_acc_base)\n",
    "avg_improvement_4 = np.mean(char_acc_4 - char_acc_base)\n",
    "print(f\"\\n배치 1 모델의 원본 대비 평균 성능 개선: {avg_improvement_1:.4f} ({avg_improvement_1*100:.2f}%p)\")\n",
    "print(f\"배치 2 모델의 원본 대비 평균 성능 개선: {avg_improvement_2:.4f} ({avg_improvement_2*100:.2f}%p)\")\n",
    "print(f\"배치 4 모델의 원본 대비 평균 성능 개선: {avg_improvement_4:.4f} ({avg_improvement_4*100:.2f}%p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91017b0",
   "metadata": {},
   "source": [
    "## 10. 상세 분석 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이별 성능 분석\n",
    "def analyze_by_text_length(results, model_name):\n",
    "    \"\"\"텍스트 길이별 성능 분석\"\"\"\n",
    "    test_data = results['test_data']\n",
    "    char_accuracies = results['char_accuracies']\n",
    "    \n",
    "    # 텍스트 길이 계산\n",
    "    text_lengths = test_data['original'].str.len()\n",
    "    \n",
    "    # 길이 구간별로 분류\n",
    "    length_bins = [0, 20, 50, 100, 200, float('inf')]\n",
    "    length_labels = ['≤20 chars', '21-50 chars', '51-100 chars', '101-200 chars', '200+ chars']\n",
    "    \n",
    "    length_categories = pd.cut(text_lengths, bins=length_bins, labels=length_labels, right=False)\n",
    "    \n",
    "    # 구간별 평균 성능\n",
    "    performance_by_length = []\n",
    "    for category in length_labels:\n",
    "        mask = length_categories == category\n",
    "        if mask.sum() > 0:\n",
    "            avg_acc = np.mean(np.array(char_accuracies)[mask])\n",
    "            count = mask.sum()\n",
    "            performance_by_length.append({\n",
    "                'length_category': category,\n",
    "                'avg_char_accuracy': avg_acc,\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(performance_by_length)\n",
    "\n",
    "# 네 모델의 길이별 성능 분석\n",
    "length_analysis_base = analyze_by_text_length(results_base, \"원본 모델\")\n",
    "length_analysis_batch_1 = analyze_by_text_length(results_batch_1, \"배치 크기 1 모델\")\n",
    "length_analysis_batch_2 = analyze_by_text_length(results_batch_2, \"배치 크기 2 모델\")\n",
    "length_analysis_batch_4 = analyze_by_text_length(results_batch_4, \"배치 크기 4 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(length_analysis_base))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, length_analysis_base['avg_char_accuracy'], width, \n",
    "               label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x - 0.5*width, length_analysis_batch_1['avg_char_accuracy'], width,\n",
    "               label='Batch Size 1 Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + 0.5*width, length_analysis_batch_2['avg_char_accuracy'], width,\n",
    "               label='Batch Size 2 Model', color='lightgreen', alpha=0.7)\n",
    "bars4 = ax.bar(x + 1.5*width, length_analysis_batch_4['avg_char_accuracy'], width,\n",
    "               label='Batch Size 4 Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Text Length Category')\n",
    "ax.set_ylabel('Average Character Accuracy')\n",
    "ax.set_title('Model Performance Comparison by Text Length (Base vs Fine-tuned with Different Batch Sizes)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(length_analysis_base['length_category'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '04_performance_by_text_length.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 텍스트 길이별 성능 분석 결과 ===\")\n",
    "combined_length_analysis = pd.merge(\n",
    "    pd.merge(\n",
    "        pd.merge(length_analysis_base, length_analysis_batch_1, on='length_category', suffixes=('_base', '_batch1')),\n",
    "        length_analysis_batch_2, on='length_category'\n",
    "    ),\n",
    "    length_analysis_batch_4, on='length_category', suffixes=('_batch2', '_batch4')\n",
    ")\n",
    "combined_length_analysis.columns = ['length_category', 'avg_char_accuracy_base', 'count_base', \n",
    "                                   'avg_char_accuracy_batch1', 'count_batch1', \n",
    "                                   'avg_char_accuracy_batch2', 'count_batch2',\n",
    "                                   'avg_char_accuracy_batch4', 'count_batch4']\n",
    "print(combined_length_analysis[['length_category', 'avg_char_accuracy_base', 'avg_char_accuracy_batch1', 'avg_char_accuracy_batch2', 'avg_char_accuracy_batch4']])\n",
    "\n",
    "# 길이별 미세조정 효과 분석\n",
    "print(\"\\n=== 길이별 미세조정 효과 ===\")\n",
    "for _, row in combined_length_analysis.iterrows():\n",
    "    category = row['length_category']\n",
    "    base_acc = row['avg_char_accuracy_base']\n",
    "    acc_batch1 = row['avg_char_accuracy_batch1']\n",
    "    acc_batch2 = row['avg_char_accuracy_batch2']\n",
    "    acc_batch4 = row['avg_char_accuracy_batch4']\n",
    "    \n",
    "    improvement_batch1 = ((acc_batch1 - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    improvement_batch2 = ((acc_batch2 - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    improvement_batch4 = ((acc_batch4 - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    \n",
    "    print(f\"{category}: 배치1 개선 {improvement_batch1:+.1f}%, 배치2 개선 {improvement_batch2:+.1f}%, 배치4 개선 {improvement_batch4:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 일치 및 부분 일치 분석\n",
    "def analyze_match_types(results, model_name):\n",
    "    \"\"\"완전 일치 및 부분 일치 분석\"\"\"\n",
    "    predictions = results['predictions']\n",
    "    references = results['references']\n",
    "    \n",
    "    perfect_matches = 0\n",
    "    high_accuracy = 0  # 90% 이상\n",
    "    medium_accuracy = 0  # 70-90%\n",
    "    low_accuracy = 0  # 70% 미만\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        char_acc = calculate_character_accuracy(pred, ref)\n",
    "        \n",
    "        if pred.strip() == ref.strip():\n",
    "            perfect_matches += 1\n",
    "        elif char_acc >= 0.9:\n",
    "            high_accuracy += 1\n",
    "        elif char_acc >= 0.7:\n",
    "            medium_accuracy += 1\n",
    "        else:\n",
    "            low_accuracy += 1\n",
    "    \n",
    "    total = len(predictions)\n",
    "    \n",
    "    return {\n",
    "        'perfect_match': perfect_matches,\n",
    "        'high_accuracy': high_accuracy,\n",
    "        'medium_accuracy': medium_accuracy,\n",
    "        'low_accuracy': low_accuracy,\n",
    "        'perfect_match_rate': perfect_matches / total,\n",
    "        'high_accuracy_rate': high_accuracy / total,\n",
    "        'medium_accuracy_rate': medium_accuracy / total,\n",
    "        'low_accuracy_rate': low_accuracy / total\n",
    "    }\n",
    "\n",
    "match_analysis_base = analyze_match_types(results_base, \"원본 모델\")\n",
    "match_analysis_batch_1 = analyze_match_types(results_batch_1, \"배치 크기 1 모델\")\n",
    "match_analysis_batch_2 = analyze_match_types(results_batch_2, \"배치 크기 2 모델\")\n",
    "match_analysis_batch_4 = analyze_match_types(results_batch_4, \"배치 크기 4 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "categories = ['Perfect Match', 'High Accuracy\\n(90%+)', 'Medium Accuracy\\n(70-90%)', 'Low Accuracy\\n(<70%)']\n",
    "values_base = [match_analysis_base['perfect_match_rate'], \n",
    "               match_analysis_base['high_accuracy_rate'],\n",
    "               match_analysis_base['medium_accuracy_rate'], \n",
    "               match_analysis_base['low_accuracy_rate']]\n",
    "values_batch_1 = [match_analysis_batch_1['perfect_match_rate'], \n",
    "                  match_analysis_batch_1['high_accuracy_rate'],\n",
    "                  match_analysis_batch_1['medium_accuracy_rate'], \n",
    "                  match_analysis_batch_1['low_accuracy_rate']]\n",
    "values_batch_2 = [match_analysis_batch_2['perfect_match_rate'], \n",
    "                  match_analysis_batch_2['high_accuracy_rate'],\n",
    "                  match_analysis_batch_2['medium_accuracy_rate'], \n",
    "                  match_analysis_batch_2['low_accuracy_rate']]\n",
    "values_batch_4 = [match_analysis_batch_4['perfect_match_rate'], \n",
    "                  match_analysis_batch_4['high_accuracy_rate'],\n",
    "                  match_analysis_batch_4['medium_accuracy_rate'], \n",
    "                  match_analysis_batch_4['low_accuracy_rate']]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, values_base, width, label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x - 0.5*width, values_batch_1, width, label='Batch Size 1 Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + 0.5*width, values_batch_2, width, label='Batch Size 2 Model', color='lightgreen', alpha=0.7)\n",
    "bars4 = ax.bar(x + 1.5*width, values_batch_4, width, label='Batch Size 4 Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Accuracy Category')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Sample Distribution by Accuracy Category (Base vs Fine-tuned with Different Batch Sizes)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars, values in zip([bars1, bars2, bars3, bars4], [values_base, values_batch_1, values_batch_2, values_batch_4]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '05_accuracy_category_distribution.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 정확도 카테고리별 분석 결과 ===\")\n",
    "print(f\"{'Category':<20} {'Base Model':<15} {'Batch 1 Model':<15} {'Batch 2 Model':<15} {'Batch 4 Model':<15}\")\n",
    "print(\"-\" * 85)\n",
    "for i, category in enumerate(categories):\n",
    "    print(f\"{category:<20} {values_base[i]:<15.1%} {values_batch_1[i]:<15.1%} {values_batch_2[i]:<15.1%} {values_batch_4[i]:<15.1%}\")\n",
    "\n",
    "print(\"\\n=== 원본 모델 대비 개선율 ===\")\n",
    "print(f\"{'Category':<20} {'Batch 1 Improvement':<20} {'Batch 2 Improvement':<20} {'Batch 4 Improvement':<20}\")\n",
    "print(\"-\" * 85)\n",
    "for i, category in enumerate(categories):\n",
    "    improvement_batch_1 = ((values_batch_1[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_batch_1[i] > 0 else 0\n",
    "    improvement_batch_2 = ((values_batch_2[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_batch_2[i] > 0 else 0\n",
    "    improvement_batch_4 = ((values_batch_4[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_batch_4[i] > 0 else 0\n",
    "    \n",
    "    if improvement_batch_1 == float('inf'):\n",
    "        imp_batch_1_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_batch_1_str = f\"{improvement_batch_1:+.1f}%\"\n",
    "        \n",
    "    if improvement_batch_2 == float('inf'):\n",
    "        imp_batch_2_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_batch_2_str = f\"{improvement_batch_2:+.1f}%\"\n",
    "        \n",
    "    if improvement_batch_4 == float('inf'):\n",
    "        imp_batch_4_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_batch_4_str = f\"{improvement_batch_4:+.1f}%\"\n",
    "    \n",
    "    print(f\"{category:<20} {imp_batch_1_str:<20} {imp_batch_2_str:<20} {imp_batch_4_str:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5877e",
   "metadata": {},
   "source": [
    "## 11. 종합 결론 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dcacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"🔍 HyperCLOVAX 배치 크기 비교 분석 - 종합 결론 (원본 vs 배치 1 vs 2 vs 4)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 **모델 성능 비교 (원본 vs 배치 크기 미세조정)**\n",
    "\n",
    "📈 **BLEU 점수**\n",
    "- 원본: {results_base['bleu']:.4f}\n",
    "- 배치 1: {results_batch_1['bleu']:.4f} (개선율: {((results_batch_1['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "- 배치 2: {results_batch_2['bleu']:.4f} (개선율: {((results_batch_2['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "- 배치 4: {results_batch_4['bleu']:.4f} (개선율: {((results_batch_4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "\n",
    "📈 **ROUGE-L 점수**\n",
    "- 원본: {results_base['rougeL']:.4f}\n",
    "- 배치 1: {results_batch_1['rougeL']:.4f} (개선율: {((results_batch_1['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "- 배치 2: {results_batch_2['rougeL']:.4f} (개선율: {((results_batch_2['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "- 배치 4: {results_batch_4['rougeL']:.4f} (개선율: {((results_batch_4['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "\n",
    "📈 **문자 정확도**\n",
    "- 원본: {results_base['char_accuracy']:.4f}\n",
    "- 배치 1: {results_batch_1['char_accuracy']:.4f} (개선율: {((results_batch_1['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "- 배치 2: {results_batch_2['char_accuracy']:.4f} (개선율: {((results_batch_2['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "- 배치 4: {results_batch_4['char_accuracy']:.4f} (개선율: {((results_batch_4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "\n",
    "📈 **완전 일치율**\n",
    "- 원본: {results_base['exact_match']:.4f}\n",
    "- 배치 1: {results_batch_1['exact_match']:.4f} (개선율: {((results_batch_1['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "- 배치 2: {results_batch_2['exact_match']:.4f} (개선율: {((results_batch_2['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "- 배치 4: {results_batch_4['exact_match']:.4f} (개선율: {((results_batch_4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "\n",
    "⏱️ **효율성 분석**\n",
    "- 원본 모델 평균 추론 시간: {results_base['avg_inference_time']:.3f}초\n",
    "- 배치 1 모델 평균 추론 시간: {results_batch_1['avg_inference_time']:.3f}초 ({results_batch_1['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "- 배치 2 모델 평균 추론 시간: {results_batch_2['avg_inference_time']:.3f}초 ({results_batch_2['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "- 배치 4 모델 평균 추론 시간: {results_batch_4['avg_inference_time']:.3f}초 ({results_batch_4['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "\n",
    "💯 **복원 품질 분석**\n",
    "- 원본 모델 완전 일치: {match_analysis_base['perfect_match']}개 ({match_analysis_base['perfect_match_rate']:.1%})\n",
    "- 배치 1 모델 완전 일치: {match_analysis_batch_1['perfect_match']}개 ({match_analysis_batch_1['perfect_match_rate']:.1%})\n",
    "- 배치 2 모델 완전 일치: {match_analysis_batch_2['perfect_match']}개 ({match_analysis_batch_2['perfect_match_rate']:.1%})\n",
    "- 배치 4 모델 완전 일치: {match_analysis_batch_4['perfect_match']}개 ({match_analysis_batch_4['perfect_match_rate']:.1%})\n",
    "\n",
    "🎯 **핵심 인사이트**\n",
    "1. 미세조정이 원본 모델 대비 모든 지표에서 현저한 성능 향상을 가져옴\n",
    "2. {'BLEU 성능: 배치 ' + str(np.argmax([results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']]) + 1) + ' > 배치 ' + str(np.argsort([results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']])[1] + 1) + ' > 배치 ' + str(np.argsort([results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']])[0] + 1)}\n",
    "3. 완전 일치율에서 가장 드라마틱한 개선을 확인 (정확한 복원 능력 향상)\n",
    "4. 추론 시간 증가는 미미하여 효율성 저하 없이 성능 향상 달성\n",
    "5. 다양한 텍스트 길이에서 안정적인 성능 향상 확인\n",
    "\n",
    "💡 **권장 사항**\n",
    "- {'BLEU 기준 최고 성능 모델: 배치 ' + str(np.argmax([results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']]) + 1) + ' 모델 사용 강력 권장'}\n",
    "- 배치 크기 조정이 모델 성능에 미치는 영향 확인\n",
    "- 더 많은 데이터로 추가 학습 시 더 큰 성능 향상 기대 가능\n",
    "- 원본 모델의 제한적 성능을 고려할 때 미세조정의 효과가 매우 의미 있음\n",
    "- 배치 크기 최적화를 통한 추가 성능 향상 가능성 탐색\n",
    "- 도메인 특화 작업에서 미세조정의 중요성 입증\n",
    "\"\"\") \n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd32b3",
   "metadata": {},
   "source": [
    "## 12. 분석 메타데이터 및 요약\n",
    "\n",
    "이 분석은 HyperCLOVAX 모델에서 배치 크기 조정이 미치는 영향을 비교 분석한 결과입니다.\n",
    "\n",
    "**분석 대상 모델:**\n",
    "- 원본 모델: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- 배치 크기 1 모델: LoRA 미세조정 (배치 크기 1)\n",
    "- 배치 크기 2 모델: LoRA 미세조정 (배치 크기 2)\n",
    "- 배치 크기 4 모델: LoRA 미세조정 (배치 크기 4)\n",
    "\n",
    "**평가 메트릭:**\n",
    "- BLEU 점수: 번역 품질 평가\n",
    "- ROUGE 점수: 요약 품질 평가 (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "- 문자 정확도: 문자 단위 정확도\n",
    "- 완전 일치율: 전체 텍스트 완전 일치 비율\n",
    "- 추론 시간: 모델 추론 속도\n",
    "\n",
    "**분석 방법:**\n",
    "- 샘플 수: {SAMPLE_SIZE}개\n",
    "- 테스트 데이터: 한국어 난독화 해제 데이터셋\n",
    "- 평가 방식: 정량적 메트릭 및 질적 분석\n",
    "\n",
    "**주요 결과:**\n",
    "1. 배치 크기 조정이 모델 성능에 상당한 영향을 미침\n",
    "2. 원본 모델 대비 모든 미세조정 모델에서 성능 향상 확인\n",
    "3. 배치 크기별로 서로 다른 성능 패턴 및 특성 발견\n",
    "4. 효율성과 성능 간의 트레이드오프 분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV 파일로 저장 (배치 크기 4개 모델 비교)\n",
    "results_summary = pd.DataFrame({\n",
    "    '모델': ['원본 모델', '배치 크기 1 모델', '배치 크기 2 모델', '배치 크기 4 모델'],\n",
    "    'BLEU 점수': [results_base['bleu'], results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_batch_1['rouge1'], results_batch_2['rouge1'], results_batch_4['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_batch_1['rouge2'], results_batch_2['rouge2'], results_batch_4['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_batch_1['rougeL'], results_batch_2['rougeL'], results_batch_4['rougeL']],\n",
    "    '문자 정확도': [results_base['char_accuracy'], results_batch_1['char_accuracy'], results_batch_2['char_accuracy'], results_batch_4['char_accuracy']],\n",
    "    '정확 일치율': [results_base['exact_match'], results_batch_1['exact_match'], results_batch_2['exact_match'], results_batch_4['exact_match']],\n",
    "    '평균 추론 시간': [results_base['avg_inference_time'], results_batch_1['avg_inference_time'], results_batch_2['avg_inference_time'], results_batch_4['avg_inference_time']],\n",
    "    '총 추론 시간': [results_base['total_inference_time'], results_batch_1['total_inference_time'], results_batch_2['total_inference_time'], results_batch_4['total_inference_time']]\n",
    "})\n",
    "\n",
    "# 상세 결과도 저장 (배치 크기 4개 모델 포함)\n",
    "detailed_results = pd.DataFrame({\n",
    "    '인덱스': range(len(results_base['predictions'])),\n",
    "    '원본': results_base['references'],\n",
    "    '난독화': results_base['test_data']['obfuscated'].tolist(),\n",
    "    '예측_원본': results_base['predictions'],\n",
    "    '예측_배치_1': results_batch_1['predictions'],\n",
    "    '예측_배치_2': results_batch_2['predictions'],\n",
    "    '예측_배치_4': results_batch_4['predictions'],\n",
    "    '문자_정확도_원본': results_base['char_accuracies'],\n",
    "    '문자_정확도_배치_1': results_batch_1['char_accuracies'],\n",
    "    '문자_정확도_배치_2': results_batch_2['char_accuracies'],\n",
    "    '문자_정확도_배치_4': results_batch_4['char_accuracies'],\n",
    "    '정확_일치_원본': results_base['exact_matches'],\n",
    "    '정확_일치_배치_1': results_batch_1['exact_matches'],\n",
    "    '정확_일치_배치_2': results_batch_2['exact_matches'],\n",
    "    '정확_일치_배치_4': results_batch_4['exact_matches'],\n",
    "    '추론_시간_원본': results_base['inference_times'],\n",
    "    '추론_시간_배치_1': results_batch_1['inference_times'],\n",
    "    '추론_시간_배치_2': results_batch_2['inference_times'],\n",
    "    '추론_시간_배치_4': results_batch_4['inference_times']\n",
    "})\n",
    "\n",
    "# 미세조정 효과 분석 결과 저장\n",
    "finetuning_analysis = pd.DataFrame({\n",
    "    '비교': ['배치 1 vs 원본', '배치 2 vs 원본', '배치 4 vs 원본', '배치 2 vs 배치 1', '배치 4 vs 배치 1', '배치 4 vs 배치 2'],\n",
    "    'BLEU_개선율': [\n",
    "        ((results_batch_1['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_batch_2['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_batch_4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_batch_2['bleu'] - results_batch_1['bleu']) / results_batch_1['bleu'] * 100) if results_batch_1['bleu'] > 0 else 0,\n",
    "        ((results_batch_4['bleu'] - results_batch_1['bleu']) / results_batch_1['bleu'] * 100) if results_batch_1['bleu'] > 0 else 0,\n",
    "        ((results_batch_4['bleu'] - results_batch_2['bleu']) / results_batch_2['bleu'] * 100) if results_batch_2['bleu'] > 0 else 0\n",
    "    ],\n",
    "    '문자정확도_개선율': [\n",
    "        ((results_batch_1['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_batch_2['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_batch_4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_batch_2['char_accuracy'] - results_batch_1['char_accuracy']) / results_batch_1['char_accuracy'] * 100),\n",
    "        ((results_batch_4['char_accuracy'] - results_batch_1['char_accuracy']) / results_batch_1['char_accuracy'] * 100),\n",
    "        ((results_batch_4['char_accuracy'] - results_batch_2['char_accuracy']) / results_batch_2['char_accuracy'] * 100)\n",
    "    ],\n",
    "    '완전일치율_개선율': [\n",
    "        ((results_batch_1['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_batch_2['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_batch_4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_batch_2['exact_match'] - results_batch_1['exact_match']) / results_batch_1['exact_match'] * 100) if results_batch_1['exact_match'] > 0 else float('inf'),\n",
    "        ((results_batch_4['exact_match'] - results_batch_1['exact_match']) / results_batch_1['exact_match'] * 100) if results_batch_1['exact_match'] > 0 else float('inf'),\n",
    "        ((results_batch_4['exact_match'] - results_batch_2['exact_match']) / results_batch_2['exact_match'] * 100) if results_batch_2['exact_match'] > 0 else float('inf')\n",
    "    ],\n",
    "    '추론시간_비율': [\n",
    "        results_batch_1['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "        results_batch_2['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "        results_batch_4['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "        results_batch_2['avg_inference_time'] / results_batch_1['avg_inference_time'],\n",
    "        results_batch_4['avg_inference_time'] / results_batch_1['avg_inference_time'],\n",
    "        results_batch_4['avg_inference_time'] / results_batch_2['avg_inference_time']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# CSV 파일 저장 경로 (분석 결과 폴더 내)\n",
    "csv1_path = os.path.join(analysis_root_dir, 'model_performance_summary_batch_size.csv')\n",
    "csv2_path = os.path.join(analysis_root_dir, 'detailed_model_comparison_batch_size.csv')\n",
    "csv3_path = os.path.join(analysis_root_dir, 'batch_size_effect_analysis.csv')\n",
    "\n",
    "results_summary.to_csv(csv1_path, index=False, encoding='utf-8-sig')\n",
    "detailed_results.to_csv(csv2_path, index=False, encoding='utf-8-sig')\n",
    "finetuning_analysis.to_csv(csv3_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"📁 결과 파일 저장 완료:\")\n",
    "print(f\"- {csv1_path}\")\n",
    "print(f\"- {csv2_path}\")\n",
    "print(f\"- {csv3_path}\")\n",
    "\n",
    "# 시각화 이미지 목록 출력\n",
    "print(\"\\n📊 생성된 시각화 이미지:\")\n",
    "image_files = os.listdir(image_save_dir)\n",
    "image_files = [f for f in image_files if f.endswith('.png')]\n",
    "for i, img_file in enumerate(sorted(image_files), 1):\n",
    "    print(f\"{i}. {os.path.join(image_save_dir, img_file)}\")\n",
    "\n",
    "# 결과 파일들을 압축하여 다운로드 준비\n",
    "import zipfile\n",
    "zip_filename = os.path.join(analysis_root_dir, 'HyperCLOVAX_BatchSize_Analysis_Complete_Results.zip')\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # CSV 파일들 추가\n",
    "    zipf.write(csv1_path, 'results/model_performance_summary_batch_size.csv')\n",
    "    zipf.write(csv2_path, 'results/detailed_model_comparison_batch_size.csv')\n",
    "    zipf.write(csv3_path, 'results/batch_size_effect_analysis.csv')\n",
    "    \n",
    "    # 이미지 파일들 추가\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_save_dir, img_file)\n",
    "        zipf.write(img_path, f'visualizations/{img_file}')\n",
    "\n",
    "print(f\"\\n📦 압축 파일 생성: {zip_filename}\")\n",
    "print(\"압축 파일 내용:\")\n",
    "print(\"  📁 results/ - CSV 분석 결과 파일들 (배치 크기 4개 모델 비교)\")\n",
    "print(\"  📁 visualizations/ - 시각화 이미지들 (배치 크기 4개 모델 비교)\")\n",
    "\n",
    "# Google Colab에서 다운로드 시도\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_filename)\n",
    "    print(\"📥 압축 파일 다운로드 완료\")\n",
    "except ImportError:\n",
    "    print(\"💾 로컬 환경에서는 다음 경로에 모든 파일이 저장되었습니다:\")\n",
    "    print(f\"   분석 결과 폴더: {analysis_root_dir}\")\n",
    "    print(f\"   압축 파일: {zip_filename}\")\n",
    "    print(f\"   이미지 폴더: {image_save_dir}\")\n",
    "\n",
    "# Google Drive 폴더 구조 안내\n",
    "print(f\"\\n📂 Google Drive 폴더 구조:\")\n",
    "print(f\"MyDrive/\")\n",
    "print(f\"├── HyperCLOVAX_BatchSize_Analysis_Results/\")\n",
    "print(f\"│   ├── model_performance_summary_batch_size.csv      # 배치 크기 4개 모델 성능 요약\")\n",
    "print(f\"│   ├── detailed_model_comparison_batch_size.csv      # 배치 크기 4개 모델 상세 비교\")\n",
    "print(f\"│   ├── batch_size_effect_analysis.csv               # 배치 크기 효과 분석\")\n",
    "print(f\"│   ├── HyperCLOVAX_BatchSize_Analysis_Complete_Results.zip\")\n",
    "print(f\"│   └── visualization_images/\")\n",
    "print(f\"│       ├── 01_batch_size_performance_comparison.png      # 배치 크기 4개 모델 성능 비교\")\n",
    "print(f\"│       ├── 02_character_accuracy_distribution.png\")\n",
    "print(f\"│       ├── 03_inference_time_comparison.png\")\n",
    "print(f\"│       ├── 04_performance_by_text_length.png\")\n",
    "print(f\"│       └── 05_accuracy_category_distribution.png\")\n",
    "print(f\"├── hyperclova-deobfuscation-lora-with-1-batch-size/\")\n",
    "print(f\"├── hyperclova-deobfuscation-lora-with-2-batch-size/\")\n",
    "print(f\"├── hyperclova-deobfuscation-lora-with-4-batch-size/\")\n",
    "print(f\"└── testdata.csv\")\n",
    "\n",
    "# 결과 요약 출력 (배치 크기 4개 모델)\n",
    "print(\"\\n=== 최종 성능 요약 (원본 vs 배치 1 vs 2 vs 4) ===\")\n",
    "print(results_summary.round(4))\n",
    "\n",
    "# 미세조정 효과 요약\n",
    "best_base_accuracy = results_base['char_accuracy']\n",
    "best_batch_1_accuracy = results_batch_1['char_accuracy']\n",
    "best_batch_2_accuracy = results_batch_2['char_accuracy']\n",
    "best_batch_4_accuracy = results_batch_4['char_accuracy']\n",
    "\n",
    "print(f\"\\n=== 미세조정 효과 분석 ===\")\n",
    "print(f\"원본 모델 문자 정확도: {best_base_accuracy:.4f}\")\n",
    "print(f\"배치 1 모델 문자 정확도: {best_batch_1_accuracy:.4f} ({((best_batch_1_accuracy - best_base_accuracy) / best_base_accuracy * 100):+.2f}%)\")\n",
    "print(f\"배치 2 모델 문자 정확도: {best_batch_2_accuracy:.4f} ({((best_batch_2_accuracy - best_base_accuracy) / best_base_accuracy * 100):+.2f}%)\")\n",
    "print(f\"배치 4 모델 문자 정확도: {best_batch_4_accuracy:.4f} ({((best_batch_4_accuracy - best_base_accuracy) / best_base_accuracy * 100):+.2f}%)\")\n",
    "\n",
    "# 최고 성능 모델 식별\n",
    "batch_accuracies = [best_batch_1_accuracy, best_batch_2_accuracy, best_batch_4_accuracy]\n",
    "best_batch_idx = np.argmax(batch_accuracies)\n",
    "batch_names = [\"배치 1\", \"배치 2\", \"배치 4\"]\n",
    "best_model = batch_names[best_batch_idx]\n",
    "best_accuracy = batch_accuracies[best_batch_idx]\n",
    "\n",
    "print(f\"\\n최고 성능 모델: {best_model} 모델 (문자 정확도: {best_accuracy:.4f})\")\n",
    "\n",
    "improvement = ((best_accuracy - best_base_accuracy) / best_base_accuracy * 100)\n",
    "print(f\"원본 모델 대비 성능 향상: {improvement:.2f}%\")\n",
    "print(f\"배치 크기 조정 효과: {'매우 효과적' if improvement > 20 else '효과적' if improvement > 10 else '보통' if improvement > 5 else '제한적'}\")\n",
    "\n",
    "# 배치 크기별 성능 트렌드 및 최상의 배치 크기 추천\n",
    "print(f\"\\n=== 배치 크기별 성능 트렌드 ===\")\n",
    "bleu_values = [results_batch_1['bleu'], results_batch_2['bleu'], results_batch_4['bleu']]\n",
    "char_acc_values = [results_batch_1['char_accuracy'], results_batch_2['char_accuracy'], results_batch_4['char_accuracy']]\n",
    "rouge_values = [results_batch_1['rouge1'], results_batch_2['rouge1'], results_batch_4['rouge1']]\n",
    "\n",
    "bleu_best_idx = np.argmax(bleu_values)\n",
    "char_best_idx = np.argmax(char_acc_values)\n",
    "rouge_best_idx = np.argmax(rouge_values)\n",
    "\n",
    "print(f\"BLEU 점수 최고: {batch_names[bleu_best_idx]} ({bleu_values[bleu_best_idx]:.4f})\")\n",
    "print(f\"문자 정확도 최고: {batch_names[char_best_idx]} ({char_acc_values[char_best_idx]:.4f})\")\n",
    "print(f\"ROUGE-1 점수 최고: {batch_names[rouge_best_idx]} ({rouge_values[rouge_best_idx]:.4f})\")\n",
    "\n",
    "# 전반적 추천\n",
    "best_overall = max(set([bleu_best_idx, char_best_idx, rouge_best_idx]), key=[bleu_best_idx, char_best_idx, rouge_best_idx].count)\n",
    "print(f\"\\n전반적 추천 배치 크기: {batch_names[best_overall]} (다수 메트릭에서 최고 성능)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
