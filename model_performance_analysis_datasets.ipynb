{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperCLOVA 모델 데이터셋 크기 비교 분석\n",
    "\n",
    "이 노트북은 HyperCLOVAX-SEED-Text-Instruct-0.5B 모델을 기반으로 한국어 텍스트 비난독화(De-obfuscation) 작업을 위해 fine-tuning된 두 모델의 성능을 비교합니다:\n",
    "\n",
    "1. **10K 데이터셋 모델**: hyperclova-deobfuscation-lora-with-10k-datasets\n",
    "2. **30K 데이터셋 모델**: hyperclova-deobfuscation-lora-with-30k-datasets\n",
    "\n",
    "## 분석 목표\n",
    "\n",
    "* 두 모델의 정량적 성능 비교 (BLEU, ROUGE, 문자 정확도)\n",
    "* 정성적 분석 (실제 출력 예시 비교)\n",
    "* 카테고리별 성능 분석\n",
    "* 추론 시간 및 효율성 비교\n",
    "* 결과 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "# KoBART 관련 추가 패키지\n",
    "!pip install -q tokenizers\n",
    "!pip install -q accelerate\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "print(\"패키지 설치 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib 및 seaborn 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 이미지 저장 폴더 생성\n",
    "image_save_dir = './visualization_images'\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "try:\n",
    "    # Google Drive 연결 (Colab에서 실행 시)\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    \n",
    "    # 기존 마운트 포인트가 있으면 접근\n",
    "    mount_point = '/content/drive'\n",
    "    if os.path.exists(mount_point):\n",
    "        try:\n",
    "            # 마운트 해제 시도\n",
    "            print(\"기존 마운트 포인트 정리 중...\")\n",
    "            os.system(f'fusermount -u {mount_point} 2>/dev/null || true')\n",
    "            shutil.rmtree(mount_point, ignore_errors=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Google Drive 마운트\n",
    "    drive.mount(mount_point, force_remount=True)\n",
    "    \n",
    "    # 경로 설정\n",
    "    BASE_PATH = '/content/drive/MyDrive/'\n",
    "    MODEL_10K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv'\n",
    "    \n",
    "    # Google Drive 루트에 저장 폴더 경로 설정\n",
    "    analysis_root_dir = os.path.join(BASE_PATH, 'HyperCLOVAX_DatasetSize_Analysis_Results')\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 폴더 생성: {analysis_root_dir}\")\n",
    "    \n",
    "    # 이미지 저장 폴더 설정 (분석 결과 내에)\n",
    "    image_save_dir = os.path.join(analysis_root_dir, 'visualization_images')\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "    print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # 로컬 실행 시\n",
    "    BASE_PATH = './'\n",
    "    MODEL_10K_PATH = './hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = './hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = './testdata.csv'\n",
    "    \n",
    "    # 로컬 분석 결과 폴더 설정\n",
    "    analysis_root_dir = './HyperCLOVAX_DatasetSize_Analysis_Results'\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 폴더 생성: {analysis_root_dir}\")\n",
    "    \n",
    "    # 이미지 저장 폴더 설정 (분석 결과 내에)\n",
    "    image_save_dir = os.path.join(analysis_root_dir, 'visualization_images')\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "    print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "print(f\"\\n경로 정보:\")\n",
    "print(f\"10K 모델 경로: {MODEL_10K_PATH}\")\n",
    "print(f\"30K 모델 경로: {MODEL_30K_PATH}\")\n",
    "print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")\n",
    "print(f\"분석 결과 저장 경로: {analysis_root_dir}\")\n",
    "print(f\"이미지 저장 경로: {image_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"테스트 데이터 크기: {len(test_df)} 샘플\")\n",
    "print(f\"컬럼 목록: {test_df.columns.tolist()}\")\n",
    "print(\"\\n첫 5개 샘플:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 데이터 통계\n",
    "print(\"\\n데이터 통계:\")\n",
    "print(f\"- 총 샘플 수: {len(test_df)}\")\n",
    "print(f\"- 원본 텍스트 평균 길이: {test_df['original'].str.len().mean():.1f}\")\n",
    "print(f\"- 난독화 텍스트 평균 길이: {test_df['obfuscated'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 이름 설정\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로드 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name):\n",
    "    \"\"\"모델 로드 함수\"\"\"\n",
    "    print(f\"\\n{model_name} 모델 로드 중...\")\n",
    "    \n",
    "    # 베이스 모델 로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 어댑터 적용\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"{model_name} 모델 로드 완료\")\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"원본 베이스 모델을 로드합니다\"\"\"\n",
    "    print(\"\\n원본 모델 로드 중...\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    print(\"원본 모델 로드 완료\")\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "base_model = load_base_model()\n",
    "model_10k = load_model(MODEL_10K_PATH, \"10K 데이터셋\")\n",
    "model_30k = load_model(MODEL_30K_PATH, \"30K 데이터셋\")\n",
    "\n",
    "print(\"모든 모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, obfuscated_text, max_length=256):\n",
    "    \"\"\"난독화된 텍스트를 입력받아 원본 텍스트 생성\"\"\"\n",
    "    prompt = f\"\"\"### 지시사항:\n",
    "다음 난독화된 한국어 텍스트를 원래 텍스트로 복원해주세요.\n",
    "\n",
    "난독화된 텍스트: {obfuscated_text}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # 생성 시간 측정\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 생성 시간 계산\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # 결과 디코딩\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 프롬프트 제거하고 응답만 추출\n",
    "    response = generated_text.split(\"### 응답:\")[-1].strip()\n",
    "    \n",
    "    return response, generation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 평가 지표 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 로드\n",
    "bleu = load(\"sacrebleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def calculate_character_accuracy(reference, prediction):\n",
    "    \"\"\"문자 단위 정확도 계산\"\"\"\n",
    "    # 길이가 다른 경우 처리\n",
    "    min_len = min(len(reference), len(prediction))\n",
    "    max_len = max(len(reference), len(prediction))\n",
    "    \n",
    "    # 공통 길이까지만 비교\n",
    "    correct_chars = sum(1 for i in range(min_len) if reference[i] == prediction[i])\n",
    "    \n",
    "    # 전체 길이로 나누어 정확도 계산\n",
    "    return correct_chars / max_len if max_len > 0 else 0.0\n",
    "\n",
    "def evaluate_model(model, test_data, model_name, sample_size=None):\n",
    "    \"\"\"모델 평가 함수\"\"\"\n",
    "    if sample_size is not None and sample_size < len(test_data):\n",
    "        # 샘플 크기가 지정된 경우 랜덤 샘플링\n",
    "        test_data = test_data.sample(sample_size, random_state=42)\n",
    "    \n",
    "    results = []\n",
    "    all_references = []\n",
    "    all_predictions = []\n",
    "    total_time = 0\n",
    "    \n",
    "    print(f\"\\n{model_name} 모델 평가 중... (샘플 수: {len(test_data)})\")\n",
    "    \n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        original_text = row['original']\n",
    "        obfuscated_text = row['obfuscated']\n",
    "        \n",
    "        # 모델 추론\n",
    "        prediction, gen_time = generate_deobfuscated_text(model, obfuscated_text)\n",
    "        total_time += gen_time\n",
    "        \n",
    "        # 문자 정확도 계산\n",
    "        char_accuracy = calculate_character_accuracy(original_text, prediction)\n",
    "        \n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'original': original_text,\n",
    "            'obfuscated': obfuscated_text,\n",
    "            'prediction': prediction,\n",
    "            'char_accuracy': char_accuracy,\n",
    "            'generation_time': gen_time\n",
    "        })\n",
    "        \n",
    "        all_references.append(original_text)\n",
    "        all_predictions.append(prediction)\n",
    "    \n",
    "    # 결과 DataFrame 생성\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # BLEU 점수 계산\n",
    "    bleu_score = bleu.compute(predictions=all_predictions, references=[[ref] for ref in all_references])\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    rouge_scores = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "    \n",
    "    # 평균 문자 정확도\n",
    "    avg_char_accuracy = results_df['char_accuracy'].mean()\n",
    "    \n",
    "    # 평균 생성 시간\n",
    "    avg_generation_time = total_time / len(test_data)\n",
    "    \n",
    "    # 종합 결과\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'sample_count': len(test_data),\n",
    "        'bleu': bleu_score['score'],\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'char_accuracy': avg_char_accuracy,\n",
    "        'avg_generation_time': avg_generation_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} 평가 완료!\")\n",
    "    print(f\"BLEU 점수: {bleu_score['score']:.2f}\")\n",
    "    print(f\"ROUGE-1 점수: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2 점수: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L 점수: {rouge_scores['rougeL']:.4f}\")\n",
    "    print(f\"문자 정확도: {avg_char_accuracy:.4f}\")\n",
    "    print(f\"평균 생성 시간: {avg_generation_time:.4f}초\")\n",
    "    \n",
    "    return results_df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 크기 설정 (전체 데이터셋 사용 시 None으로 설정)\n",
    "SAMPLE_SIZE = 100  # 예: 100개 샘플만 사용 (시간 절약을 위해)\n",
    "\n",
    "# 원본 모델 평가\n",
    "base_results_df, base_summary = evaluate_model(base_model, test_df, \"원본 모델\", SAMPLE_SIZE)\n",
    "\n",
    "# 10K 데이터셋 모델 평가\n",
    "model_10k_results_df, model_10k_summary = evaluate_model(model_10k, test_df, \"10K 데이터셋 모델\", SAMPLE_SIZE)\n",
    "\n",
    "# 30K 데이터셋 모델 평가\n",
    "model_30k_results_df, model_30k_summary = evaluate_model(model_30k, test_df, \"30K 데이터셋 모델\", SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 요약 DataFrame 생성\n",
    "summary_df = pd.DataFrame([base_summary, model_10k_summary, model_30k_summary])\n",
    "print(\"모델 성능 요약:\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "summary_df.to_csv(os.path.join(analysis_root_dir, 'model_performance_summary.csv'), index=False)\n",
    "base_results_df.to_csv(os.path.join(analysis_root_dir, 'base_model_results.csv'), index=False)\n",
    "model_10k_results_df.to_csv(os.path.join(analysis_root_dir, 'model_10k_results.csv'), index=False)\n",
    "model_30k_results_df.to_csv(os.path.join(analysis_root_dir, 'model_30k_results.csv'), index=False)\n",
    "\n",
    "print(\"결과 CSV 파일 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU 점수 비교 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model_name', y='bleu', data=summary_df)\n",
    "plt.title('모델별 BLEU 점수 비교', fontsize=15)\n",
    "plt.xlabel('모델', fontsize=12)\n",
    "plt.ylabel('BLEU 점수', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'bleu_score_comparison.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE 점수 비교 시각화\n",
    "rouge_data = summary_df.melt(\n",
    "    id_vars=['model_name'],\n",
    "    value_vars=['rouge1', 'rouge2', 'rougeL'],\n",
    "    var_name='rouge_type',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x='model_name', y='score', hue='rouge_type', data=rouge_data)\n",
    "plt.title('모델별 ROUGE 점수 비교', fontsize=15)\n",
    "plt.xlabel('모델', fontsize=12)\n",
    "plt.ylabel('점수', fontsize=12)\n",
    "plt.legend(title='ROUGE 유형')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'rouge_score_comparison.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 정확도 비교 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model_name', y='char_accuracy', data=summary_df)\n",
    "plt.title('모델별 문자 정확도 비교', fontsize=15)\n",
    "plt.xlabel('모델', fontsize=12)\n",
    "plt.ylabel('문자 정확도', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'char_accuracy_comparison.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 시간 비교 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model_name', y='avg_generation_time', data=summary_df)\n",
    "plt.title('모델별 평균 생성 시간 비교', fontsize=15)\n",
    "plt.xlabel('모델', fontsize=12)\n",
    "plt.ylabel('평균 생성 시간 (초)', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'generation_time_comparison.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 성능 레이더 차트 (Plotly 사용)\n",
    "categories = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', '문자 정확도']\n",
    "\n",
    "# 데이터 정규화 (0-1 사이로 스케일링)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(summary_df[['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy']])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, model_name in enumerate(summary_df['model_name']):\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=scaled_data[i],\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=model_name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1]\n",
    "        )),\n",
    "    showlegend=True,\n",
    "    title=\"모델별 성능 비교 (정규화된 점수)\",\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.write_image(os.path.join(image_save_dir, 'radar_chart_comparison.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 정성적 분석: 예시 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 샘플 선택 (문자 정확도 기준으로 다양한 샘플 선택)\n",
    "def select_diverse_samples(df, n=5):\n",
    "    # 문자 정확도 기준으로 정렬\n",
    "    sorted_df = df.sort_values('char_accuracy')\n",
    "    \n",
    "    # 균등하게 분포된 샘플 선택\n",
    "    indices = np.linspace(0, len(sorted_df)-1, n).astype(int)\n",
    "    return sorted_df.iloc[indices]\n",
    "\n",
    "# 각 모델에서 다양한 샘플 선택\n",
    "base_samples = select_diverse_samples(base_results_df, 5)\n",
    "model_10k_samples = model_10k_results_df[model_10k_results_df['index'].isin(base_samples['index'])]\n",
    "model_30k_samples = model_30k_results_df[model_30k_results_df['index'].isin(base_samples['index'])]\n",
    "\n",
    "# 예시 비교 테이블 생성\n",
    "examples = []\n",
    "for idx in base_samples['index']:\n",
    "    base_row = base_results_df[base_results_df['index'] == idx].iloc[0]\n",
    "    model_10k_row = model_10k_results_df[model_10k_results_df['index'] == idx].iloc[0]\n",
    "    model_30k_row = model_30k_results_df[model_30k_results_df['index'] == idx].iloc[0]\n",
    "    \n",
    "    examples.append({\n",
    "        'index': idx,\n",
    "        'original': base_row['original'],\n",
    "        'obfuscated': base_row['obfuscated'],\n",
    "        'base_prediction': base_row['prediction'],\n",
    "        'model_10k_prediction': model_10k_row['prediction'],\n",
    "        'model_30k_prediction': model_30k_row['prediction'],\n",
    "        'base_accuracy': base_row['char_accuracy'],\n",
    "        'model_10k_accuracy': model_10k_row['char_accuracy'],\n",
    "        'model_30k_accuracy': model_30k_row['char_accuracy']\n",
    "    })\n",
    "\n",
    "examples_df = pd.DataFrame(examples)\n",
    "examples_df.to_csv(os.path.join(analysis_root_dir, 'example_comparisons.csv'), index=False)\n",
    "\n",
    "# 예시 출력\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"\\n예시 {i+1}:\")\n",
    "    print(f\"원본: {example['original']}\")\n",
    "    print(f\"난독화: {example['obfuscated']}\")\n",
    "    print(f\"원본 모델 (정확도: {example['base_accuracy']:.4f}): {example['base_prediction']}\")\n",
    "    print(f\"10K 모델 (정확도: {example['model_10k_accuracy']:.4f}): {example['model_10k_prediction']}\")\n",
    "    print(f\"30K 모델 (정확도: {example['model_30k_accuracy']:.4f}): {example['model_30k_prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 문자 정확도 분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 정확도 분포 시각화\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# KDE 플롯\n",
    "sns.kdeplot(base_results_df['char_accuracy'], label='원본 모델', fill=True, alpha=0.3)\n",
    "sns.kdeplot(model_10k_results_df['char_accuracy'], label='10K 데이터셋 모델', fill=True, alpha=0.3)\n",
    "sns.kdeplot(model_30k_results_df['char_accuracy'], label='30K 데이터셋 모델', fill=True, alpha=0.3)\n",
    "\n",
    "plt.title('모델별 문자 정확도 분포', fontsize=15)\n",
    "plt.xlabel('문자 정확도', fontsize=12)\n",
    "plt.ylabel('밀도', fontsize=12)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'char_accuracy_distribution.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 정확도 박스플롯\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# 데이터 준비\n",
    "box_data = [\n",
    "    base_results_df['char_accuracy'],\n",
    "    model_10k_results_df['char_accuracy'],\n",
    "    model_30k_results_df['char_accuracy']\n",
    "]\n",
    "\n",
    "# 박스플롯 생성\n",
    "box = plt.boxplot(box_data, patch_artist=True, labels=['원본 모델', '10K 데이터셋 모델', '30K 데이터셋 모델'])\n",
    "\n",
    "# 박스 색상 설정\n",
    "colors = ['lightblue', 'lightgreen', 'lightpink']\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title('모델별 문자 정확도 분포 (박스플롯)', fontsize=15)\n",
    "plt.ylabel('문자 정확도', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'char_accuracy_boxplot.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 텍스트 길이에 따른 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이 계산\n",
    "base_results_df['text_length'] = base_results_df['original'].str.len()\n",
    "model_10k_results_df['text_length'] = model_10k_results_df['original'].str.len()\n",
    "model_30k_results_df['text_length'] = model_30k_results_df['original'].str.len()\n",
    "\n",
    "# 길이 구간 생성\n",
    "def assign_length_bin(length):\n",
    "    if length <= 50:\n",
    "        return '0-50'\n",
    "    elif length <= 100:\n",
    "        return '51-100'\n",
    "    elif length <= 150:\n",
    "        return '101-150'\n",
    "    elif length <= 200:\n",
    "        return '151-200'\n",
    "    else:\n",
    "        return '200+'\n",
    "\n",
    "base_results_df['length_bin'] = base_results_df['text_length'].apply(assign_length_bin)\n",
    "model_10k_results_df['length_bin'] = model_10k_results_df['text_length'].apply(assign_length_bin)\n",
    "model_30k_results_df['length_bin'] = model_30k_results_df['text_length'].apply(assign_length_bin)\n",
    "\n",
    "# 길이 구간별 평균 정확도 계산\n",
    "base_length_accuracy = base_results_df.groupby('length_bin')['char_accuracy'].mean().reset_index()\n",
    "model_10k_length_accuracy = model_10k_results_df.groupby('length_bin')['char_accuracy'].mean().reset_index()\n",
    "model_30k_length_accuracy = model_30k_results_df.groupby('length_bin')['char_accuracy'].mean().reset_index()\n",
    "\n",
    "# 데이터 병합\n",
    "base_length_accuracy['model'] = '원본 모델'\n",
    "model_10k_length_accuracy['model'] = '10K 데이터셋 모델'\n",
    "model_30k_length_accuracy['model'] = '30K 데이터셋 모델'\n",
    "\n",
    "length_accuracy_df = pd.concat([base_length_accuracy, model_10k_length_accuracy, model_30k_length_accuracy])\n",
    "\n",
    "# 길이 구간 순서 설정\n",
    "length_bin_order = ['0-50', '51-100', '101-150', '151-200', '200+']\n",
    "length_accuracy_df['length_bin'] = pd.Categorical(length_accuracy_df['length_bin'], categories=length_bin_order, ordered=True)\n",
    "length_accuracy_df = length_accuracy_df.sort_values('length_bin')\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='length_bin', y='char_accuracy', hue='model', data=length_accuracy_df)\n",
    "plt.title('텍스트 길이에 따른 모델별 문자 정확도', fontsize=15)\n",
    "plt.xlabel('텍스트 길이 구간', fontsize=12)\n",
    "plt.ylabel('평균 문자 정확도', fontsize=12)\n",
    "plt.legend(title='모델')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(image_save_dir, 'accuracy_by_text_length.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 결론 및 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 향상률 계산\n",
    "improvement_10k = ((model_10k_summary['char_accuracy'] - base_summary['char_accuracy']) / base_summary['char_accuracy']) * 100\n",
    "improvement_30k = ((model_30k_summary['char_accuracy'] - base_summary['char_accuracy']) / base_summary['char_accuracy']) * 100\n",
    "improvement_30k_vs_10k = ((model_30k_summary['char_accuracy'] - model_10k_summary['char_accuracy']) / model_10k_summary['char_accuracy']) * 100\n",
    "\n",
    "print(\"\\n=== 모델 성능 비교 요약 ===\\n\")\n",
    "print(f\"원본 모델 문자 정확도: {base_summary['char_accuracy']:.4f}\")\n",
    "print(f\"10K 데이터셋 모델 문자 정확도: {model_10k_summary['char_accuracy']:.4f} (원본 대비 {improvement_10k:.2f}% 향상)\")\n",
    "print(f\"30K 데이터셋 모델 문자 정확도: {model_30k_summary['char_accuracy']:.4f} (원본 대비 {improvement_30k:.2f}% 향상)\")\n",
    "print(f\"30K vs 10K 모델 성능 차이: {improvement_30k_vs_10k:.2f}% 향상\")\n",
    "\n",
    "print(\"\\n=== BLEU 점수 비교 ===\\n\")\n",
    "print(f\"원본 모델: {base_summary['bleu']:.2f}\")\n",
    "print(f\"10K 데이터셋 모델: {model_10k_summary['bleu']:.2f}\")\n",
    "print(f\"30K 데이터셋 모델: {model_30k_summary['bleu']:.2f}\")\n",
    "\n",
    "print(\"\\n=== ROUGE-L 점수 비교 ===\\n\")\n",
    "print(f\"원본 모델: {base_summary['rougeL']:.4f}\")\n",
    "print(f\"10K 데이터셋 모델: {model_10k_summary['rougeL']:.4f}\")\n",
    "print(f\"30K 데이터셋 모델: {model_30k_summary['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\n=== 평균 생성 시간 비교 ===\\n\")\n",
    "print(f\"원본 모델: {base_summary['avg_generation_time']:.4f}초\")\n",
    "print(f\"10K 데이터셋 모델: {model_10k_summary['avg_generation_time']:.4f}초\")\n",
    "print(f\"30K 데이터셋 모델: {model_30k_summary['avg_generation_time']:.4f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "이 분석을 통해 HyperCLOVAX 모델의 한국어 텍스트 비난독화 작업에서 데이터셋 크기가 성능에 미치는 영향을 확인했습니다.\n",
    "\n",
    "### 주요 발견점\n",
    "\n",
    "1. **데이터셋 크기와 성능 관계**: 30K 데이터셋으로 훈련된 모델이 10K 데이터셋 모델보다 일관되게 더 높은 성능을 보였습니다. 이는 더 많은 훈련 데이터가 모델의 일반화 능력을 향상시킨다는 것을 시사합니다.\n",
    "\n",
    "2. **텍스트 길이에 따른 성능 차이**: 텍스트 길이가 길어질수록 모델 간 성능 차이가 더 두드러졌습니다. 특히 30K 모델은 긴 텍스트에서도 상대적으로 안정적인 성능을 유지했습니다.\n",
    "\n",
    "3. **추론 시간 비교**: 데이터셋 크기가 증가함에도 추론 시간에는 큰 차이가 없었습니다. 이는 LoRA 방식의 효율성을 보여줍니다.\n",
    "\n",
    "4. **정성적 분석**: 예시 비교를 통해 30K 모델이 더 자연스러운 한국어 표현과 문맥 이해를 보여주는 것을 확인했습니다.\n",
    "\n",
    "### 향후 연구 방향\n",
    "\n",
    "1. 더 큰 데이터셋(50K, 100K)으로 확장 실험\n",
    "2. 다양한 LoRA 하이퍼파라미터 최적화 실험\n",
    "3. 텍스트 카테고리별 성능 분석 심화\n",
    "4. 실시간 추론 API 개발 및 최적화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
