{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bbf116",
   "metadata": {},
   "source": [
    "# HyperCLOVAX 모델 Learning Rate 비교 분석\n",
    "\n",
    "이 노트북은 원본 모델과 서로 다른 learning rate로 미세조정된 HyperCLOVAX 모델의 성능을 비교합니다.\n",
    "\n",
    "## 분석 목표\n",
    "- 세 모델 간 정량적 성능 비교 (BLEU, ROUGE, 문자 정확도)\n",
    "- 정성적 분석 (실제 출력 예시 비교)\n",
    "- 카테고리별 성능 분석\n",
    "- 추론 시간 및 효율성 비교\n",
    "- Learning Rate 미세조정 효과 분석\n",
    "- 결과 시각화\n",
    "\n",
    "## 모델 정보\n",
    "- **원본 모델**: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- **1e-4 Learning Rate 모델**: `hyperclova-deobfuscation-lora-1e-4-learning-rate`\n",
    "- **5e-4 Learning Rate 모델**: `hyperclova-deobfuscation-lora-5e-4-learning-rate`\n",
    "- **테스트 데이터**: `testdata.csv` (1,002 샘플)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb29ec6",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"패키지 설치 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib 및 seaborn 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 이미지 저장 폴더 생성\n",
    "image_save_dir = '/content/drive/MyDrive/Colab Notebooks/analysis_images'\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5b03f",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 연결 (Colab에서 실행 시)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    \n",
    "    # 기존 마운트 포인트가 있으면 정리\n",
    "    mount_point = '/content/drive'\n",
    "    if os.path.exists(mount_point):\n",
    "        try:\n",
    "            # 마운트 해제 시도\n",
    "            print(\"기존 마운트 포인트 정리 중...\")\n",
    "            os.system(f'fusermount -u {mount_point} 2>/dev/null || true')\n",
    "            shutil.rmtree(mount_point, ignore_errors=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Google Drive 마운트\n",
    "    drive.mount(mount_point, force_remount=True)\n",
    "    \n",
    "    # 경로 설정\n",
    "    BASE_PATH = '/content/drive/MyDrive/'\n",
    "    MODEL_1E4_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-1e-4-learning-rate'\n",
    "    MODEL_5E4_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-5e-4-learning-rate'\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv'\n",
    "    \n",
    "    # Google Drive 루트에 전용 분석 결과 폴더 생성\n",
    "    analysis_root_dir = os.path.join(BASE_PATH, 'HyperCLOVAX_LearningRate_Analysis_Results')\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # 로컬 실행 시\n",
    "    BASE_PATH = './'\n",
    "    MODEL_1E4_PATH = './hyperclova-deobfuscation-lora-1e-4-learning-rate'\n",
    "    MODEL_5E4_PATH = './hyperclova-deobfuscation-lora-5e-4-learning-rate'\n",
    "    TEST_DATA_PATH = './testdata.csv'\n",
    "    \n",
    "    # 로컬용 분석 결과 폴더\n",
    "    analysis_root_dir = './HyperCLOVAX_LearningRate_Analysis_Results'\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "\n",
    "# 이미지 저장 폴더 생성 (분석 결과 폴더 내에)\n",
    "image_save_dir = os.path.join(analysis_root_dir, 'visualization_images')\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "print(f\"\\n경로 설정 완료:\")\n",
    "print(f\"1e-4 모델 경로: {MODEL_1E4_PATH}\")\n",
    "print(f\"5e-4 모델 경로: {MODEL_5E4_PATH}\")\n",
    "print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")\n",
    "print(f\"분석 결과 저장 경로: {analysis_root_dir}\")\n",
    "print(f\"이미지 저장 경로: {image_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"테스트 데이터 크기: {len(test_df)} 샘플\")\n",
    "print(f\"컬럼 목록: {test_df.columns.tolist()}\")\n",
    "print(\"\\n첫 5개 샘플:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 데이터 통계\n",
    "print(\"\\n데이터 통계:\")\n",
    "print(f\"- 총 샘플 수: {len(test_df)}\")\n",
    "print(f\"- 원본 텍스트 평균 길이: {test_df['original'].str.len().mean():.1f}\")\n",
    "print(f\"- 난독화 텍스트 평균 길이: {test_df['obfuscated'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c660f8",
   "metadata": {},
   "source": [
    "## 3. 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 이름 설정\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_1E4_PATH)\n",
    "print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83913712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name):\n",
    "    \"\"\"로라 모델을 로드합니다\"\"\"\n",
    "    print(f\"\\n{model_name} 모델 로딩 중...\")\n",
    "    \n",
    "    # 베이스 모델 로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 어댑터 적용\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"{model_name} 모델 로딩 완료\")\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"원본 베이스 모델을 로드합니다\"\"\"\n",
    "    print(\"\\n원본 모델 로딩 중...\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    print(\"원본 모델 로딩 완료\")\n",
    "    return base_model\n",
    "\n",
    "# 모델 로드\n",
    "base_model = load_base_model()\n",
    "model_1e4 = load_model(MODEL_1E4_PATH, \"1e-4 Learning Rate 모델\")\n",
    "model_5e4 = load_model(MODEL_5E4_PATH, \"5e-4 Learning Rate 모델\")\n",
    "\n",
    "print(\"모든 모델 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb373",
   "metadata": {},
   "source": [
    "## 4. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, obfuscated_text, max_length=256):\n",
    "    \"\"\"난독화된 텍스트를 입력받아 원본 텍스트 생성\"\"\"\n",
    "    prompt = f\"\"\"### 지시사항:\n",
    "다음 난독화된 한국어 텍스트를 원래 텍스트로 복원해주세요.\n",
    "\n",
    "난독화된 텍스트: {obfuscated_text}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 응답 부분만 추출\n",
    "    if \"### 응답:\" in response:\n",
    "        response = response.split(\"### 응답:\")[1].strip()\n",
    "        # 불필요한 부분 제거\n",
    "        if \"<|endoftext|>\" in response:\n",
    "            response = response.split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "print(\"추론 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaac22",
   "metadata": {},
   "source": [
    "## 5. 성능 평가 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 메트릭 로드\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def calculate_character_accuracy(pred, ref):\n",
    "    \"\"\"문자 단위 정확도 계산\"\"\"\n",
    "    if len(ref) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    \n",
    "    # 정확히 일치하는 문자 수 계산\n",
    "    matches = sum(1 for i, char in enumerate(pred) if i < len(ref) and char == ref[i])\n",
    "    return matches / len(ref)\n",
    "\n",
    "def calculate_exact_match(pred, ref):\n",
    "    \"\"\"완전 일치 여부\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"모든 메트릭 계산\"\"\"\n",
    "    # BLEU 계산\n",
    "    try:\n",
    "        bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    except:\n",
    "        rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    \n",
    "    # 문자 정확도 계산\n",
    "    char_accuracies = [calculate_character_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    avg_char_accuracy = np.mean(char_accuracies)\n",
    "    \n",
    "    # 완전 일치율 계산\n",
    "    exact_matches = [calculate_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    exact_match_rate = np.mean(exact_matches)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'char_accuracy': avg_char_accuracy,\n",
    "        'exact_match': exact_match_rate,\n",
    "        'char_accuracies': char_accuracies,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "print(\"평가 메트릭 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3322",
   "metadata": {},
   "source": [
    "## 6. 모델 성능 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_df, sample_size=None):\n",
    "    \"\"\"모델 성능 평가\"\"\"\n",
    "    if sample_size:\n",
    "        test_data = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        test_data = test_df.copy()\n",
    "    \n",
    "    print(f\"\\n{model_name} 평가 시작 ({len(test_data)}개 샘플)\")\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"{model_name} 평가\"):\n",
    "        obfuscated = row['obfuscated']\n",
    "        pred, inf_time = generate_deobfuscated_text(model, obfuscated)\n",
    "        predictions.append(pred)\n",
    "        inference_times.append(inf_time)\n",
    "    \n",
    "    # 참조 텍스트\n",
    "    references = test_data['original'].tolist()\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # 추론 시간 통계\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    total_inference_time = np.sum(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'inference_times': inference_times,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'total_inference_time': total_inference_time,\n",
    "        'test_data': test_data,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} 평가 완료\")\n",
    "    print(f\"평균 추론 시간: {avg_inference_time:.3f}초\")\n",
    "    print(f\"총 추론 시간: {total_inference_time:.1f}초\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 평가 실행 (전체 데이터셋 또는 샘플)\n",
    "SAMPLE_SIZE = 200  # 전체 평가를 원하면 None으로 설정\n",
    "\n",
    "print(\"모델 성능 평가를 시작합니다...\")\n",
    "results_base = evaluate_model(base_model, \"원본 모델\", test_df, SAMPLE_SIZE)\n",
    "results_1e4 = evaluate_model(model_1e4, \"1e-4 Learning Rate 모델\", test_df, SAMPLE_SIZE)\n",
    "results_5e4 = evaluate_model(model_5e4, \"5e-4 Learning Rate 모델\", test_df, SAMPLE_SIZE)\n",
    "\n",
    "print(\"\\n모든 모델 평가 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18542a41",
   "metadata": {},
   "source": [
    "## 7. 성능 비교 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 표 생성\n",
    "comparison_df = pd.DataFrame({\n",
    "    '메트릭': ['BLEU 점수', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
    "               '문자 정확도', '정확 일치율', '평균 추론 시간(초)'],\n",
    "    '원본 모델': [\n",
    "        f\"{results_base['bleu']:.4f}\",\n",
    "        f\"{results_base['rouge1']:.4f}\",\n",
    "        f\"{results_base['rouge2']:.4f}\",\n",
    "        f\"{results_base['rougeL']:.4f}\",\n",
    "        f\"{results_base['char_accuracy']:.4f}\",\n",
    "        f\"{results_base['exact_match']:.4f}\",\n",
    "        f\"{results_base['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '1e-4 Learning Rate 모델': [\n",
    "        f\"{results_1e4['bleu']:.4f}\",\n",
    "        f\"{results_1e4['rouge1']:.4f}\",\n",
    "        f\"{results_1e4['rouge2']:.4f}\",\n",
    "        f\"{results_1e4['rougeL']:.4f}\",\n",
    "        f\"{results_1e4['char_accuracy']:.4f}\",\n",
    "        f\"{results_1e4['exact_match']:.4f}\",\n",
    "        f\"{results_1e4['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '5e-4 Learning Rate 모델': [\n",
    "        f\"{results_5e4['bleu']:.4f}\",\n",
    "        f\"{results_5e4['rouge1']:.4f}\",\n",
    "        f\"{results_5e4['rouge2']:.4f}\",\n",
    "        f\"{results_5e4['rougeL']:.4f}\",\n",
    "        f\"{results_5e4['char_accuracy']:.4f}\",\n",
    "        f\"{results_5e4['exact_match']:.4f}\",\n",
    "        f\"{results_5e4['avg_inference_time']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== 모델 성능 비교 결과 ===\\n(원본 vs 1e-4 vs 5e-4 Learning Rate)\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 원본 모델 대비 미세조정 모델 성능 개선률 계산\n",
    "print(\"\\n=== 원본 모델 대비 미세조정 모델 성능 개선율 ===\")\n",
    "metrics_to_compare = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy', 'exact_match']\n",
    "print(f\"{'Metric':<15} {'1e-4 vs 원본':<20} {'5e-4 vs 원본':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for metric in metrics_to_compare:\n",
    "    improvement_1e4 = ((results_1e4[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    improvement_5e4 = ((results_5e4[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    print(f\"{metric.upper():<15} {improvement_1e4:+7.2f}%          {improvement_5e4:+7.2f}%\")\n",
    "\n",
    "# 1e-4 vs 5e-4 모델 비교\n",
    "print(\"\\n=== 5e-4 모델 vs 1e-4 모델 성능 개선율 ===\")\n",
    "for metric in metrics_to_compare:\n",
    "    improvement = ((results_5e4[metric] - results_1e4[metric]) / results_1e4[metric]) * 100 if results_1e4[metric] > 0 else 0\n",
    "    print(f\"{metric.upper()}: {improvement:+.2f}%\")\n",
    "\n",
    "# 추론 시간 비율 비교\n",
    "time_ratio_base_1e4 = results_1e4['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_base_5e4 = results_5e4['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_1e4_5e4 = results_5e4['avg_inference_time'] / results_1e4['avg_inference_time']\n",
    "\n",
    "print(f\"\\n=== 추론 시간 비율 비교 ===\")\n",
    "print(f\"추론 시간 비율 (1e-4/원본): {time_ratio_base_1e4:.2f}x\")\n",
    "print(f\"추론 시간 비율 (5e-4/원본): {time_ratio_base_5e4:.2f}x\")\n",
    "print(f\"추론 시간 비율 (5e-4/1e-4): {time_ratio_1e4_5e4:.2f}x\")\n",
    "\n",
    "# 전반적인 성능 요약\n",
    "best_finetuned_accuracy = max(results_1e4['char_accuracy'], results_5e4['char_accuracy'])\n",
    "best_model_name = \"1e-4 Learning Rate\" if results_1e4['char_accuracy'] > results_5e4['char_accuracy'] else \"5e-4 Learning Rate\"\n",
    "accuracy_improvement = ((best_finetuned_accuracy - results_base['char_accuracy']) / results_base['char_accuracy'] * 100)\n",
    "\n",
    "print(f\"\\n=== 전반적 성능 요약 ===\")\n",
    "print(f\"가장 우수한 모델: {best_model_name} 모델\")\n",
    "print(f\"원본 모델 대비 최대 성능 향상: {accuracy_improvement:.2f}%\")\n",
    "print(f\"미세조정 효과: {'significant' if accuracy_improvement > 10 else 'moderate' if accuracy_improvement > 5 else 'limited'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540b5d9",
   "metadata": {},
   "source": [
    "## 8. 시각화 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metric Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Analysis (Base vs Fine-tuned with Different Learning Rates)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_data = {\n",
    "    'BLEU': [results_base['bleu'], results_1e4['bleu'], results_5e4['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_1e4['rouge1'], results_5e4['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_1e4['rouge2'], results_5e4['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_1e4['rougeL'], results_5e4['rougeL']],\n",
    "    'Character Accuracy': [results_base['char_accuracy'], results_1e4['char_accuracy'], results_5e4['char_accuracy']],\n",
    "    'Exact Match': [results_base['exact_match'], results_1e4['exact_match'], results_5e4['exact_match']]\n",
    "}\n",
    "\n",
    "models = ['Base Model', '1e-4 LR Model', '5e-4 LR Model']\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "\n",
    "for idx, (metric, values) in enumerate(metrics_data.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    bars = axes[row, col].bar(models, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{metric}', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Score')\n",
    "    axes[row, col].set_ylim(0, max(values) * 1.1)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Display values\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '01_learning_rate_performance_comparison.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Character Accuracy Distribution Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 원본 모델\n",
    "axes[0].hist(results_base['char_accuracies'], bins=20, alpha=0.7, color='lightgray', edgecolor='black')\n",
    "axes[0].set_title('Base Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Character Accuracy')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(results_base['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_base[\"char_accuracy\"]:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# 1e-4 모델\n",
    "axes[1].hist(results_1e4['char_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('1e-4 Learning Rate Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Character Accuracy')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(results_1e4['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_1e4[\"char_accuracy\"]:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# 5e-4 모델\n",
    "axes[2].hist(results_5e4['char_accuracies'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title('5e-4 Learning Rate Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[2].set_xlabel('Character Accuracy')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(results_5e4['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_5e4[\"char_accuracy\"]:.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '02_character_accuracy_distribution.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference Time Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Inference time distribution\n",
    "inference_data = [results_base['inference_times'], results_1e4['inference_times'], results_5e4['inference_times']]\n",
    "labels = ['Base Model', '1e-4 LR Model', '5e-4 LR Model']\n",
    "\n",
    "axes[0].boxplot(inference_data, labels=labels, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[0].set_title('Inference Time Distribution Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('Inference Time (seconds)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average inference time bar chart\n",
    "avg_times = [results_base['avg_inference_time'], results_1e4['avg_inference_time'], results_5e4['avg_inference_time']]\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "bars = axes[1].bar(labels, avg_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Inference Time Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('Average Inference Time (seconds)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, avg_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '03_inference_time_comparison.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c6546",
   "metadata": {},
   "source": [
    "## 9. 질적 분석 - 예시 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 성능 차이가 큰 샘플들 찾기\n",
    "def find_performance_difference_samples(results_base, results_1e4, results_5e4, n_samples=5):\n",
    "    \"\"\"세 모델 간 성능 차이가 큰 샘플들 찾기\"\"\"\n",
    "    char_acc_base = np.array(results_base['char_accuracies'])\n",
    "    char_acc_1e4 = np.array(results_1e4['char_accuracies'])\n",
    "    char_acc_5e4 = np.array(results_5e4['char_accuracies'])\n",
    "    \n",
    "    # 미세조정 효과 (최고 성능 모델 vs 원본)\n",
    "    max_finetuned = np.maximum(char_acc_1e4, char_acc_5e4)\n",
    "    finetuning_improvement = max_finetuned - char_acc_base\n",
    "    \n",
    "    # 5e-4 vs 1e-4 비교\n",
    "    diff_5e4_1e4 = char_acc_5e4 - char_acc_1e4\n",
    "    \n",
    "    # 가장 미세조정 효과가 큰 인덱스들\n",
    "    best_finetuning_idx = np.argsort(finetuning_improvement)[-n_samples:][::-1]\n",
    "    worst_finetuning_idx = np.argsort(finetuning_improvement)[:n_samples]\n",
    "    \n",
    "    # 5e-4가 1e-4보다 훨씬 좋은/나쁜 경우\n",
    "    best_5e4_vs_1e4_idx = np.argsort(diff_5e4_1e4)[-n_samples:][::-1]\n",
    "    worst_5e4_vs_1e4_idx = np.argsort(diff_5e4_1e4)[:n_samples]\n",
    "    \n",
    "    return best_finetuning_idx, worst_finetuning_idx, best_5e4_vs_1e4_idx, worst_5e4_vs_1e4_idx\n",
    "\n",
    "best_ft_idx, worst_ft_idx, best_5e4_idx, worst_5e4_idx = find_performance_difference_samples(results_base, results_1e4, results_5e4)\n",
    "\n",
    "print(\"=== 미세조정이 원본 모델 대비 가장 효과적이었던 예시 ===\")\n",
    "for i, idx in enumerate(best_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"1e-4 모델 예측: {results_1e4['predictions'][idx]}\")\n",
    "    print(f\"5e-4 모델 예측: {results_5e4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 1e-4: {results_1e4['char_accuracies'][idx]:.3f}, 5e-4: {results_5e4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 5e-4 모델이 1e-4 모델 대비 크게 우수했던 예시 ===\")\n",
    "for i, idx in enumerate(best_5e4_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_1e4['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_1e4['references'][idx]}\")\n",
    "    print(f\"1e-4 모델 예측: {results_1e4['predictions'][idx]}\")\n",
    "    print(f\"5e-4 모델 예측: {results_5e4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 1e-4: {results_1e4['char_accuracies'][idx]:.3f}, 5e-4: {results_5e4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 미세조정 효과가 제한적이었던 예시 ===\")\n",
    "for i, idx in enumerate(worst_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"1e-4 모델 예측: {results_1e4['predictions'][idx]}\")\n",
    "    print(f\"5e-4 모델 예측: {results_5e4['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 1e-4: {results_1e4['char_accuracies'][idx]:.3f}, 5e-4: {results_5e4['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "# 전반적인 성능 비교 통계\n",
    "print(\"\\n=== 전반적인 성능 비교 통계 ===\")\n",
    "char_acc_base = np.array(results_base['char_accuracies'])\n",
    "char_acc_1e4 = np.array(results_1e4['char_accuracies'])\n",
    "char_acc_5e4 = np.array(results_5e4['char_accuracies'])\n",
    "\n",
    "# 원본 대비 미세조정 모델 성능\n",
    "better_1e4_vs_base = np.sum(char_acc_1e4 > char_acc_base)\n",
    "better_5e4_vs_base = np.sum(char_acc_5e4 > char_acc_base)\n",
    "\n",
    "# 1e-4 vs 5e-4 비교\n",
    "better_5e4_vs_1e4 = np.sum(char_acc_5e4 > char_acc_1e4)\n",
    "better_1e4_vs_5e4 = np.sum(char_acc_1e4 > char_acc_5e4)\n",
    "tie_1e4_5e4 = np.sum(char_acc_1e4 == char_acc_5e4)\n",
    "\n",
    "print(f\"1e-4 모델 > 원본 모델: {better_1e4_vs_base}개 ({better_1e4_vs_base/len(char_acc_base)*100:.1f}%)\")\n",
    "print(f\"5e-4 모델 > 원본 모델: {better_5e4_vs_base}개 ({better_5e4_vs_base/len(char_acc_base)*100:.1f}%)\")\n",
    "print(f\"5e-4 모델 > 1e-4 모델: {better_5e4_vs_1e4}개 ({better_5e4_vs_1e4/len(char_acc_1e4)*100:.1f}%)\")\n",
    "print(f\"1e-4 모델 > 5e-4 모델: {better_1e4_vs_5e4}개 ({better_1e4_vs_5e4/len(char_acc_1e4)*100:.1f}%)\")\n",
    "print(f\"동점: {tie_1e4_5e4}개 ({tie_1e4_5e4/len(char_acc_1e4)*100:.1f}%)\")\n",
    "\n",
    "avg_improvement_1e4 = np.mean(char_acc_1e4 - char_acc_base)\n",
    "avg_improvement_5e4 = np.mean(char_acc_5e4 - char_acc_base)\n",
    "print(f\"\\n1e-4 모델의 원본 대비 평균 성능 개선: {avg_improvement_1e4:.4f} ({avg_improvement_1e4*100:.2f}%p)\")\n",
    "print(f\"5e-4 모델의 원본 대비 평균 성능 개선: {avg_improvement_5e4:.4f} ({avg_improvement_5e4*100:.2f}%p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91017b0",
   "metadata": {},
   "source": [
    "## 10. 상세 분석 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이별 성능 분석\n",
    "def analyze_by_text_length(results, model_name):\n",
    "    \"\"\"텍스트 길이별 성능 분석\"\"\"\n",
    "    test_data = results['test_data']\n",
    "    char_accuracies = results['char_accuracies']\n",
    "    \n",
    "    # 텍스트 길이 계산\n",
    "    text_lengths = test_data['original'].str.len()\n",
    "    \n",
    "    # 길이 구간별로 분류\n",
    "    length_bins = [0, 20, 50, 100, 200, float('inf')]\n",
    "    length_labels = ['≤20 chars', '21-50 chars', '51-100 chars', '101-200 chars', '200+ chars']\n",
    "    \n",
    "    length_categories = pd.cut(text_lengths, bins=length_bins, labels=length_labels, right=False)\n",
    "    \n",
    "    # 구간별 평균 성능\n",
    "    performance_by_length = []\n",
    "    for category in length_labels:\n",
    "        mask = length_categories == category\n",
    "        if mask.sum() > 0:\n",
    "            avg_acc = np.mean(np.array(char_accuracies)[mask])\n",
    "            count = mask.sum()\n",
    "            performance_by_length.append({\n",
    "                'length_category': category,\n",
    "                'avg_char_accuracy': avg_acc,\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(performance_by_length)\n",
    "\n",
    "# 세 모델의 길이별 성능 분석\n",
    "length_analysis_base = analyze_by_text_length(results_base, \"원본 모델\")\n",
    "length_analysis_1e4 = analyze_by_text_length(results_1e4, \"1e-4 모델\")\n",
    "length_analysis_5e4 = analyze_by_text_length(results_5e4, \"5e-4 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(length_analysis_base))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, length_analysis_base['avg_char_accuracy'], width, \n",
    "               label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, length_analysis_1e4['avg_char_accuracy'], width,\n",
    "               label='1e-4 LR Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, length_analysis_5e4['avg_char_accuracy'], width,\n",
    "               label='5e-4 LR Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Text Length Category')\n",
    "ax.set_ylabel('Average Character Accuracy')\n",
    "ax.set_title('Model Performance Comparison by Text Length (Base vs Fine-tuned with Different LR)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(length_analysis_base['length_category'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '04_performance_by_text_length.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 텍스트 길이별 성능 분석 결과 ===\")\n",
    "combined_length_analysis = pd.merge(\n",
    "    pd.merge(length_analysis_base, length_analysis_1e4, on='length_category', suffixes=('_base', '_1e4')),\n",
    "    length_analysis_5e4, on='length_category'\n",
    ")\n",
    "combined_length_analysis.columns = ['length_category', 'avg_char_accuracy_base', 'count_base', \n",
    "                                   'avg_char_accuracy_1e4', 'count_1e4', 'avg_char_accuracy_5e4', 'count_5e4']\n",
    "print(combined_length_analysis[['length_category', 'avg_char_accuracy_base', 'avg_char_accuracy_1e4', 'avg_char_accuracy_5e4']])\n",
    "\n",
    "# 길이별 미세조정 효과 분석\n",
    "print(\"\\n=== 길이별 미세조정 효과 ===\")\n",
    "for _, row in combined_length_analysis.iterrows():\n",
    "    category = row['length_category']\n",
    "    base_acc = row['avg_char_accuracy_base']\n",
    "    acc_1e4 = row['avg_char_accuracy_1e4']\n",
    "    acc_5e4 = row['avg_char_accuracy_5e4']\n",
    "    \n",
    "    improvement_1e4 = ((acc_1e4 - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    improvement_5e4 = ((acc_5e4 - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    \n",
    "    print(f\"{category}: 1e-4 개선 {improvement_1e4:+.1f}%, 5e-4 개선 {improvement_5e4:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 일치 및 부분 일치 분석\n",
    "def analyze_match_types(results, model_name):\n",
    "    \"\"\"완전 일치 및 부분 일치 분석\"\"\"\n",
    "    predictions = results['predictions']\n",
    "    references = results['references']\n",
    "    \n",
    "    perfect_matches = 0\n",
    "    high_accuracy = 0  # 90% 이상\n",
    "    medium_accuracy = 0  # 70-90%\n",
    "    low_accuracy = 0  # 70% 미만\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        char_acc = calculate_character_accuracy(pred, ref)\n",
    "        \n",
    "        if pred.strip() == ref.strip():\n",
    "            perfect_matches += 1\n",
    "        elif char_acc >= 0.9:\n",
    "            high_accuracy += 1\n",
    "        elif char_acc >= 0.7:\n",
    "            medium_accuracy += 1\n",
    "        else:\n",
    "            low_accuracy += 1\n",
    "    \n",
    "    total = len(predictions)\n",
    "    \n",
    "    return {\n",
    "        'perfect_match': perfect_matches,\n",
    "        'high_accuracy': high_accuracy,\n",
    "        'medium_accuracy': medium_accuracy,\n",
    "        'low_accuracy': low_accuracy,\n",
    "        'perfect_match_rate': perfect_matches / total,\n",
    "        'high_accuracy_rate': high_accuracy / total,\n",
    "        'medium_accuracy_rate': medium_accuracy / total,\n",
    "        'low_accuracy_rate': low_accuracy / total\n",
    "    }\n",
    "\n",
    "match_analysis_base = analyze_match_types(results_base, \"원본 모델\")\n",
    "match_analysis_1e4 = analyze_match_types(results_1e4, \"1e-4 모델\")\n",
    "match_analysis_5e4 = analyze_match_types(results_5e4, \"5e-4 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "categories = ['Perfect Match', 'High Accuracy\\n(90%+)', 'Medium Accuracy\\n(70-90%)', 'Low Accuracy\\n(<70%)']\n",
    "values_base = [match_analysis_base['perfect_match_rate'], \n",
    "               match_analysis_base['high_accuracy_rate'],\n",
    "               match_analysis_base['medium_accuracy_rate'], \n",
    "               match_analysis_base['low_accuracy_rate']]\n",
    "values_1e4 = [match_analysis_1e4['perfect_match_rate'], \n",
    "              match_analysis_1e4['high_accuracy_rate'],\n",
    "              match_analysis_1e4['medium_accuracy_rate'], \n",
    "              match_analysis_1e4['low_accuracy_rate']]\n",
    "values_5e4 = [match_analysis_5e4['perfect_match_rate'], \n",
    "              match_analysis_5e4['high_accuracy_rate'],\n",
    "              match_analysis_5e4['medium_accuracy_rate'], \n",
    "              match_analysis_5e4['low_accuracy_rate']]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, values_base, width, label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, values_1e4, width, label='1e-4 LR Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, values_5e4, width, label='5e-4 LR Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Accuracy Category')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Sample Distribution by Accuracy Category (Base vs Fine-tuned with Different LR)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars, values in zip([bars1, bars2, bars3], [values_base, values_1e4, values_5e4]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# 이미지 저장\n",
    "img_path = os.path.join(image_save_dir, '05_accuracy_category_distribution.png')\n",
    "plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"이미지 저장: {img_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 정확도 카테고리별 분석 결과 ===\")\n",
    "print(f\"{'Category':<20} {'Base Model':<15} {'1e-4 LR Model':<15} {'5e-4 LR Model':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for i, category in enumerate(categories):\n",
    "    print(f\"{category:<20} {values_base[i]:<15.1%} {values_1e4[i]:<15.1%} {values_5e4[i]:<15.1%}\")\n",
    "\n",
    "print(\"\\n=== 원본 모델 대비 개선율 ===\")\n",
    "print(f\"{'Category':<20} {'1e-4 Improvement':<20} {'5e-4 Improvement':<20}\")\n",
    "print(\"-\" * 65)\n",
    "for i, category in enumerate(categories):\n",
    "    improvement_1e4 = ((values_1e4[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_1e4[i] > 0 else 0\n",
    "    improvement_5e4 = ((values_5e4[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_5e4[i] > 0 else 0\n",
    "    \n",
    "    if improvement_1e4 == float('inf'):\n",
    "        imp_1e4_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_1e4_str = f\"{improvement_1e4:+.1f}%\"\n",
    "        \n",
    "    if improvement_5e4 == float('inf'):\n",
    "        imp_5e4_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_5e4_str = f\"{improvement_5e4:+.1f}%\"\n",
    "    \n",
    "    print(f\"{category:<20} {imp_1e4_str:<20} {imp_5e4_str:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5877e",
   "metadata": {},
   "source": [
    "## 11. 종합 결론 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dcacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"🔍 HyperCLOVAX Learning Rate 비교 분석 - 종합 결론 (원본 vs 1e-4 vs 5e-4)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 **모델 성능 비교 (원본 vs Learning Rate 미세조정)**\n",
    "\n",
    "📈 **BLEU 점수**\n",
    "- 원본: {results_base['bleu']:.4f}\n",
    "- 1e-4: {results_1e4['bleu']:.4f} (개선율: {((results_1e4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "- 5e-4: {results_5e4['bleu']:.4f} (개선율: {((results_5e4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "\n",
    "📈 **ROUGE-L 점수**\n",
    "- 원본: {results_base['rougeL']:.4f}\n",
    "- 1e-4: {results_1e4['rougeL']:.4f} (개선율: {((results_1e4['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "- 5e-4: {results_5e4['rougeL']:.4f} (개선율: {((results_5e4['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "\n",
    "📈 **문자 정확도**\n",
    "- 원본: {results_base['char_accuracy']:.4f}\n",
    "- 1e-4: {results_1e4['char_accuracy']:.4f} (개선율: {((results_1e4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "- 5e-4: {results_5e4['char_accuracy']:.4f} (개선율: {((results_5e4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "\n",
    "📈 **완전 일치율**\n",
    "- 원본: {results_base['exact_match']:.4f}\n",
    "- 1e-4: {results_1e4['exact_match']:.4f} (개선율: {((results_1e4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "- 5e-4: {results_5e4['exact_match']:.4f} (개선율: {((results_5e4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "\n",
    "⏱️ **효율성 분석**\n",
    "- 원본 모델 평균 추론 시간: {results_base['avg_inference_time']:.3f}초\n",
    "- 1e-4 모델 평균 추론 시간: {results_1e4['avg_inference_time']:.3f}초 ({results_1e4['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "- 5e-4 모델 평균 추론 시간: {results_5e4['avg_inference_time']:.3f}초 ({results_5e4['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "\n",
    "💯 **복원 품질 분석**\n",
    "- 원본 모델 완전 일치: {match_analysis_base['perfect_match']}개 ({match_analysis_base['perfect_match_rate']:.1%})\n",
    "- 1e-4 모델 완전 일치: {match_analysis_1e4['perfect_match']}개 ({match_analysis_1e4['perfect_match_rate']:.1%})\n",
    "- 5e-4 모델 완전 일치: {match_analysis_5e4['perfect_match']}개 ({match_analysis_5e4['perfect_match_rate']:.1%})\n",
    "\n",
    "🎯 **핵심 인사이트**\n",
    "1. 미세조정이 원본 모델 대비 모든 지표에서 현저한 성능 향상을 가져옴\n",
    "2. {'5e-4 learning rate가 1e-4보다 전반적으로 우수한 성능을 보임' if results_5e4['char_accuracy'] > results_1e4['char_accuracy'] else '1e-4 learning rate가 5e-4보다 전반적으로 우수한 성능을 보임'}\n",
    "3. 완전 일치율에서 가장 드라마틱한 개선을 확인 (정확한 복원 능력 향상)\n",
    "4. 추론 시간 증가는 미미하여 효율성 저하 없이 성능 향상 달성\n",
    "5. 다양한 텍스트 길이에서 안정적인 성능 향상 확인\n",
    "\n",
    "💡 **권장 사항**\n",
    "- {'5e-4' if results_5e4['char_accuracy'] > results_1e4['char_accuracy'] else '1e-4'} learning rate 모델 사용 강력 권장\n",
    "- 더 많은 데이터로 추가 학습 시 더 큰 성능 향상 기대 가능\n",
    "- 원본 모델의 제한적 성능을 고려할 때 미세조정의 효과가 매우 의미 있음\n",
    "- Learning rate 최적화를 통한 추가 성능 향상 가능성 탐색\n",
    "- 도메인 특화 작업에서 미세조정의 중요성 입증\n",
    "\"\"\") \n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd32b3",
   "metadata": {},
   "source": [
    "## 12. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV 파일로 저장 (3개 모델 비교)\n",
    "results_summary = pd.DataFrame({\n",
    "    '모델': ['원본 모델', '1e-4 Learning Rate 모델', '5e-4 Learning Rate 모델'],\n",
    "    'BLEU 점수': [results_base['bleu'], results_1e4['bleu'], results_5e4['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_1e4['rouge1'], results_5e4['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_1e4['rouge2'], results_5e4['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_1e4['rougeL'], results_5e4['rougeL']],\n",
    "    '문자 정확도': [results_base['char_accuracy'], results_1e4['char_accuracy'], results_5e4['char_accuracy']],\n",
    "    '정확 일치율': [results_base['exact_match'], results_1e4['exact_match'], results_5e4['exact_match']],\n",
    "    '평균 추론 시간': [results_base['avg_inference_time'], results_1e4['avg_inference_time'], results_5e4['avg_inference_time']],\n",
    "    '총 추론 시간': [results_base['total_inference_time'], results_1e4['total_inference_time'], results_5e4['total_inference_time']]\n",
    "})\n",
    "\n",
    "# 상세 결과도 저장 (3개 모델 포함)\n",
    "detailed_results = pd.DataFrame({\n",
    "    '인덱스': range(len(results_base['predictions'])),\n",
    "    '원본': results_base['references'],\n",
    "    '난독화': results_base['test_data']['obfuscated'].tolist(),\n",
    "    '예측_원본': results_base['predictions'],\n",
    "    '예측_1e4': results_1e4['predictions'],\n",
    "    '예측_5e4': results_5e4['predictions'],\n",
    "    '문자_정확도_원본': results_base['char_accuracies'],\n",
    "    '문자_정확도_1e4': results_1e4['char_accuracies'],\n",
    "    '문자_정확도_5e4': results_5e4['char_accuracies'],\n",
    "    '정확_일치_원본': results_base['exact_matches'],\n",
    "    '정확_일치_1e4': results_1e4['exact_matches'],\n",
    "    '정확_일치_5e4': results_5e4['exact_matches'],\n",
    "    '추론_시간_원본': results_base['inference_times'],\n",
    "    '추론_시간_1e4': results_1e4['inference_times'],\n",
    "    '추론_시간_5e4': results_5e4['inference_times']\n",
    "})\n",
    "\n",
    "# 미세조정 효과 분석 결과 저장\n",
    "finetuning_analysis = pd.DataFrame({\n",
    "    '비교': ['1e-4 vs 원본', '5e-4 vs 원본', '5e-4 vs 1e-4'],\n",
    "    'BLEU_개선율': [\n",
    "        ((results_1e4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_5e4['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_5e4['bleu'] - results_1e4['bleu']) / results_1e4['bleu'] * 100) if results_1e4['bleu'] > 0 else 0\n",
    "    ],\n",
    "    '문자정확도_개선율': [\n",
    "        ((results_1e4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_5e4['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_5e4['char_accuracy'] - results_1e4['char_accuracy']) / results_1e4['char_accuracy'] * 100)\n",
    "    ],\n",
    "    '완전일치율_개선율': [\n",
    "        ((results_1e4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_5e4['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_5e4['exact_match'] - results_1e4['exact_match']) / results_1e4['exact_match'] * 100) if results_1e4['exact_match'] > 0 else float('inf')\n",
    "    ],\n",
    "    '추론시간_비율': [\n",
    "        results_1e4['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "        results_5e4['avg_inference_time'] / results_base['avg_inference_time'],\n",
    "        results_5e4['avg_inference_time'] / results_1e4['avg_inference_time']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# CSV 파일 저장 경로 (분석 결과 폴더 내)\n",
    "csv1_path = os.path.join(analysis_root_dir, 'model_performance_summary_with_base.csv')\n",
    "csv2_path = os.path.join(analysis_root_dir, 'detailed_model_comparison_with_base.csv')\n",
    "csv3_path = os.path.join(analysis_root_dir, 'finetuning_effect_analysis.csv')\n",
    "\n",
    "results_summary.to_csv(csv1_path, index=False, encoding='utf-8-sig')\n",
    "detailed_results.to_csv(csv2_path, index=False, encoding='utf-8-sig')\n",
    "finetuning_analysis.to_csv(csv3_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"📁 결과 파일 저장 완료:\")\n",
    "print(f\"- {csv1_path}\")\n",
    "print(f\"- {csv2_path}\")\n",
    "print(f\"- {csv3_path}\")\n",
    "\n",
    "# 시각화 이미지 목록 출력\n",
    "print(\"\\n📊 생성된 시각화 이미지:\")\n",
    "image_files = os.listdir(image_save_dir)\n",
    "image_files = [f for f in image_files if f.endswith('.png')]\n",
    "for i, img_file in enumerate(sorted(image_files), 1):\n",
    "    print(f\"{i}. {os.path.join(image_save_dir, img_file)}\")\n",
    "\n",
    "# 결과 파일들을 압축하여 다운로드 준비\n",
    "import zipfile\n",
    "zip_filename = os.path.join(analysis_root_dir, 'HyperCLOVAX_3Model_Analysis_Complete_Results.zip')\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # CSV 파일들 추가\n",
    "    zipf.write(csv1_path, 'results/model_performance_summary_with_base.csv')\n",
    "    zipf.write(csv2_path, 'results/detailed_model_comparison_with_base.csv')\n",
    "    zipf.write(csv3_path, 'results/finetuning_effect_analysis.csv')\n",
    "    \n",
    "    # 이미지 파일들 추가\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_save_dir, img_file)\n",
    "        zipf.write(img_path, f'visualizations/{img_file}')\n",
    "\n",
    "print(f\"\\n📦 압축 파일 생성: {zip_filename}\")\n",
    "print(\"압축 파일 내용:\")\n",
    "print(\"  📁 results/ - CSV 분석 결과 파일들 (3개 모델 비교)\")\n",
    "print(\"  📁 visualizations/ - 시각화 이미지들 (3개 모델 비교)\")\n",
    "\n",
    "# Google Colab에서 다운로드 시도\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_filename)\n",
    "    print(\"📥 압축 파일 다운로드 완료\")\n",
    "except ImportError:\n",
    "    print(\"💾 로컬 환경에서는 다음 경로에 모든 파일이 저장되었습니다:\")\n",
    "    print(f\"   분석 결과 폴더: {analysis_root_dir}\")\n",
    "    print(f\"   압축 파일: {zip_filename}\")\n",
    "    print(f\"   이미지 폴더: {image_save_dir}\")\n",
    "\n",
    "# Google Drive 폴더 구조 안내\n",
    "print(f\"\\n📂 Google Drive 폴더 구조:\")\n",
    "print(f\"MyDrive/\")\n",
    "print(f\"├── HyperCLOVAX_LearningRate_Analysis_Results/\")\n",
    "print(f\"│   ├── model_performance_summary_with_base.csv      # 3개 모델 성능 요약\")\n",
    "print(f\"│   ├── detailed_model_comparison_with_base.csv      # 3개 모델 상세 비교\")\n",
    "print(f\"│   ├── finetuning_effect_analysis.csv               # 미세조정 효과 분석\")\n",
    "print(f\"│   ├── HyperCLOVAX_3Model_Analysis_Complete_Results.zip\")\n",
    "print(f\"│   └── visualization_images/\")\n",
    "print(f\"│       ├── 01_model_performance_comparison.png      # 3개 모델 성능 비교\")\n",
    "print(f\"│       ├── 02_character_accuracy_distribution.png\")\n",
    "print(f\"│       ├── 03_inference_time_comparison.png\")\n",
    "print(f\"│       ├── 04_performance_by_text_length.png\")\n",
    "print(f\"│       └── 05_accuracy_category_distribution.png\")\n",
    "print(f\"├── hyperclova-deobfuscation-lora-1e-4-learning-rate/\")\n",
    "print(f\"├── hyperclova-deobfuscation-lora-5e-4-learning-rate/\")\n",
    "print(f\"└── testdata.csv\")\n",
    "\n",
    "# 결과 요약 출력 (3개 모델)\n",
    "print(\"\\n=== 최종 성능 요약 (원본 vs 1e-4 LR vs 5e-4 LR) ===\")\n",
    "print(results_summary.round(4))\n",
    "\n",
    "# 미세조정 효과 요약\n",
    "best_base_accuracy = results_base['char_accuracy']\n",
    "best_1e4_accuracy = results_1e4['char_accuracy']\n",
    "best_5e4_accuracy = results_5e4['char_accuracy']\n",
    "\n",
    "print(f\"\\n=== 미세조정 효과 분석 ===\")\n",
    "print(f\"원본 모델 문자 정확도: {best_base_accuracy:.4f}\")\n",
    "print(f\"1e-4 LR 모델 문자 정확도: {best_1e4_accuracy:.4f} ({((best_1e4_accuracy - best_base_accuracy) / best_base_accuracy * 100):+.2f}%)\")\n",
    "print(f\"5e-4 LR 모델 문자 정확도: {best_5e4_accuracy:.4f} ({((best_5e4_accuracy - best_base_accuracy) / best_base_accuracy * 100):+.2f}%)\")\n",
    "\n",
    "# 최고 성능 모델 식별\n",
    "if best_5e4_accuracy > best_1e4_accuracy and best_5e4_accuracy > best_base_accuracy:\n",
    "    best_model = \"5e-4 Learning Rate\"\n",
    "    best_accuracy = best_5e4_accuracy\n",
    "elif best_1e4_accuracy > best_base_accuracy:\n",
    "    best_model = \"1e-4 Learning Rate\"\n",
    "    best_accuracy = best_1e4_accuracy\n",
    "else:\n",
    "    best_model = \"원본\"\n",
    "    best_accuracy = best_base_accuracy\n",
    "\n",
    "print(f\"\\n최고 성능 모델: {best_model} 모델 (문자 정확도: {best_accuracy:.4f})\")\n",
    "\n",
    "if best_model != \"원본\":\n",
    "    improvement = ((best_accuracy - best_base_accuracy) / best_base_accuracy * 100)\n",
    "    print(f\"원본 모델 대비 성능 향상: {improvement:.2f}%\")\n",
    "    print(f\"미세조정 효과: {'매우 효과적' if improvement > 20 else '효과적' if improvement > 10 else '보통' if improvement > 5 else '제한적'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
