# π” Korean Text De-obfuscation Fine-tuning Project

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![HuggingFace](https://img.shields.io/badge/π¤—-HuggingFace-yellow)](https://huggingface.co/)

μ΄ ν”„λ΅μ νΈλ” **Naver HyperCLOVAX-SEED-Text-Instruct-0.5B** λ¨λΈμ„ ν•κµ­μ–΄ ν…μ¤νΈ λΉ„λ‚λ…ν™”(De-obfuscation) μ‘μ—…μ„ μ„ν•΄ fine-tuningν•λ” μ—°κµ¬ ν”„λ΅μ νΈμ…λ‹λ‹¤.

## π“‹ ν”„λ΅μ νΈ κ°μ”

### ν€ μ •λ³΄
- **ν€λ…**: 91veMe4Plus
- **ν”„λ΅μ νΈλ…**: ν•κµ­μ–΄ ν…μ¤νΈ λΉ„λ‚λ…ν™” AI λ¨λΈ μ„±λ¥ λ¶„μ„
- **λΌμ΄μ„ μ¤**: MIT License
- **μ €μ‘κ¶**: Copyright (c) 2025 91veMe4Plus

### λ©ν‘
- λ‚λ…ν™”λ ν•κµ­μ–΄ ν…μ¤νΈλ¥Ό μ›λ³Έ ν…μ¤νΈλ΅ λ³µμ›ν•λ” λ¨λΈ κ°λ°
- **LoRA (Low-Rank Adaptation)**λ¥Ό μ‚¬μ©ν• ν¨μ¨μ μΈ fine-tuning
- λ‹¤μ–‘ν• ν…μ¤νΈ μ ν•μ— λ€ν• μ„±λ¥ ν‰κ°€ λ° λΉ„κµ
- ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό ν†µν• λ¨λΈ μ„±λ¥ λ¶„μ„

## π― ν•µμ‹¬ λ©ν‘

1. **λ¨λΈ νμΈνλ‹**: Naver HyperCLOVAX-SEED-Text-Instruct-0.5B λ¨λΈμ„ ν•κµ­μ–΄ ν…μ¤νΈ λΉ„λ‚λ…ν™” μ‘μ—…μ— μµμ ν™”
2. **ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”**: Learning Rate, Batch Size, Dataset Size λ“± λ‹¤μ–‘ν• νλΌλ―Έν„°μ μν–¥ λ¶„μ„
3. **μ„±λ¥ λΉ„κµ λ¶„μ„**: μ›λ³Έ λ¨λΈ λ€λΉ„ νμΈνλ‹ λ¨λΈμ μ •λ‰μ /μ •μ„±μ  μ„±λ¥ κ°μ„  μΈ΅μ •
4. **ν¨μ¨μ  ν•™μµ κΈ°λ²•**: LoRA (Low-Rank Adaptation)λ¥Ό ν™μ©ν• νλΌλ―Έν„° ν¨μ¨μ  νμΈνλ‹

## π“ λ°μ΄ν„°μ…‹ μ •λ³΄

### ν›λ ¨ λ°μ΄ν„°μ…‹
6κ°€μ§€ ν•κµ­μ–΄ ν…μ¤νΈ μ ν•μ λ‚λ…ν™” λ°μ΄ν„° (μ΄ 696,024 μƒν”):
- `κµ¬μ–΄μ²΄_λ€ν™”μ²΄_16878_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (16,878 μƒν”)
- `λ‰΄μ¤λ¬Έμ–΄μ²΄_281932_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (281,932 μƒν”)
- `λ¬Έν™”λ¬Έμ–΄μ²΄_25628_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (25,628 μƒν”)
- `μ „λ¬Έλ¶„μ•Ό λ¬Έμ–΄μ²΄_306542_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (306,542 μƒν”)
- `μ΅°λ΅€λ¬Έμ–΄μ²΄_36339_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (36,339 μƒν”)
- `μ§€μμ²΄μ›Ήμ‚¬μ΄νΈ λ¬Έμ–΄μ²΄_28705_sample_λ‚λ…ν™”κ²°κ³Ό.csv` (28,705 μƒν”)

### ν…μ¤νΈ λ°μ΄ν„°μ…‹
- `testdata.csv` (1,002 μƒν”)
- λ‚λ…ν™”λ ν…μ¤νΈμ™€ μ›λ³Έ ν…μ¤νΈ μμΌλ΅ κµ¬μ„±

## π”§ νμΈνλ‹ μ „λµ

### 1. λ¨λΈ μ•„ν‚¤ν…μ² λ° μ„¤μ •

#### λ² μ΄μ¤ λ¨λΈ
- **λ¨λΈλ…**: Naver HyperCLOVAX-SEED-Text-Instruct-0.5B
- **λ¨λΈ μ ν•**: Causal Language Model (Auto-regressive)
- **νλΌλ―Έν„° μ**: 0.5B (5μ–µ κ°)
- **ν† ν¬λ‚μ΄μ €**: HyperCLOVAX μ „μ© ν† ν¬λ‚μ΄μ €

#### μ–‘μν™” μ„¤μ • (λ©”λ¨λ¦¬ μµμ ν™”)
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

### 2. LoRA (Low-Rank Adaptation) κµ¬μ„±

#### LoRA ν•μ΄νΌνλΌλ―Έν„°
- **Rank (r)**: 16
- **Alpha**: 32 (scaling parameter)
- **Dropout**: 0.1
- **Target Modules**: 
  - `q_proj`, `k_proj`, `v_proj`, `o_proj` (Attention layers)
  - `gate_proj`, `up_proj`, `down_proj` (Feed-forward layers)
- **Task Type**: CAUSAL_LM
- **Bias**: "none"

#### LoRA μ„¤μ • μ½”λ“
```python
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
```

### 3. λ°μ΄ν„° μ „μ²λ¦¬ μ „λµ

#### ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
```
### μ§€μ‹μ‚¬ν•­:
λ‹¤μ λ‚λ…ν™”λ ν•κµ­μ–΄ ν…μ¤νΈλ¥Ό μ›λ ν…μ¤νΈλ΅ λ³µμ›ν•΄μ£Όμ„Έμ”.

λ‚λ…ν™”λ ν…μ¤νΈ: {obfuscated_text}

### μ‘λ‹µ:
{original_text}
```

#### λ°μ΄ν„° μƒν”λ§ μ „λµ
- **κ· ν• μƒν”λ§**: 6κ°€μ§€ ν…μ¤νΈ μ ν•μ—μ„ κ· λ“±ν•κ² μƒν”λ§
- **μµλ€ κΈΈμ΄**: 512 ν† ν°μΌλ΅ μ ν•
- **ν›λ ¨/κ²€μ¦ λ¶„ν• **: 9:1 λΉ„μ¨

### 4. ν›λ ¨ μ„¤μ •

#### κ³µν†µ ν›λ ¨ νλΌλ―Έν„°
- **Epochs**: 3
- **Gradient Accumulation Steps**: 4
- **Warmup Steps**: 100
- **Weight Decay**: 0.01
- **FP16**: True (Mixed Precision Training)
- **Evaluation Strategy**: Steps (λ§¤ 200 μ¤ν…)
- **Save Strategy**: Best model κΈ°μ¤€ μ €μ¥

#### μµμ ν™” μ•κ³ λ¦¬μ¦
- **Optimizer**: AdamW
- **Scheduler**: Linear Warmup + Decay

### 5. μ¶”λ΅  μ„¤μ •

#### μƒμ„± νλΌλ―Έν„°
- **Max New Tokens**: 128
- **Do Sample**: True
- **Temperature**: 0.7
- **Top-p**: 0.9
- **Repetition Penalty**: λ―Έμ μ©

### 6. μ„±λ¥ μµμ ν™” κΈ°λ²•

#### λ©”λ¨λ¦¬ μµμ ν™”
- **4-bit μ–‘μν™”**: BitsAndBytes ν™μ©
- **Gradient Checkpointing**: λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ°μ†
- **DataLoader Pin Memory**: False (Colab ν™κ²½ μµμ ν™”)

#### ν›λ ¨ μ•μ •μ„±
- **Learning Rate Warmup**: μ΄κΈ° 100 μ¤ν… λ™μ• μ μ§„μ  μ¦κ°€
- **Gradient Clipping**: κΈ°λ³Έκ°’ μ μ©
- **Early Stopping**: Validation Loss κΈ°μ¤€

### 7. μ‹¤ν—λ³„ λ³€μ μ„¤μ •

#### Learning Rate μ‹¤ν—
- **μ‹¤ν— A**: 1e-4 (λ³΄μμ  ν•™μµ)
- **μ‹¤ν— B**: 5e-4 (μ κ·Ήμ  ν•™μµ)

#### Batch Size μ‹¤ν—  
- **μ‹¤ν— A**: Per Device Batch Size 1 (λ©”λ¨λ¦¬ ν¨μ¨)
- **μ‹¤ν— B**: Per Device Batch Size 2 (κ· ν•)
- **μ‹¤ν— C**: Per Device Batch Size 4 (μ†λ„ μ°μ„ )

#### Dataset Size μ‹¤ν—
- **μ‹¤ν— A**: 10,000 μƒν” (ν¨μ¨μ„± κ²€μ¦)
- **μ‹¤ν— B**: 30,000 μƒν” (μ„±λ¥ μµλ€ν™”)

## π§ μ‹¤ν— μ„¤κ³„ λ° λ¶„μ„

### 1. Learning Rate μ‹¤ν—
**νμΌ**: `learning_rate_hyperclova_deobfuscation_finetuning.ipynb`

#### μ‹¤ν— μ΅°κ±΄
- **μ‹¤ν— A**: λ‚®μ€ ν•™μµλ¥  (1e-4)
- **μ‹¤ν— B**: λ†’μ€ ν•™μµλ¥  (5e-4)

#### κ²°κ³Ό μ”μ•½
| λ¨λΈ | BLEU μ μ | ROUGE-1 | ROUGE-2 | ROUGE-L | λ¬Έμ μ •ν™•λ„ | μ¶”λ΅  μ‹κ°„ |
|------|-----------|---------|---------|---------|-------------|----------|
| μ›λ³Έ λ¨λΈ | 0.0029 | 0.138 | 0.064 | 0.138 | 0.151 | 6.78s |
| 1e-4 Learning Rate | 0.0233 | 0.276 | 0.148 | 0.279 | 0.332 | 3.03s |
| 5e-4 Learning Rate | 0.0211 | 0.277 | 0.148 | 0.279 | 0.313 | 2.89s |

### 2. Batch Size μ‹¤ν—
**νμΌ**: `batch_size_hyperclova_deobfuscation_finetuning.ipynb`

#### μ‹¤ν— μ΅°κ±΄
- **Batch Size 1**: λ©”λ¨λ¦¬ ν¨μ¨μ , ν•™μµ μ•μ •μ„± λ†’μ
- **Batch Size 2**: κ· ν•μ΅ν μ„¤μ •
- **Batch Size 4**: λΉ λ¥Έ μλ ΄, λ†’μ€ λ©”λ¨λ¦¬ μ‚¬μ©

#### κ²°κ³Ό μ”μ•½
| λ¨λΈ | BLEU μ μ | ROUGE-1 | ROUGE-2 | ROUGE-L | λ¬Έμ μ •ν™•λ„ | μ¶”λ΅  μ‹κ°„ |
|------|-----------|---------|---------|---------|-------------|----------|
| μ›λ³Έ λ¨λΈ | 0.0022 | 0.123 | 0.052 | 0.122 | 0.145 | 7.37s |
| λ°°μΉ ν¬κΈ° 1 | 0.0193 | 0.279 | 0.145 | 0.279 | 0.315 | 3.12s |
| λ°°μΉ ν¬κΈ° 2 | 0.0192 | 0.279 | 0.148 | 0.279 | 0.326 | 3.12s |
| λ°°μΉ ν¬κΈ° 4 | 0.0220 | 0.279 | 0.149 | 0.279 | 0.331 | 3.15s |

### 3. Dataset Size μ‹¤ν—
**νμΌ**: `datasets_hyperclova_deobfuscation_finetuning.ipynb`

#### μ‹¤ν— μ΅°κ±΄
- **μ‹¤ν— A**: 1λ§κ° μƒν”λ΅ νμΈνλ‹
- **μ‹¤ν— B**: 3λ§κ° μƒν”λ΅ νμΈνλ‹

#### κ²°κ³Ό μ”μ•½
| λ¨λΈ | BLEU μ μ | ROUGE-1 | ROUGE-2 | ROUGE-L | λ¬Έμ μ •ν™•λ„ | μ¶”λ΅  μ‹κ°„ |
|------|-----------|---------|---------|---------|-------------|----------|
| μ›λ³Έ λ¨λΈ | 0.0024 | 0.124 | 0.063 | 0.123 | 0.124 | 7.53s |
| 10K λ°μ΄ν„°μ…‹ | 0.0220 | 0.279 | 0.148 | 0.278 | 0.327 | 3.13s |
| 30K λ°μ΄ν„°μ…‹ | 0.0201 | 0.279 | 0.149 | 0.279 | 0.324 | 3.12s |

## π“ μ„±λ¥ λ¶„μ„ κ²°κ³Ό

### μ£Όμ” μ„±κ³Ό
1. **νμΈνλ‹ ν¨κ³Ό μ…μ¦**: λ¨λ“  μ‹¤ν—μ—μ„ μ›λ³Έ λ¨λΈ λ€λΉ„ μƒλ‹Ήν• μ„±λ¥ ν–¥μƒ ν™•μΈ
   - BLEU μ μ: 7-10λ°° ν–¥μƒ
   - ROUGE μ μ: 2λ°° μ΄μƒ ν–¥μƒ
   - λ¬Έμ μ •ν™•λ„: 2λ°° μ΄μƒ ν–¥μƒ
   - μ¶”λ΅  μ‹κ°„: 50% μ΄μƒ λ‹¨μ¶•

2. **μµμ  ν•μ΄νΌνλΌλ―Έν„° λ°κ²¬**:
   - Learning Rate: 1e-4κ°€ μ•½κ°„ λ” λ‚μ€ μ„±λ¥
   - Batch Size: 4κ°€ κ°€μ¥ κ· ν•μ΅ν μ„±λ¥
   - Dataset Size: 10K λ°μ΄ν„°μ…‹μ΄ ν¨μ¨μ μΈ μ„±λ¥

## π”§ κΈ°μ  μ¤νƒ

### ν•µμ‹¬ λΌμ΄λΈλ¬λ¦¬
- **Transformers**: HuggingFace νΈλμ¤ν¬λ¨Έ λ¨λΈ λΌμ΄λΈλ¬λ¦¬
- **PEFT**: Parameter Efficient Fine-Tuning (LoRA κµ¬ν„)
- **TRL**: Transformer Reinforcement Learning
- **Datasets**: HuggingFace λ°μ΄ν„°μ…‹ λΌμ΄λΈλ¬λ¦¬
- **BitsAndBytes**: μ–‘μν™” λΌμ΄λΈλ¬λ¦¬
- **Accelerate**: λ¶„μ‚° ν•™μµ μ§€μ›

### ν‰κ°€ λ©”νΈλ¦­
- **BLEU Score**: κΈ°κ³„λ²μ—­ ν’μ§ ν‰κ°€
- **ROUGE-1/2/L**: μ”μ•½ ν’μ§ ν‰κ°€
- **λ¬Έμ μ •ν™•λ„**: λ¬Έμ λ‹¨μ„ μ •ν™•λ„
- **μ™„μ „ μΌμΉμ¨**: μ „μ²΄ ν…μ¤νΈ μ™„μ „ μΌμΉ λΉ„μ¨
- **μ¶”λ΅  μ‹κ°„**: λ¨λΈ μ¶”λ΅  μ†λ„

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
FineTuningLLM/
β”β”€β”€ π“„ ν•™μµ λ…ΈνΈλ¶
β”‚   β”β”€β”€ learning_rate_hyperclova_deobfuscation_finetuning.ipynb
β”‚   β”β”€β”€ batch_size_hyperclova_deobfuscation_finetuning.ipynb
β”‚   β””β”€β”€ datasets_hyperclova_deobfuscation_finetuning.ipynb
β”‚
β”β”€β”€ π“ μ„±λ¥ λ¶„μ„ λ…ΈνΈλ¶
β”‚   β”β”€β”€ model_performance_analysis_learning_rate.ipynb
β”‚   β”β”€β”€ model_performance_analysis_batch_size.ipynb
β”‚   β”β”€β”€ model_performance_analysis_datasets.ipynb
β”‚   β””β”€β”€ team_all_model_performance_analysis_learning_rate.ipynb
β”‚
β”β”€β”€ π“‚ λ°μ΄ν„°μ…‹
β”‚   β”β”€β”€ testdata.csv
β”‚   β””β”€β”€ [6κ°μ ν•κµ­μ–΄ ν…μ¤νΈ μ ν•λ³„ λ‚λ…ν™” λ°μ΄ν„°]
β”‚
β”β”€β”€ π¤– ν›λ ¨λ λ¨λΈ
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-1e-4-learning-rate/
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-5e-4-learning-rate/
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-with-1-batch-size/
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-with-2-batch-size/
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-with-4-batch-size/
β”‚   β”β”€β”€ hyperclova-deobfuscation-lora-with-10k-datasets/
β”‚   β””β”€β”€ hyperclova-deobfuscation-lora-with-30k-datasets/
β”‚
β”β”€β”€ π“ λ¶„μ„ κ²°κ³Ό
β”‚   β”β”€β”€ 1μ°¨ ν•™μµλ¥  μ΅°μ • μ°¨μ΄μ— λ€ν• μ„±λ¥ λ¶„μ„/
β”‚   β”β”€β”€ 1μ°¨ λ°°μΉκ°’ μ΅°μ • μ°¨μ΄μ— λ€ν• μ„±λ¥ λ¶„μ„/
β”‚   β””β”€β”€ 1μ°¨ ν•™μµλ‰μ— μ°¨μ΄μ— λ”°λ¥Έ μ„±λ¥ λ¶„μ„/
β”‚
β””β”€β”€ π“‹ λ¬Έμ„
    β”β”€β”€ LICENSE
    β”β”€β”€ .github/README.md
    β””β”€β”€ PROJECT_DOCUMENTATION.md
```

## π› οΈ μ„¤μΉ λ° μ‹¤ν–‰ κ°€μ΄λ“

### ν•„μ μ”κµ¬μ‚¬ν•­
- Python 3.8+
- CUDA μ§€μ› GPU (κ¶μ¥)
- 16GB+ RAM
- μ¶©λ¶„ν• μ €μ¥ κ³µκ°„ (λ¨λΈ λ° λ°μ΄ν„°μ©)

### μ„¤μΉ λ°©λ²•
```bash
# μ €μ¥μ† ν΄λ΅ 
git clone https://github.com/91veMe4Plus/FineTuningLLM.git
cd FineTuningLLM

# ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ
pip install transformers>=4.35.0
pip install peft>=0.6.0
pip install trl>=0.7.0
pip install datasets>=2.14.0
pip install bitsandbytes>=0.41.0
pip install accelerate>=0.24.0
pip install evaluate>=0.4.0
pip install rouge-score>=0.1.2
pip install gradio>=4.0.0
pip install scikit-learn>=1.3.0
```

### μ‹¤ν–‰ λ°©λ²•
1. **λ°μ΄ν„°μ…‹ μ¤€λΉ„**: λ°μ΄ν„°μ…‹μ„ μ μ ν• κ²½λ΅μ— λ°°μΉ
2. **μ‹¤ν— μ‹¤ν–‰**: μ›ν•λ” μ‹¤ν— λ…ΈνΈλ¶μ„ μ„ νƒν•μ—¬ μ‹¤ν–‰
   ```bash
   jupyter notebook learning_rate_hyperclova_deobfuscation_finetuning.ipynb
   ```
3. **μ„±λ¥ λ¶„μ„**: μ„±λ¥ λ¶„μ„ λ…ΈνΈλ¶μΌλ΅ κ²°κ³Ό λ¶„μ„
   ```bash
   jupyter notebook model_performance_analysis_learning_rate.ipynb
   ```

## π”® ν–¥ν›„ μ—°κµ¬ λ°©ν–¥

### 1. λ¨λΈ ν™•μ¥
- λ” ν° λ¨λΈ (1B, 3B νλΌλ―Έν„°)μ—μ„μ μ‹¤ν—
- λ‹¤λ¥Έ λ² μ΄μ¤ λ¨λΈ (KoBART, KoGPT λ“±) λΉ„κµ λ¶„μ„

### 2. λ°μ΄ν„° ν™•μ¥
- λ” λ‹¤μ–‘ν• ν…μ¤νΈ λ„λ©”μΈ μ¶”κ°€
- λ” ν° λ°μ΄ν„°μ…‹ (50K, 100K) μ‹¤ν—
- μ‹¤μ‹κ°„ λ°μ΄ν„° μμ§‘ λ° μ§€μ† ν•™μµ

### 3. κΈ°μ  κ°μ„ 
- λ‹¤λ¥Έ PEFT κΈ°λ²• (AdaLoRA, QLoRA λ“±) λΉ„κµ
- μ•™μƒλΈ” λ¨λΈ κµ¬μ¶•
- μ‹¤μ‹κ°„ μ¶”λ΅  API κ°λ°

### 4. μ‘μ© λ¶„μ•Ό ν™•μ¥
- λ‹¤κµ­μ–΄ λΉ„λ‚λ…ν™” ν™•μ¥
- μ‹¤μ‹κ°„ μ±„ν… ν•„ν„°λ§ μ‹μ¤ν…
- μ›Ή λΈλΌμ°μ € ν™•μ¥ ν”„λ΅κ·Έλ¨

## π“ μ‹κ°ν™” λ° λ¶„μ„ κ²°κ³Ό

ν”„λ΅μ νΈμ—λ” λ‹¤μκ³Ό κ°™μ€ μ‹κ°ν™” κ²°κ³Όλ¬Όμ΄ ν¬ν•¨λμ–΄ μμµλ‹λ‹¤:

### CSV λ¶„μ„ νμΌ
- `model_performance_summary_*.csv`: λ¨λΈλ³„ μ„±λ¥ μ”μ•½
- `detailed_model_comparison_*.csv`: μƒμ„Έ λΉ„κµ λ¶„μ„ κ²°κ³Ό
- `finetuning_effect_analysis.csv`: νμΈνλ‹ ν¨κ³Ό λ¶„μ„

### μ‹κ°ν™” μ΄λ―Έμ§€
- μ„±λ¥ λΉ„κµ μ°¨νΈ (PNG ν•μ‹)
- λ¬Έμ μ •ν™•λ„ λ¶„ν¬ νμ¤ν† κ·Έλ¨
- μ¶”λ΅  μ‹κ°„ λΉ„κµ κ·Έλν”„
- ν…μ¤νΈ κΈΈμ΄λ³„ μ„±λ¥ λ¶„μ„
- μΉ΄ν…κ³ λ¦¬λ³„ μ •ν™•λ„ λ¶„ν¬

---

**Β© 2025 91veMe4Plus Team. All rights reserved.**
