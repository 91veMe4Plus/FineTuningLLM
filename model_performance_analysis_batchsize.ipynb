{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bbf116",
   "metadata": {},
   "source": [
    "# HyperCLOVAX 모델 Batch Size 비교 분석\n",
    "\n",
    "이 노트북은 원본 모델과 서로 다른 Batch Size로 미세조정된 HyperCLOVAX 모델의 성능을 비교합니다.\n",
    "\n",
    "## 분석 목표\n",
    "- 네 모델 간 정량적 성능 비교 (BLEU, ROUGE, 문자 정확도)\n",
    "- 정성적 분석 (실제 출력 예시 비교)\n",
    "- 추론 시간 및 효율성 비교\n",
    "- Batch Size 미세조정 효과 분석\n",
    "- 결과 시각화\n",
    "\n",
    "## 모델 정보\n",
    "- **원본 모델**: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- **Batch Size 1 모델**: `hyperclova-deobfuscation-lora-1-batch-size` (가정된 경로)\n",
    "- **Batch Size 2 모델**: `hyperclova-deobfuscation-lora-2-batch-size` (가정된 경로)\n",
    "- **Batch Size 4 모델**: `hyperclova-deobfuscation-lora-4-batch-size` (가정된 경로)\n",
    "- **테스트 데이터**: `testdata.csv` (1,002 샘플)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb29ec6",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "!pip install -q python-Levenshtein\n",
    "\n",
    "print(\"패키지 설치 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib 및 seaborn 설정\n",
    "# Colab 환경에서 한글 폰트 설정 (필요시)\n",
    "# !sudo apt-get install -y fonts-nanum\n",
    "# !sudo fc-cache -fv\n",
    "# !rm ~/.cache/matplotlib -rf\n",
    "# plt.rc('font', family='NanumBarunGothic') \n",
    "plt.rcParams['font.family'] = 'DejaVu Sans' # 기본 폰트 사용\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5b03f",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 연결 (Colab에서 실행 시)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    \n",
    "    # 기존 마운트 포인트가 있으면 정리\n",
    "    mount_point = '/content/drive'\n",
    "    if os.path.exists(mount_point):\n",
    "        try:\n",
    "            # 마운트 해제 시도\n",
    "            print(\"기존 마운트 포인트 정리 중...\")\n",
    "            os.system(f'fusermount -u {mount_point} 2>/dev/null || true')\n",
    "            shutil.rmtree(mount_point, ignore_errors=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Google Drive 마운트\n",
    "    drive.mount(mount_point, force_remount=True)\n",
    "    \n",
    "    # 경로 설정 (Google Drive 기준)\n",
    "    BASE_PATH = '/content/drive/MyDrive/' # 사용자의 Drive 경로에 맞게 수정 필요\n",
    "    MODEL_BS1_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-1-batch-size' # 실제 경로로 수정 필요\n",
    "    MODEL_BS2_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-2-batch-size' # 실제 경로로 수정 필요\n",
    "    MODEL_BS4_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-4-batch-size' # 실제 경로로 수정 필요\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv' # 실제 경로로 수정 필요\n",
    "    \n",
    "    # Google Drive 루트에 전용 분석 결과 폴더 생성\n",
    "    analysis_root_dir = os.path.join(BASE_PATH, 'HyperCLOVAX_BatchSize_Analysis_Results')\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # 로컬 실행 시 (경로를 로컬 환경에 맞게 수정)\n",
    "    BASE_PATH = './'\n",
    "    MODEL_BS1_PATH = './hyperclova-deobfuscation-lora-1-batch-size' # 실제 경로로 수정 필요\n",
    "    MODEL_BS2_PATH = './hyperclova-deobfuscation-lora-2-batch-size' # 실제 경로로 수정 필요\n",
    "    MODEL_BS4_PATH = './hyperclova-deobfuscation-lora-4-batch-size' # 실제 경로로 수정 필요\n",
    "    TEST_DATA_PATH = './testdata.csv' # 실제 경로로 수정 필요\n",
    "    \n",
    "    # 로컬용 분석 결과 폴더\n",
    "    analysis_root_dir = './HyperCLOVAX_BatchSize_Analysis_Results'\n",
    "    os.makedirs(analysis_root_dir, exist_ok=True)\n",
    "    print(f\"분석 결과 루트 폴더 생성: {analysis_root_dir}\")\n",
    "\n",
    "# 이미지 저장 폴더 생성 (분석 결과 폴더 내에)\n",
    "image_save_dir = os.path.join(analysis_root_dir, 'visualization_images')\n",
    "os.makedirs(image_save_dir, exist_ok=True)\n",
    "print(f\"이미지 저장 폴더 생성: {image_save_dir}\")\n",
    "\n",
    "print(f\"\\n경로 설정 완료:\")\n",
    "print(f\"Batch Size 1 모델 경로: {MODEL_BS1_PATH}\")\n",
    "print(f\"Batch Size 2 모델 경로: {MODEL_BS2_PATH}\")\n",
    "print(f\"Batch Size 4 모델 경로: {MODEL_BS4_PATH}\")\n",
    "print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")\n",
    "print(f\"분석 결과 저장 경로: {analysis_root_dir}\")\n",
    "print(f\"이미지 저장 경로: {image_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    print(f\"테스트 데이터 크기: {len(test_df)} 샘플\")\n",
    "    print(f\"컬럼 목록: {test_df.columns.tolist()}\")\n",
    "    print(\"\\n첫 5개 샘플:\")\n",
    "    print(test_df.head())\n",
    "    \n",
    "    # 데이터 통계\n",
    "    print(\"\\n데이터 통계:\")\n",
    "    print(f\"- 총 샘플 수: {len(test_df)}\")\n",
    "    print(f\"- 원본 텍스트 평균 길이: {test_df['original'].str.len().mean():.1f}\")\n",
    "    print(f\"- 난독화 텍스트 평균 길이: {test_df['obfuscated'].str.len().mean():.1f}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 테스트 데이터 파일({TEST_DATA_PATH})을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "    # 필요한 경우 여기서 실행 중단 또는 기본 데이터프레임 생성\n",
    "    test_df = pd.DataFrame() # 빈 데이터프레임으로 초기화하여 이후 코드 오류 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c660f8",
   "metadata": {},
   "source": [
    "## 3. 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 이름 설정\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# 토크나이저 로드 (하나의 모델 경로에서 로드)\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "try:\n",
    "    # Batch Size 1 모델 경로에서 토크나이저 로드 시도\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_BS1_PATH)\n",
    "    print(f\"토크나이저 로딩 완료 (어휘 크기: {len(tokenizer)})\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: {MODEL_BS1_PATH}에서 토크나이저 로딩 실패: {e}\")\n",
    "    print(f\"베이스 모델({BASE_MODEL_NAME})에서 토크나이저 로딩 시도...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "        print(f\"베이스 모델 토크나이저 로딩 완료 (어휘 크기: {len(tokenizer)})\")\n",
    "    except Exception as e2:\n",
    "        print(f\"오류: 베이스 모델({BASE_MODEL_NAME})에서도 토크나이저 로딩 실패: {e2}\")\n",
    "        # 토크나이저 로딩 실패 시 이후 코드 실행 불가\n",
    "        tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83913712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_model(model_path, model_name):\n",
    "    \"\"\"LoRA 모델을 로드합니다\"\"\"\n",
    "    print(f\"\\n{model_name} ({model_path}) 로딩 중...\")\n",
    "    try:\n",
    "        # 베이스 모델 로드\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\" # GPU 자동 할당\n",
    "        )\n",
    "        print(f\" - 베이스 모델({BASE_MODEL_NAME}) 로드 완료\")\n",
    "        \n",
    "        # LoRA 어댑터 적용\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        model = model.to(device) # 모델을 지정된 장치로 이동\n",
    "        model.eval() # 평가 모드로 설정\n",
    "        print(f\" - LoRA 어댑터 적용 완료\")\n",
    "        print(f\"{model_name} 로딩 완료\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"오류: {model_name} 로딩 실패: {e}\")\n",
    "        print(f\" - 경로({model_path}) 또는 모델 파일 확인 필요\")\n",
    "        return None\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"원본 베이스 모델을 로드합니다\"\"\"\n",
    "    print(f\"\\n원본 모델 ({BASE_MODEL_NAME}) 로딩 중...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        base_model = base_model.to(device)\n",
    "        base_model.eval() # 평가 모드로 설정\n",
    "        print(f\"원본 모델 로딩 완료\")\n",
    "        return base_model\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 원본 모델 로딩 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 모델 로드\n",
    "models = {}\n",
    "models['원본 모델'] = load_base_model()\n",
    "models['Batch Size 1 모델'] = load_lora_model(MODEL_BS1_PATH, \"Batch Size 1 모델\")\n",
    "models['Batch Size 2 모델'] = load_lora_model(MODEL_BS2_PATH, \"Batch Size 2 모델\")\n",
    "models['Batch Size 4 모델'] = load_lora_model(MODEL_BS4_PATH, \"Batch Size 4 모델\")\n",
    "\n",
    "# 로드 성공한 모델만 필터링\n",
    "loaded_models = {name: model for name, model in models.items() if model is not None}\n",
    "\n",
    "if len(loaded_models) > 0:\n",
    "    print(f\"\\n총 {len(loaded_models)}개 모델 로딩 완료: {list(loaded_models.keys())}\")\n",
    "else:\n",
    "    print(\"\\n오류: 로드된 모델이 없습니다. 경로 및 파일 확인 후 다시 시도해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb373",
   "metadata": {},
   "source": [
    "## 4. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, tokenizer, obfuscated_text, max_length=256):\n",
    "    \"\"\"난독화된 텍스트를 입력받아 원본 텍스트 생성\"\"\"\n",
    "    if tokenizer is None:\n",
    "        print(\"오류: 토크나이저가 로드되지 않아 추론을 진행할 수 없습니다.\")\n",
    "        return \"토크나이저 로딩 오류\", 0.0\n",
    "        \n",
    "    prompt = f\"\"\"### 지시사항:\n",
    "다음 난독화된 한국어 텍스트를 원래 텍스트로 복원해주세요.\n",
    "\n",
    "난독화된 텍스트: {obfuscated_text}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True, # 샘플링 사용\n",
    "                temperature=0.7, # 약간의 다양성 부여\n",
    "                top_p=0.9, # 상위 90% 확률 누적 토큰만 고려\n",
    "                pad_token_id=tokenizer.eos_token_id, # 패딩 토큰 설정\n",
    "                eos_token_id=tokenizer.eos_token_id # 종료 토큰 설정\n",
    "            )\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # 생성된 토큰 ID만 디코딩 (입력 프롬프트 제외)\n",
    "        output_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # 후처리: 불필요한 공백 제거 및 종료 토큰 관련 문자열 제거\n",
    "        response = response.strip()\n",
    "        # 가끔 <|endoftext|> 같은 특수 토큰이 포함될 경우 제거\n",
    "        response = response.replace('<|endoftext|>', '').strip()\n",
    "            \n",
    "        return response, inference_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류: 추론 중 오류 발생 - {e}\")\n",
    "        return \"추론 오류\", 0.0\n",
    "\n",
    "print(\"추론 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaac22",
   "metadata": {},
   "source": [
    "## 5. 성능 평가 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 메트릭 로드\n",
    "try:\n",
    "    bleu = load(\"bleu\")\n",
    "    rouge = load(\"rouge\")\n",
    "    print(\"BLEU 및 ROUGE 메트릭 로드 완료\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: 평가 메트릭 로드 실패 - {e}\")\n",
    "    bleu = None\n",
    "    rouge = None\n",
    "\n",
    "def calculate_character_accuracy(pred, ref):\n",
    "    \"\"\"문자 단위 정확도 계산 (Levenshtein 거리 기반 유사도)\n",
    "    pip install python-Levenshtein 필요\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import Levenshtein\n",
    "        if len(ref) == 0:\n",
    "            return 1.0 if len(pred) == 0 else 0.0\n",
    "        distance = Levenshtein.distance(pred, ref)\n",
    "        # 유사도 = 1 - (편집 거리 / 더 긴 문자열 길이)\n",
    "        similarity = 1.0 - (distance / max(len(pred), len(ref)))\n",
    "        return max(0.0, similarity) # 0 미만 값 방지\n",
    "    except ImportError:\n",
    "        print(\"경고: python-Levenshtein 패키지가 설치되지 않았습니다. 문자 정확도 계산을 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "        print(\"설치 방법: !pip install python-Levenshtein\")\n",
    "        # 대체 계산법 (간단한 일치율)\n",
    "        if len(ref) == 0:\n",
    "             return 1.0 if len(pred) == 0 else 0.0\n",
    "        matches = sum(1 for i, char in enumerate(pred) if i < len(ref) and char == ref[i])\n",
    "        return matches / len(ref)\n",
    "    except Exception as e:\n",
    "        print(f\"문자 정확도 계산 중 오류: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_exact_match(pred, ref):\n",
    "    \"\"\"완전 일치 여부 (공백 제거 후 비교)\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"모든 메트릭 계산\"\"\"\n",
    "    metrics_results = {\n",
    "        'bleu': 0.0,\n",
    "        'rouge1': 0.0,\n",
    "        'rouge2': 0.0,\n",
    "        'rougeL': 0.0,\n",
    "        'char_accuracy': 0.0,\n",
    "        'exact_match': 0.0,\n",
    "        'char_accuracies': [],\n",
    "        'exact_matches': []\n",
    "    }\n",
    "    \n",
    "    if not predictions or not references:\n",
    "        print(\"경고: 예측 또는 참조 목록이 비어있어 메트릭 계산을 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "        return metrics_results\n",
    "        \n",
    "    # BLEU 계산\n",
    "    if bleu:\n",
    "        try:\n",
    "            # references를 sacrebleu 형식에 맞게 변환: [[ref1], [ref2], ...] -> [[ref1_sent1, ref1_sent2,...], [ref2_sent1,...]]\n",
    "            # 여기서는 각 샘플당 참조가 하나이므로 [[ref] for ref in references] 사용\n",
    "            bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            metrics_results['bleu'] = bleu_score['bleu'] if bleu_score and 'bleu' in bleu_score else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"BLEU 계산 중 오류: {e}\")\n",
    "            metrics_results['bleu'] = 0.0\n",
    "    else:\n",
    "        print(\"BLEU 메트릭이 로드되지 않아 계산을 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    if rouge:\n",
    "        try:\n",
    "            rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "            metrics_results['rouge1'] = rouge_scores['rouge1'] if rouge_scores and 'rouge1' in rouge_scores else 0.0\n",
    "            metrics_results['rouge2'] = rouge_scores['rouge2'] if rouge_scores and 'rouge2' in rouge_scores else 0.0\n",
    "            metrics_results['rougeL'] = rouge_scores['rougeL'] if rouge_scores and 'rougeL' in rouge_scores else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE 계산 중 오류: {e}\")\n",
    "            metrics_results['rouge1'] = 0.0\n",
    "            metrics_results['rouge2'] = 0.0\n",
    "            metrics_results['rougeL'] = 0.0\n",
    "    else:\n",
    "        print(\"ROUGE 메트릭이 로드되지 않아 계산을 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "    \n",
    "    # 문자 정확도 및 완전 일치율 계산\n",
    "    char_accuracies = []\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        char_accuracies.append(calculate_character_accuracy(pred, ref))\n",
    "        exact_matches.append(calculate_exact_match(pred, ref))\n",
    "        \n",
    "    if char_accuracies:\n",
    "        metrics_results['char_accuracy'] = np.mean(char_accuracies)\n",
    "        metrics_results['char_accuracies'] = char_accuracies\n",
    "    if exact_matches:\n",
    "        metrics_results['exact_match'] = np.mean(exact_matches)\n",
    "        metrics_results['exact_matches'] = exact_matches\n",
    "        \n",
    "    return metrics_results\n",
    "\n",
    "print(\"평가 메트릭 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3322",
   "metadata": {},
   "source": [
    "## 6. 모델 성능 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, tokenizer, test_df, sample_size=None):\n",
    "    \"\"\"모델 성능 평가\"\"\"\n",
    "    if model is None:\n",
    "        print(f\"경고: {model_name}이(가) 로드되지 않아 평가를 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "        return None\n",
    "    if tokenizer is None:\n",
    "        print(f\"경고: 토크나이저가 로드되지 않아 {model_name} 평가를 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "        return None\n",
    "    if test_df.empty:\n",
    "        print(f\"경고: 테스트 데이터가 비어있어 {model_name} 평가를 건너<0xEB><0x9A><0x8D>니다.\")\n",
    "        return None\n",
    "        \n",
    "    if sample_size and sample_size < len(test_df):\n",
    "        test_data = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f\"\\n{model_name} 평가 시작 ({sample_size}개 샘플 무작위 추출)\")\n",
    "    else:\n",
    "        test_data = test_df.copy()\n",
    "        print(f\"\\n{model_name} 평가 시작 ({len(test_data)}개 전체 샘플)\")\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    references = test_data['original'].tolist() # 참조 텍스트 미리 준비\n",
    "    \n",
    "    # tqdm 설정: desc에 모델 이름 포함, unit='샘플'\n",
    "    progress_bar = tqdm(test_data.iterrows(), total=len(test_data), desc=f\"{model_name} 평가\", unit='샘플')\n",
    "    \n",
    "    for idx, row in progress_bar:\n",
    "        obfuscated = row['obfuscated']\n",
    "        # generate_deobfuscated_text 함수에 tokenizer 전달\n",
    "        pred, inf_time = generate_deobfuscated_text(model, tokenizer, obfuscated)\n",
    "        predictions.append(pred)\n",
    "        inference_times.append(inf_time)\n",
    "        # 진행률 표시줄에 현재 처리 중인 인덱스 표시 (선택 사항)\n",
    "        # progress_bar.set_postfix({'idx': idx}) \n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # 추론 시간 통계 (0인 경우 제외)\n",
    "    valid_inference_times = [t for t in inference_times if t > 0]\n",
    "    avg_inference_time = np.mean(valid_inference_times) if valid_inference_times else 0.0\n",
    "    total_inference_time = np.sum(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'inference_times': inference_times,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'total_inference_time': total_inference_time,\n",
    "        'test_data_indices': test_data.index.tolist(), # 평가에 사용된 데이터 인덱스 저장\n",
    "        **metrics # 계산된 메트릭 결과 통합\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} 평가 완료\")\n",
    "    print(f\" - 평균 추론 시간: {avg_inference_time:.3f}초 (유효 샘플 기준)\")\n",
    "    print(f\" - 총 추론 시간: {total_inference_time:.1f}초\")\n",
    "    print(f\" - BLEU: {metrics['bleu']:.4f}, ROUGE-L: {metrics['rougeL']:.4f}, Char Acc: {metrics['char_accuracy']:.4f}, Exact Match: {metrics['exact_match']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- 평가 실행 --- #\n",
    "SAMPLE_SIZE = 200  # None으로 설정하면 전체 데이터셋 사용 (시간이 오래 걸릴 수 있음)\n",
    "evaluation_results = {}\n",
    "\n",
    "if not loaded_models:\n",
    "    print(\"오류: 평가할 모델이 로드되지 않았습니다.\")\n",
    "elif test_df.empty:\n",
    "    print(\"오류: 테스트 데이터가 비어있어 평가를 진행할 수 없습니다.\")\n",
    "elif tokenizer is None:\n",
    "    print(\"오류: 토크나이저가 로드되지 않아 평가를 진행할 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"모델 성능 평가를 시작합니다 (샘플 크기: {SAMPLE_SIZE if SAMPLE_SIZE else '전체'})...\")\n",
    "    \n",
    "    # 로드된 모델들에 대해 순차적으로 평가 실행\n",
    "    for model_name, model_instance in loaded_models.items():\n",
    "        evaluation_results[model_name] = evaluate_model(model_instance, model_name, tokenizer, test_df, SAMPLE_SIZE)\n",
    "        # 메모리 관리: 평가 끝난 모델의 GPU 메모리 해제 시도 (선택 사항)\n",
    "        # del model_instance\n",
    "        # if torch.cuda.is_available():\n",
    "        #     torch.cuda.empty_cache()\n",
    "            \n",
    "    # 평가가 완료된 결과만 필터링\n",
    "    completed_evaluations = {name: res for name, res in evaluation_results.items() if res is not None}\n",
    "    \n",
    "    if completed_evaluations:\n",
    "        print(f\"\\n총 {len(completed_evaluations)}개 모델 평가 완료!\")\n",
    "    else:\n",
    "        print(\"\\n오류: 성공적으로 평가된 모델이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18542a41",
   "metadata": {},
   "source": [
    "## 7. 성능 비교 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 표 생성\n",
    "if 'completed_evaluations' in locals() and completed_evaluations:\n",
    "    metrics_to_display = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy', 'exact_match', 'avg_inference_time']\n",
    "    metric_names_kr = ['BLEU 점수', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', '문자 정확도', '정확 일치율', '평균 추론 시간(초)']\n",
    "    \n",
    "    comparison_data = {'메트릭': metric_names_kr}\n",
    "    \n",
    "    # 평가된 모델 순서대로 데이터 추가 (원본 모델 우선)\n",
    "    model_order = ['원본 모델'] + [name for name in completed_evaluations if name != '원본 모델']\n",
    "    \n",
    "    for model_name in model_order:\n",
    "        if model_name in completed_evaluations:\n",
    "            results = completed_evaluations[model_name]\n",
    "            model_column = []\n",
    "            for metric_key in metrics_to_display:\n",
    "                value = results.get(metric_key, 0.0) # 메트릭 값이 없는 경우 0.0으로 처리\n",
    "                if metric_key == 'avg_inference_time':\n",
    "                    model_column.append(f\"{value:.3f}\") # 소수점 3자리\n",
    "                else:\n",
    "                    model_column.append(f\"{value:.4f}\") # 소수점 4자리\n",
    "            comparison_data[model_name] = model_column\n",
    "            \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"--- 모델 성능 비교표 ---\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # 결과를 CSV 파일로 저장\n",
    "    comparison_csv_path = os.path.join(analysis_root_dir, 'model_performance_comparison_batchsize.csv')\n",
    "    try:\n",
    "        comparison_df.to_csv(comparison_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n성능 비교표를 CSV 파일로 저장했습니다: {comparison_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 성능 비교표 CSV 저장 실패 - {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"오류: 평가 결과가 없어 성능 비교표를 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8b7c5",
   "metadata": {},
   "source": [
    "## 8. 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수 정의\n",
    "def plot_comparison_bar_chart(df, title, filename):\n",
    "    \"\"\"성능 비교 막대 그래프 생성\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"경고: 데이터프레임이 비어있어 '{title}' 그래프를 생성할 수 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # '메트릭' 컬럼을 인덱스로 설정\n",
    "        df_plot = df.set_index('메트릭')\n",
    "        # 숫자형으로 변환 시도, 오류 발생 시 0으로 대체\n",
    "        df_plot = df_plot.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "        \n",
    "        # 추론 시간 제외하고 시각화 (스케일 차이 때문)\n",
    "        metrics_for_plot = [m for m in df_plot.index if '시간' not in m]\n",
    "        df_plot_filtered = df_plot.loc[metrics_for_plot]\n",
    "        \n",
    "        if df_plot_filtered.empty:\n",
    "             print(f\"경고: '{title}' 그래프에 표시할 유효한 메트릭 데이터가 없습니다.\")\n",
    "             return\n",
    "             \n",
    "        ax = df_plot_filtered.plot(kind='bar', figsize=(14, 8), rot=0)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.ylabel('점수', fontsize=12)\n",
    "        plt.xlabel('평가 메트릭', fontsize=12)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.legend(title='모델', fontsize=10, title_fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 그래프 파일 저장\n",
    "        filepath = os.path.join(image_save_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        print(f\"그래프 저장 완료: {filepath}\")\n",
    "        plt.show()\n",
    "        plt.close() # 그래프 창 닫기\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{title}' 그래프 생성 중 오류 발생 - {e}\")\n",
    "\n",
    "# 메인 성능 지표 비교 그래프\n",
    "if 'comparison_df' in locals() and not comparison_df.empty:\n",
    "    plot_comparison_bar_chart(comparison_df, \"모델별 주요 성능 지표 비교 (Batch Size)\", \"batchsize_main_metrics_comparison.png\")\n",
    "else:\n",
    "    print(\"성능 비교 데이터가 없어 메인 성능 지표 그래프를 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 시간 비교 그래프\n",
    "def plot_inference_time_comparison(df, title, filename):\n",
    "    \"\"\"추론 시간 비교 막대 그래프 생성\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"경고: 데이터프레임이 비어있어 '{title}' 그래프를 생성할 수 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # '평균 추론 시간(초)' 행만 선택\n",
    "        inference_time_data = df[df['메트릭'] == '평균 추론 시간(초)'].set_index('메트릭')\n",
    "        # 숫자형으로 변환 시도, 오류 발생 시 0으로 대체\n",
    "        inference_time_data = inference_time_data.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "        \n",
    "        if inference_time_data.empty:\n",
    "             print(f\"경고: '{title}' 그래프에 표시할 추론 시간 데이터가 없습니다.\")\n",
    "             return\n",
    "             \n",
    "        ax = inference_time_data.T.plot(kind='bar', figsize=(10, 6), legend=False, rot=0)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.ylabel('평균 추론 시간 (초)', fontsize=12)\n",
    "        plt.xlabel('모델', fontsize=12)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 막대 위에 값 표시\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f')\n",
    "            \n",
    "        # 그래프 파일 저장\n",
    "        filepath = os.path.join(image_save_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        print(f\"그래프 저장 완료: {filepath}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{title}' 그래프 생성 중 오류 발생 - {e}\")\n",
    "\n",
    "# 추론 시간 비교 그래프 실행\n",
    "if 'comparison_df' in locals() and not comparison_df.empty:\n",
    "    plot_inference_time_comparison(comparison_df, \"모델별 평균 추론 시간 비교 (Batch Size)\", \"batchsize_inference_time_comparison.png\")\n",
    "else:\n",
    "    print(\"성능 비교 데이터가 없어 추론 시간 비교 그래프를 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 메트릭 분포 시각화 (Box Plot 등)\n",
    "def plot_metric_distribution(results_dict, metric_key, metric_name_kr, title, filename):\n",
    "    \"\"\"개별 메트릭 분포 시각화 (Box Plot)\"\"\"\n",
    "    if not results_dict:\n",
    "        print(f\"경고: 평가 결과가 없어 '{title}' 그래프를 생성할 수 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    plot_data = []\n",
    "    model_names = []\n",
    "    \n",
    "    # 모델 순서 정의 (원본 모델 우선)\n",
    "    model_order = ['원본 모델'] + [name for name in results_dict if name != '원본 모델']\n",
    "    \n",
    "    for model_name in model_order:\n",
    "        if model_name in results_dict:\n",
    "            results = results_dict[model_name]\n",
    "            # 메트릭 키 이름 변경 (예: 'char_accuracies' -> 'char_accuracy')\n",
    "            individual_metric_key = metric_key + 's' if metric_key in ['char_accuracy', 'exact_match'] else metric_key\n",
    "            if individual_metric_key in results and isinstance(results[individual_metric_key], list):\n",
    "                plot_data.append(results[individual_metric_key])\n",
    "                model_names.append(model_name)\n",
    "            elif metric_key in results: # 단일 값 메트릭 (예: bleu, rouge) - 분포 시각화 부적합\n",
    "                print(f\"정보: '{metric_name_kr}'은(는) 단일 값 메트릭이므로 분포 시각화 대신 평균값을 사용합니다.\")\n",
    "                # 단일 값이라도 표시하려면 다른 방식 필요 (예: 점 그래프)\n",
    "            else:\n",
    "                 print(f\"경고: {model_name} 결과에 '{individual_metric_key}' 또는 '{metric_key}' 데이터가 없습니다.\")\n",
    "                 \n",
    "    if not plot_data:\n",
    "        print(f\"경고: '{title}' 그래프에 표시할 데이터가 없습니다.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.boxplot(data=plot_data)\n",
    "        plt.xticks(ticks=range(len(model_names)), labels=model_names, rotation=10, ha='right')\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.ylabel(metric_name_kr, fontsize=12)\n",
    "        plt.xlabel('모델', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 그래프 파일 저장\n",
    "        filepath = os.path.join(image_save_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        print(f\"그래프 저장 완료: {filepath}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{title}' 그래프 생성 중 오류 발생 - {e}\")\n",
    "\n",
    "# 문자 정확도 분포 시각화\n",
    "if 'completed_evaluations' in locals() and completed_evaluations:\n",
    "    plot_metric_distribution(completed_evaluations, 'char_accuracy', '문자 정확도', \"모델별 문자 정확도 분포 (Batch Size)\", \"batchsize_char_accuracy_distribution.png\")\n",
    "else:\n",
    "    print(\"평가 결과가 없어 문자 정확도 분포 그래프를 생성할 수 없습니다.\")\n",
    "\n",
    "# 정확 일치율 분포 시각화 (Box plot보다는 Bar plot이 적합할 수 있음)\n",
    "# 여기서는 Box Plot으로 시도\n",
    "if 'completed_evaluations' in locals() and completed_evaluations:\n",
    "    plot_metric_distribution(completed_evaluations, 'exact_match', '정확 일치율', \"모델별 정확 일치율 분포 (Batch Size)\", \"batchsize_exact_match_distribution.png\")\n",
    "else:\n",
    "    print(\"평가 결과가 없어 정확 일치율 분포 그래프를 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8d7c6",
   "metadata": {},
   "source": [
    "## 9. 정성적 분석 (출력 예시 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정성적 비교를 위한 데이터프레임 생성\n",
    "def create_qualitative_comparison_df(results_dict, num_samples=10):\n",
    "    \"\"\"정성적 비교를 위한 샘플 데이터프레임 생성\"\"\"\n",
    "    if not results_dict:\n",
    "        print(\"경고: 평가 결과가 없어 정성적 비교 데이터프레임을 생성할 수 없습니다.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 기준 모델 (예: 원본 모델)의 결과 가져오기\n",
    "    base_model_name = '원본 모델'\n",
    "    if base_model_name not in results_dict:\n",
    "        # 원본 모델 없으면 첫 번째 모델 사용\n",
    "        base_model_name = list(results_dict.keys())[0]\n",
    "        \n",
    "    base_results = results_dict[base_model_name]\n",
    "    if 'test_data_indices' not in base_results or 'references' not in base_results or 'predictions' not in base_results:\n",
    "        print(f\"경고: {base_model_name} 결과에 필요한 데이터(인덱스, 참조, 예측)가 부족합니다.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 평가에 사용된 원본 데이터에서 샘플 인덱스 가져오기\n",
    "    eval_indices = base_results['test_data_indices']\n",
    "    if not eval_indices:\n",
    "        print(\"경고: 평가에 사용된 데이터 인덱스가 없습니다.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 원본 test_df에서 해당 인덱스의 데이터 추출\n",
    "    if 'test_df' not in globals() or test_df.empty:\n",
    "        print(\"경고: 원본 test_df가 로드되지 않았거나 비어있습니다.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    sampled_indices = np.random.choice(eval_indices, size=min(num_samples, len(eval_indices)), replace=False)\n",
    "    qualitative_df = test_df.loc[sampled_indices, ['original', 'obfuscated']].copy()\n",
    "    qualitative_df.rename(columns={'original': '정답 (Original)', 'obfuscated': '입력 (Obfuscated)'}, inplace=True)\n",
    "    \n",
    "    # 모델 순서 정의 (원본 모델 우선)\n",
    "    model_order = ['원본 모델'] + [name for name in results_dict if name != '원본 모델']\n",
    "    \n",
    "    # 각 모델의 예측 결과 추가\n",
    "    for model_name in model_order:\n",
    "        if model_name in results_dict:\n",
    "            results = results_dict[model_name]\n",
    "            if 'predictions' in results and 'test_data_indices' in results:\n",
    "                # 결과 딕셔너리 생성 (인덱스 -> 예측)\n",
    "                pred_dict = dict(zip(results['test_data_indices'], results['predictions']))\n",
    "                # 샘플된 인덱스에 해당하는 예측만 추출\n",
    "                qualitative_df[f'예측 ({model_name})'] = [pred_dict.get(idx, 'N/A') for idx in sampled_indices]\n",
    "            else:\n",
    "                qualitative_df[f'예측 ({model_name})'] = '결과 없음'\n",
    "                \n",
    "    return qualitative_df\n",
    "\n",
    "# 정성적 비교 실행 (15개 샘플)\n",
    "if 'completed_evaluations' in locals() and completed_evaluations:\n",
    "    qualitative_comparison_df = create_qualitative_comparison_df(completed_evaluations, num_samples=15)\n",
    "    if not qualitative_comparison_df.empty:\n",
    "        print(\"\\n--- 정성적 비교 샘플 (15개) ---\")\n",
    "        # Pandas 출력 옵션 설정 (줄바꿈 등)\n",
    "        pd.set_option('display.max_colwidth', None) # 전체 내용 표시\n",
    "        pd.set_option('display.width', 1000) # 너비 확장\n",
    "        print(qualitative_comparison_df)\n",
    "        \n",
    "        # 결과를 CSV 파일로 저장\n",
    "        qualitative_csv_path = os.path.join(analysis_root_dir, 'qualitative_comparison_samples_batchsize.csv')\n",
    "        try:\n",
    "            qualitative_comparison_df.to_csv(qualitative_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n정성적 비교 샘플을 CSV 파일로 저장했습니다: {qualitative_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"오류: 정성적 비교 샘플 CSV 저장 실패 - {e}\")\n",
    "    else:\n",
    "        print(\"정성적 비교 데이터프레임 생성에 실패했습니다.\")\n",
    "else:\n",
    "    print(\"평가 결과가 없어 정성적 비교를 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6b5a4",
   "metadata": {},
   "source": [
    "## 10. 결론 및 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5a493",
   "metadata": {},
   "source": [
    "### 분석 요약\n",
    "\n",
    "이 분석에서는 원본 HyperCLOVAX 모델과 Batch Size 1, 2, 4로 각각 미세조정된 LoRA 모델의 성능을 비교했습니다.\n",
    "\n",
    "**주요 평가지표:**\n",
    "- BLEU, ROUGE (1, 2, L): 번역 및 요약 품질 평가 지표\n",
    "- 문자 정확도 (Character Accuracy): 생성된 텍스트와 정답 텍스트 간의 문자 수준 유사도\n",
    "- 정확 일치율 (Exact Match Rate): 생성된 텍스트가 정답과 완전히 일치하는 비율\n",
    "- 평균 추론 시간: 샘플 당 텍스트 생성에 소요된 평균 시간\n",
    "\n",
    "**결과 해석:**\n",
    "\n",
    "*   **[Batch Size별 성능 경향 요약]** \n",
    "    (예시) Batch Size가 증가함에 따라 BLEU 및 ROUGE 점수가 [증가/감소/유사]하는 경향을 보였습니다. \n",
    "    문자 정확도와 정확 일치율은 Batch Size [값]에서 가장 높게 나타났습니다.\n",
    "    Batch Size가 성능에 미치는 영향은 [크다/작다/미미하다]고 판단됩니다.\n",
    "\n",
    "*   **[추론 시간]** \n",
    "    (예시) 평균 추론 시간은 모든 모델에서 유사하게 나타났으며, Batch Size 변경이 추론 속도에 큰 영향을 주지 않았습니다. \n",
    "    (또는) Batch Size [값] 모델이 가장 [빠른/느린] 추론 속도를 보였습니다.\n",
    "\n",
    "*   **[정성적 분석]** \n",
    "    (예시) 실제 생성된 텍스트 예시를 비교한 결과, Batch Size [값] 모델이 문맥을 더 잘 파악하고 자연스러운 문장을 생성하는 경향이 있었습니다. \n",
    "    원본 모델 대비 미세조정된 모델들이 [개선된 점/특이사항]을 보였습니다.\n",
    "\n",
    "**최종 결론:**\n",
    "\n",
    "(예시) 종합적으로 고려했을 때, Batch Size [값]으로 미세조정한 모델이 성능과 효율성 측면에서 가장 균형 잡힌 결과를 보여주었습니다. \n",
    "특정 Batch Size가 다른 크기에 비해 [우수한 점]을 나타냈으며, 이는 [이유 추론] 때문일 수 있습니다. \n",
    "향후 모델 개선 방향으로는 [제안] 등을 고려해볼 수 있습니다.\n",
    "\n",
    "*(주의: 위 결과 해석 및 결론은 실제 실행 결과에 따라 달라지므로, 노트북 실행 후 해당 섹션을 구체적인 수치와 관찰 내용으로 채워야 합니다.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
