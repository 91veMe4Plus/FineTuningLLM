{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bbf116",
   "metadata": {},
   "source": [
    "# HyperCLOVAX ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì›ë³¸ ëª¨ë¸ê³¼ 10k ë° 30k ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •ëœ HyperCLOVAX ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ë¶„ì„ ëª©í‘œ\n",
    "- ì„¸ ëª¨ë¸ ê°„ ì •ëŸ‰ì  ì„±ëŠ¥ ë¹„êµ (BLEU, ROUGE, ë¬¸ì ì •í™•ë„)\n",
    "- ì •ì„±ì  ë¶„ì„ (ì‹¤ì œ ì¶œë ¥ ì˜ˆì‹œ ë¹„êµ)\n",
    "- ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "- ì¶”ë¡  ì‹œê°„ ë° íš¨ìœ¨ì„± ë¹„êµ\n",
    "- ë¯¸ì„¸ì¡°ì • íš¨ê³¼ ë¶„ì„\n",
    "- ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "## ëª¨ë¸ ì •ë³´\n",
    "- **ì›ë³¸ ëª¨ë¸**: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- **10k ëª¨ë¸**: `hyperclova-deobfuscation-lora-with-10k-datasets`\n",
    "- **30k ëª¨ë¸**: `hyperclova-deobfuscation-lora-with-30k-datasets`\n",
    "- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: `testdata.csv` (1,002 ìƒ˜í”Œ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb29ec6",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib ë° seaborn ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ì¥ì¹˜ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5b03f",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ì—°ê²° (Colabì—ì„œ ì‹¤í–‰ ì‹œ)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # ê²½ë¡œ ì„¤ì •\n",
    "    BASE_PATH = '/content/drive/MyDrive/'\n",
    "    MODEL_10K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv'\n",
    "    \n",
    "except ImportError:\n",
    "    # ë¡œì»¬ ì‹¤í–‰ ì‹œ\n",
    "    BASE_PATH = './'\n",
    "    MODEL_10K_PATH = './hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = './hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = './testdata.csv'\n",
    "\n",
    "print(f\"ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\")\n",
    "print(f\"10k ëª¨ë¸ ê²½ë¡œ: {MODEL_10K_PATH}\")\n",
    "print(f\"30k ëª¨ë¸ ê²½ë¡œ: {MODEL_30K_PATH}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(test_df)} ìƒ˜í”Œ\")\n",
    "print(f\"ì»¬ëŸ¼ ëª©ë¡: {test_df.columns.tolist()}\")\n",
    "print(\"\\nì²« 5ê°œ ìƒ˜í”Œ:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# ë°ì´í„° í†µê³„\n",
    "print(\"\\në°ì´í„° í†µê³„:\")\n",
    "print(f\"- ì´ ìƒ˜í”Œ ìˆ˜: {len(test_df)}\")\n",
    "print(f\"- ì›ë³¸ í…ìŠ¤íŠ¸ í‰ê·  ê¸¸ì´: {test_df['original'].str.len().mean():.1f}\")\n",
    "print(f\"- ë‚œë…í™” í…ìŠ¤íŠ¸ í‰ê·  ê¸¸ì´: {test_df['obfuscated'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c660f8",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_10K_PATH)\n",
    "print(f\"í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸°: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83913712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name):\n",
    "    \"\"\"ë¡œë¼ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤\"\"\"\n",
    "    print(f\"\\n{model_name} ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    \n",
    "    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA ì–´ëŒ‘í„° ì ìš©\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"{model_name} ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"ì›ë³¸ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤\"\"\"\n",
    "    print(\"\\nì›ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    print(\"ì›ë³¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "    return base_model\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "base_model = load_base_model()\n",
    "model_10k = load_model(MODEL_10K_PATH, \"10k ë°ì´í„°ì…‹ ëª¨ë¸\")\n",
    "model_30k = load_model(MODEL_30K_PATH, \"30k ë°ì´í„°ì…‹ ëª¨ë¸\")\n",
    "\n",
    "print(\"ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb373",
   "metadata": {},
   "source": [
    "## 4. ì¶”ë¡  í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, obfuscated_text, max_length=256):\n",
    "    \"\"\"ë‚œë…í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì›ë³¸ í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "    prompt = f\"\"\"### ì§€ì‹œì‚¬í•­:\n",
    "ë‹¤ìŒ ë‚œë…í™”ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì›ë˜ í…ìŠ¤íŠ¸ë¡œ ë³µì›í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ë‚œë…í™”ëœ í…ìŠ¤íŠ¸: {obfuscated_text}\n",
    "\n",
    "### ì‘ë‹µ:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    if \"### ì‘ë‹µ:\" in response:\n",
    "        response = response.split(\"### ì‘ë‹µ:\")[1].strip()\n",
    "        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°\n",
    "        if \"<|endoftext|>\" in response:\n",
    "            response = response.split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "print(\"ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaac22",
   "metadata": {},
   "source": [
    "## 5. ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­ ë¡œë“œ\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def calculate_character_accuracy(pred, ref):\n",
    "    \"\"\"ë¬¸ì ë‹¨ìœ„ ì •í™•ë„ ê³„ì‚°\"\"\"\n",
    "    if len(ref) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    \n",
    "    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì ìˆ˜ ê³„ì‚°\n",
    "    matches = sum(1 for i, char in enumerate(pred) if i < len(ref) and char == ref[i])\n",
    "    return matches / len(ref)\n",
    "\n",
    "def calculate_exact_match(pred, ref):\n",
    "    \"\"\"ì™„ì „ ì¼ì¹˜ ì—¬ë¶€\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    # BLEU ê³„ì‚°\n",
    "    try:\n",
    "        bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE ê³„ì‚°\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    except:\n",
    "        rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    \n",
    "    # ë¬¸ì ì •í™•ë„ ê³„ì‚°\n",
    "    char_accuracies = [calculate_character_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    avg_char_accuracy = np.mean(char_accuracies)\n",
    "    \n",
    "    # ì™„ì „ ì¼ì¹˜ìœ¨ ê³„ì‚°\n",
    "    exact_matches = [calculate_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    exact_match_rate = np.mean(exact_matches)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'char_accuracy': avg_char_accuracy,\n",
    "        'exact_match': exact_match_rate,\n",
    "        'char_accuracies': char_accuracies,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "print(\"í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3322",
   "metadata": {},
   "source": [
    "## 6. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_df, sample_size=None):\n",
    "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "    if sample_size:\n",
    "        test_data = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        test_data = test_df.copy()\n",
    "    \n",
    "    print(f\"\\n{model_name} í‰ê°€ ì‹œì‘ ({len(test_data)}ê°œ ìƒ˜í”Œ)\")\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"{model_name} í‰ê°€\"):\n",
    "        obfuscated = row['obfuscated']\n",
    "        pred, inf_time = generate_deobfuscated_text(model, obfuscated)\n",
    "        predictions.append(pred)\n",
    "        inference_times.append(inf_time)\n",
    "    \n",
    "    # ì°¸ì¡° í…ìŠ¤íŠ¸\n",
    "    references = test_data['original'].tolist()\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # ì¶”ë¡  ì‹œê°„ í†µê³„\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    total_inference_time = np.sum(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'inference_times': inference_times,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'total_inference_time': total_inference_time,\n",
    "        'test_data': test_data,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} í‰ê°€ ì™„ë£Œ\")\n",
    "    print(f\"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.3f}ì´ˆ\")\n",
    "    print(f\"ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time:.1f}ì´ˆ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰ (ì „ì²´ ë°ì´í„°ì…‹ ë˜ëŠ” ìƒ˜í”Œ)\n",
    "SAMPLE_SIZE = 200  # ì „ì²´ í‰ê°€ë¥¼ ì›í•˜ë©´ Noneìœ¼ë¡œ ì„¤ì •\n",
    "\n",
    "print(\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "results_base = evaluate_model(base_model, \"ì›ë³¸ ëª¨ë¸\", test_df, SAMPLE_SIZE)\n",
    "results_10k = evaluate_model(model_10k, \"10k ëª¨ë¸\", test_df, SAMPLE_SIZE)\n",
    "results_30k = evaluate_model(model_30k, \"30k ëª¨ë¸\", test_df, SAMPLE_SIZE)\n",
    "\n",
    "print(\"\\nëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18542a41",
   "metadata": {},
   "source": [
    "## 7. ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ í‘œ ìƒì„±\n",
    "comparison_df = pd.DataFrame({\n",
    "    'ë©”íŠ¸ë¦­': ['BLEU ì ìˆ˜', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
    "               'ë¬¸ì ì •í™•ë„', 'ì •í™• ì¼ì¹˜ìœ¨', 'í‰ê·  ì¶”ë¡  ì‹œê°„(ì´ˆ)'],\n",
    "    'ì›ë³¸ ëª¨ë¸': [\n",
    "        f\"{results_base['bleu']:.4f}\",\n",
    "        f\"{results_base['rouge1']:.4f}\",\n",
    "        f\"{results_base['rouge2']:.4f}\",\n",
    "        f\"{results_base['rougeL']:.4f}\",\n",
    "        f\"{results_base['char_accuracy']:.4f}\",\n",
    "        f\"{results_base['exact_match']:.4f}\",\n",
    "        f\"{results_base['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '10k ëª¨ë¸': [\n",
    "        f\"{results_10k['bleu']:.4f}\",\n",
    "        f\"{results_10k['rouge1']:.4f}\",\n",
    "        f\"{results_10k['rouge2']:.4f}\",\n",
    "        f\"{results_10k['rougeL']:.4f}\",\n",
    "        f\"{results_10k['char_accuracy']:.4f}\",\n",
    "        f\"{results_10k['exact_match']:.4f}\",\n",
    "        f\"{results_10k['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '30k ëª¨ë¸': [\n",
    "        f\"{results_30k['bleu']:.4f}\",\n",
    "        f\"{results_30k['rouge1']:.4f}\",\n",
    "        f\"{results_30k['rouge2']:.4f}\",\n",
    "        f\"{results_30k['rougeL']:.4f}\",\n",
    "        f\"{results_30k['char_accuracy']:.4f}\",\n",
    "        f\"{results_30k['exact_match']:.4f}\",\n",
    "        f\"{results_30k['avg_inference_time']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ ë¥  ê³„ì‚°\n",
    "print(\"\\n=== ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ ìœ¨ ===\")\n",
    "metrics_to_compare = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy', 'exact_match']\n",
    "print(f\"{'Metric':<15} {'10k ëª¨ë¸':<15} {'30k ëª¨ë¸':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in metrics_to_compare:\n",
    "    improvement_10k = ((results_10k[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    improvement_30k = ((results_30k[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    print(f\"{metric.upper():<15} {improvement_10k:+7.2f}%      {improvement_30k:+7.2f}%\")\n",
    "\n",
    "# 10k vs 30k ëª¨ë¸ ë¹„êµ\n",
    "print(\"\\n=== 30k ëª¨ë¸ vs 10k ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ìœ¨ ===\")\n",
    "for metric in metrics_to_compare:\n",
    "    improvement = ((results_30k[metric] - results_10k[metric]) / results_10k[metric]) * 100 if results_10k[metric] > 0 else 0\n",
    "    print(f\"{metric.upper()}: {improvement:+.2f}%\")\n",
    "\n",
    "# ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨ ë¹„êµ\n",
    "time_ratio_base_10k = results_10k['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_base_30k = results_30k['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_10k_30k = results_30k['avg_inference_time'] / results_10k['avg_inference_time']\n",
    "\n",
    "print(f\"\\n=== ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨ ë¹„êµ ===\")\n",
    "print(f\"ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨ (10k/ì›ë³¸): {time_ratio_base_10k:.2f}x\")\n",
    "print(f\"ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨ (30k/ì›ë³¸): {time_ratio_base_30k:.2f}x\")\n",
    "print(f\"ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨ (30k/10k): {time_ratio_10k_30k:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540b5d9",
   "metadata": {},
   "source": [
    "## 8. ì‹œê°í™” ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metric Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Analysis (Base vs Fine-tuned)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_data = {\n",
    "    'BLEU': [results_base['bleu'], results_10k['bleu'], results_30k['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_10k['rouge1'], results_30k['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_10k['rouge2'], results_30k['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_10k['rougeL'], results_30k['rougeL']],\n",
    "    'Character Accuracy': [results_base['char_accuracy'], results_10k['char_accuracy'], results_30k['char_accuracy']],\n",
    "    'Exact Match': [results_base['exact_match'], results_10k['exact_match'], results_30k['exact_match']]\n",
    "}\n",
    "\n",
    "models = ['Base Model', '10k Model', '30k Model']\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "\n",
    "for idx, (metric, values) in enumerate(metrics_data.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    bars = axes[row, col].bar(models, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{metric}', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Score')\n",
    "    axes[row, col].set_ylim(0, max(values) * 1.1)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Display values\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Character Accuracy Distribution Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# ì›ë³¸ ëª¨ë¸\n",
    "axes[0].hist(results_base['char_accuracies'], bins=20, alpha=0.7, color='lightgray', edgecolor='black')\n",
    "axes[0].set_title('Base Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Character Accuracy')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(results_base['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_base[\"char_accuracy\"]:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# 10k ëª¨ë¸\n",
    "axes[1].hist(results_10k['char_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('10k Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Character Accuracy')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(results_10k['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_10k[\"char_accuracy\"]:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# 30k ëª¨ë¸\n",
    "axes[2].hist(results_30k['char_accuracies'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title('30k Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[2].set_xlabel('Character Accuracy')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(results_30k['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_30k[\"char_accuracy\"]:.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference Time Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Inference time distribution\n",
    "inference_data = [results_base['inference_times'], results_10k['inference_times'], results_30k['inference_times']]\n",
    "labels = ['Base Model', '10k Model', '30k Model']\n",
    "\n",
    "axes[0].boxplot(inference_data, labels=labels, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[0].set_title('Inference Time Distribution Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('Inference Time (seconds)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average inference time bar chart\n",
    "avg_times = [results_base['avg_inference_time'], results_10k['avg_inference_time'], results_30k['avg_inference_time']]\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "bars = axes[1].bar(labels, avg_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Inference Time Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('Average Inference Time (seconds)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, avg_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c6546",
   "metadata": {},
   "source": [
    "## 9. ì§ˆì  ë¶„ì„ - ì˜ˆì‹œ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ì¥ ì„±ëŠ¥ ì°¨ì´ê°€ í° ìƒ˜í”Œë“¤ ì°¾ê¸°\n",
    "def find_performance_difference_samples(results_base, results_10k, results_30k, n_samples=5):\n",
    "    \"\"\"ì„¸ ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í° ìƒ˜í”Œë“¤ ì°¾ê¸°\"\"\"\n",
    "    char_acc_base = np.array(results_base['char_accuracies'])\n",
    "    char_acc_10k = np.array(results_10k['char_accuracies'])\n",
    "    char_acc_30k = np.array(results_30k['char_accuracies'])\n",
    "    \n",
    "    # ë¯¸ì„¸ì¡°ì • íš¨ê³¼ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ vs ì›ë³¸)\n",
    "    max_finetuned = np.maximum(char_acc_10k, char_acc_30k)\n",
    "    finetuning_improvement = max_finetuned - char_acc_base\n",
    "    \n",
    "    # 30k vs 10k ë¹„êµ\n",
    "    diff_30k_10k = char_acc_30k - char_acc_10k\n",
    "    \n",
    "    # ê°€ì¥ ë¯¸ì„¸ì¡°ì • íš¨ê³¼ê°€ í° ì¸ë±ìŠ¤ë“¤\n",
    "    best_finetuning_idx = np.argsort(finetuning_improvement)[-n_samples:][::-1]\n",
    "    worst_finetuning_idx = np.argsort(finetuning_improvement)[:n_samples]\n",
    "    \n",
    "    # 30kê°€ 10kë³´ë‹¤ í›¨ì”¬ ì¢‹ì€/ë‚˜ìœ ê²½ìš°\n",
    "    best_30k_vs_10k_idx = np.argsort(diff_30k_10k)[-n_samples:][::-1]\n",
    "    worst_30k_vs_10k_idx = np.argsort(diff_30k_10k)[:n_samples]\n",
    "    \n",
    "    return best_finetuning_idx, worst_finetuning_idx, best_30k_vs_10k_idx, worst_30k_vs_10k_idx\n",
    "\n",
    "best_ft_idx, worst_ft_idx, best_30k_idx, worst_30k_idx = find_performance_difference_samples(results_base, results_10k, results_30k)\n",
    "\n",
    "print(\"=== ë¯¸ì„¸ì¡°ì •ì´ ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ê°€ì¥ íš¨ê³¼ì ì´ì—ˆë˜ ì˜ˆì‹œ ===\")\n",
    "for i, idx in enumerate(best_ft_idx):\n",
    "    print(f\"\\n[ì˜ˆì‹œ {i+1}]\")\n",
    "    print(f\"ë‚œë…í™” í…ìŠ¤íŠ¸: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"ì •ë‹µ í…ìŠ¤íŠ¸: {results_base['references'][idx]}\")\n",
    "    print(f\"ì›ë³¸ ëª¨ë¸ ì˜ˆì¸¡: {results_base['predictions'][idx]}\")\n",
    "    print(f\"10k ëª¨ë¸ ì˜ˆì¸¡: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k ëª¨ë¸ ì˜ˆì¸¡: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"ë¬¸ì ì •í™•ë„ - ì›ë³¸: {results_base['char_accuracies'][idx]:.3f}, 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 30k ëª¨ë¸ì´ 10k ëª¨ë¸ ëŒ€ë¹„ í¬ê²Œ ìš°ìˆ˜í–ˆë˜ ì˜ˆì‹œ ===\")\n",
    "for i, idx in enumerate(best_30k_idx):\n",
    "    print(f\"\\n[ì˜ˆì‹œ {i+1}]\")\n",
    "    print(f\"ë‚œë…í™” í…ìŠ¤íŠ¸: {results_10k['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"ì •ë‹µ í…ìŠ¤íŠ¸: {results_10k['references'][idx]}\")\n",
    "    print(f\"10k ëª¨ë¸ ì˜ˆì¸¡: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k ëª¨ë¸ ì˜ˆì¸¡: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"ë¬¸ì ì •í™•ë„ - 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== ë¯¸ì„¸ì¡°ì • íš¨ê³¼ê°€ ì œí•œì ì´ì—ˆë˜ ì˜ˆì‹œ ===\")\n",
    "for i, idx in enumerate(worst_ft_idx):\n",
    "    print(f\"\\n[ì˜ˆì‹œ {i+1}]\")\n",
    "    print(f\"ë‚œë…í™” í…ìŠ¤íŠ¸: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"ì •ë‹µ í…ìŠ¤íŠ¸: {results_base['references'][idx]}\")\n",
    "    print(f\"ì›ë³¸ ëª¨ë¸ ì˜ˆì¸¡: {results_base['predictions'][idx]}\")\n",
    "    print(f\"10k ëª¨ë¸ ì˜ˆì¸¡: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k ëª¨ë¸ ì˜ˆì¸¡: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"ë¬¸ì ì •í™•ë„ - ì›ë³¸: {results_base['char_accuracies'][idx]:.3f}, 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91017b0",
   "metadata": {},
   "source": [
    "## 10. ìƒì„¸ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "def analyze_by_text_length(results, model_name):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
    "    test_data = results['test_data']\n",
    "    char_accuracies = results['char_accuracies']\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°\n",
    "    text_lengths = test_data['original'].str.len()\n",
    "    \n",
    "    # ê¸¸ì´ êµ¬ê°„ë³„ë¡œ ë¶„ë¥˜\n",
    "    length_bins = [0, 20, 50, 100, 200, float('inf')]\n",
    "    length_labels = ['â‰¤20 chars', '21-50 chars', '51-100 chars', '101-200 chars', '200+ chars']\n",
    "    \n",
    "    length_categories = pd.cut(text_lengths, bins=length_bins, labels=length_labels, right=False)\n",
    "    \n",
    "    # êµ¬ê°„ë³„ í‰ê·  ì„±ëŠ¥\n",
    "    performance_by_length = []\n",
    "    for category in length_labels:\n",
    "        mask = length_categories == category\n",
    "        if mask.sum() > 0:\n",
    "            avg_acc = np.mean(np.array(char_accuracies)[mask])\n",
    "            count = mask.sum()\n",
    "            performance_by_length.append({\n",
    "                'length_category': category,\n",
    "                'avg_char_accuracy': avg_acc,\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(performance_by_length)\n",
    "\n",
    "# ì„¸ ëª¨ë¸ì˜ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "length_analysis_base = analyze_by_text_length(results_base, \"ì›ë³¸ ëª¨ë¸\")\n",
    "length_analysis_10k = analyze_by_text_length(results_10k, \"10k ëª¨ë¸\")\n",
    "length_analysis_30k = analyze_by_text_length(results_30k, \"30k ëª¨ë¸\")\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(length_analysis_base))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, length_analysis_base['avg_char_accuracy'], width, \n",
    "               label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, length_analysis_10k['avg_char_accuracy'], width,\n",
    "               label='10k Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, length_analysis_30k['avg_char_accuracy'], width,\n",
    "               label='30k Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Text Length Category')\n",
    "ax.set_ylabel('Average Character Accuracy')\n",
    "ax.set_title('Model Performance Comparison by Text Length (Base vs Fine-tuned)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(length_analysis_base['length_category'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ===\")\n",
    "combined_length_analysis = pd.merge(\n",
    "    pd.merge(length_analysis_base, length_analysis_10k, on='length_category', suffixes=('_base', '_10k')),\n",
    "    length_analysis_30k, on='length_category'\n",
    ")\n",
    "combined_length_analysis.columns = ['length_category', 'avg_char_accuracy_base', 'count_base', \n",
    "                                   'avg_char_accuracy_10k', 'count_10k', 'avg_char_accuracy_30k', 'count_30k']\n",
    "print(combined_length_analysis[['length_category', 'avg_char_accuracy_base', 'avg_char_accuracy_10k', 'avg_char_accuracy_30k']])\n",
    "\n",
    "# ê¸¸ì´ë³„ ë¯¸ì„¸ì¡°ì • íš¨ê³¼ ë¶„ì„\n",
    "print(\"\\n=== ê¸¸ì´ë³„ ë¯¸ì„¸ì¡°ì • íš¨ê³¼ ===\")\n",
    "for _, row in combined_length_analysis.iterrows():\n",
    "    category = row['length_category']\n",
    "    base_acc = row['avg_char_accuracy_base']\n",
    "    acc_10k = row['avg_char_accuracy_10k']\n",
    "    acc_30k = row['avg_char_accuracy_30k']\n",
    "    \n",
    "    improvement_10k = ((acc_10k - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    improvement_30k = ((acc_30k - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    \n",
    "    print(f\"{category}: 10k ê°œì„  {improvement_10k:+.1f}%, 30k ê°œì„  {improvement_30k:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì™„ì „ ì¼ì¹˜ ë° ë¶€ë¶„ ì¼ì¹˜ ë¶„ì„\n",
    "def analyze_match_types(results, model_name):\n",
    "    \"\"\"ì™„ì „ ì¼ì¹˜ ë° ë¶€ë¶„ ì¼ì¹˜ ë¶„ì„\"\"\"\n",
    "    predictions = results['predictions']\n",
    "    references = results['references']\n",
    "    \n",
    "    perfect_matches = 0\n",
    "    high_accuracy = 0  # 90% ì´ìƒ\n",
    "    medium_accuracy = 0  # 70-90%\n",
    "    low_accuracy = 0  # 70% ë¯¸ë§Œ\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        char_acc = calculate_character_accuracy(pred, ref)\n",
    "        \n",
    "        if pred.strip() == ref.strip():\n",
    "            perfect_matches += 1\n",
    "        elif char_acc >= 0.9:\n",
    "            high_accuracy += 1\n",
    "        elif char_acc >= 0.7:\n",
    "            medium_accuracy += 1\n",
    "        else:\n",
    "            low_accuracy += 1\n",
    "    \n",
    "    total = len(predictions)\n",
    "    \n",
    "    return {\n",
    "        'perfect_match': perfect_matches,\n",
    "        'high_accuracy': high_accuracy,\n",
    "        'medium_accuracy': medium_accuracy,\n",
    "        'low_accuracy': low_accuracy,\n",
    "        'perfect_match_rate': perfect_matches / total,\n",
    "        'high_accuracy_rate': high_accuracy / total,\n",
    "        'medium_accuracy_rate': medium_accuracy / total,\n",
    "        'low_accuracy_rate': low_accuracy / total\n",
    "    }\n",
    "\n",
    "match_analysis_base = analyze_match_types(results_base, \"ì›ë³¸ ëª¨ë¸\")\n",
    "match_analysis_10k = analyze_match_types(results_10k, \"10k ëª¨ë¸\")\n",
    "match_analysis_30k = analyze_match_types(results_30k, \"30k ëª¨ë¸\")\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "categories = ['Perfect Match', 'High Accuracy\\n(90%+)', 'Medium Accuracy\\n(70-90%)', 'Low Accuracy\\n(<70%)']\n",
    "values_base = [match_analysis_base['perfect_match_rate'], \n",
    "               match_analysis_base['high_accuracy_rate'],\n",
    "               match_analysis_base['medium_accuracy_rate'], \n",
    "               match_analysis_base['low_accuracy_rate']]\n",
    "values_10k = [match_analysis_10k['perfect_match_rate'], \n",
    "              match_analysis_10k['high_accuracy_rate'],\n",
    "              match_analysis_10k['medium_accuracy_rate'], \n",
    "              match_analysis_10k['low_accuracy_rate']]\n",
    "values_30k = [match_analysis_30k['perfect_match_rate'], \n",
    "              match_analysis_30k['high_accuracy_rate'],\n",
    "              match_analysis_30k['medium_accuracy_rate'], \n",
    "              match_analysis_30k['low_accuracy_rate']]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, values_base, width, label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, values_10k, width, label='10k Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, values_30k, width, label='30k Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Accuracy Category')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Sample Distribution by Accuracy Category (Base vs Fine-tuned)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars, values in zip([bars1, bars2, bars3], [values_base, values_10k, values_30k]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== ì •í™•ë„ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼ ===\")\n",
    "print(f\"{'Category':<20} {'Base Model':<15} {'10k Model':<15} {'30k Model':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for i, category in enumerate(categories):\n",
    "    print(f\"{category:<20} {values_base[i]:<15.1%} {values_10k[i]:<15.1%} {values_30k[i]:<15.1%}\")\n",
    "\n",
    "print(\"\\n=== ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ ìœ¨ ===\")\n",
    "print(f\"{'Category':<20} {'10k Improvement':<20} {'30k Improvement':<20}\")\n",
    "print(\"-\" * 65)\n",
    "for i, category in enumerate(categories):\n",
    "    improvement_10k = ((values_10k[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_10k[i] > 0 else 0\n",
    "    improvement_30k = ((values_30k[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_30k[i] > 0 else 0\n",
    "    \n",
    "    if improvement_10k == float('inf'):\n",
    "        imp_10k_str = \"N/A (0â†’+)\"\n",
    "    else:\n",
    "        imp_10k_str = f\"{improvement_10k:+.1f}%\"\n",
    "        \n",
    "    if improvement_30k == float('inf'):\n",
    "        imp_30k_str = \"N/A (0â†’+)\"\n",
    "    else:\n",
    "        imp_30k_str = f\"{improvement_30k:+.1f}%\"\n",
    "    \n",
    "    print(f\"{category:<20} {imp_10k_str:<20} {imp_30k_str:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5877e",
   "metadata": {},
   "source": [
    "## 11. ì¢…í•© ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dcacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"ğŸ” HyperCLOVAX ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ - ì¢…í•© ê²°ë¡  (ì›ë³¸ vs ë¯¸ì„¸ì¡°ì •)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š **ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ë¯¸ì„¸ì¡°ì • ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**\n",
    "\n",
    "ğŸ“ˆ **BLEU ì ìˆ˜**\n",
    "- ì›ë³¸: {results_base['bleu']:.4f}\n",
    "- 10k: {results_10k['bleu']:.4f} (ê°œì„ ìœ¨: {((results_10k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['bleu']:.4f} (ê°œì„ ìœ¨: {((results_30k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "\n",
    "ğŸ“ˆ **ROUGE-L ì ìˆ˜**\n",
    "- ì›ë³¸: {results_base['rougeL']:.4f}\n",
    "- 10k: {results_10k['rougeL']:.4f} (ê°œì„ ìœ¨: {((results_10k['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['rougeL']:.4f} (ê°œì„ ìœ¨: {((results_30k['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "\n",
    "ğŸ“ˆ **ë¬¸ì ì •í™•ë„**\n",
    "- ì›ë³¸: {results_base['char_accuracy']:.4f}\n",
    "- 10k: {results_10k['char_accuracy']:.4f} (ê°œì„ ìœ¨: {((results_10k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['char_accuracy']:.4f} (ê°œì„ ìœ¨: {((results_30k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "\n",
    "ğŸ“ˆ **ì™„ì „ ì¼ì¹˜ìœ¨**\n",
    "- ì›ë³¸: {results_base['exact_match']:.4f}\n",
    "- 10k: {results_10k['exact_match']:.4f} (ê°œì„ ìœ¨: {((results_10k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "- 30k: {results_30k['exact_match']:.4f} (ê°œì„ ìœ¨: {((results_30k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "\n",
    "â±ï¸ **íš¨ìœ¨ì„± ë¶„ì„**\n",
    "- ì›ë³¸ ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {results_base['avg_inference_time']:.3f}ì´ˆ\n",
    "- 10k ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {results_10k['avg_inference_time']:.3f}ì´ˆ ({results_10k['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "- 30k ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {results_30k['avg_inference_time']:.3f}ì´ˆ ({results_30k['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "\n",
    "ğŸ’¯ **ë³µì› í’ˆì§ˆ ë¶„ì„**\n",
    "- ì›ë³¸ ëª¨ë¸ ì™„ì „ ì¼ì¹˜: {match_analysis_base['perfect_match']}ê°œ ({match_analysis_base['perfect_match_rate']:.1%})\n",
    "- 10k ëª¨ë¸ ì™„ì „ ì¼ì¹˜: {match_analysis_10k['perfect_match']}ê°œ ({match_analysis_10k['perfect_match_rate']:.1%})\n",
    "- 30k ëª¨ë¸ ì™„ì „ ì¼ì¹˜: {match_analysis_30k['perfect_match']}ê°œ ({match_analysis_30k['perfect_match_rate']:.1%})\n",
    "\n",
    "ğŸ¯ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**\n",
    "1. ë¯¸ì„¸ì¡°ì •ì´ ì›ë³¸ ëª¨ë¸ ëŒ€ë¹„ ëª¨ë“  ì§€í‘œì—ì„œ í˜„ì €í•œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´\n",
    "2. 30k ë°ì´í„°ì…‹ ëª¨ë¸ì´ 10k ë°ì´í„°ì…‹ ëª¨ë¸ë³´ë‹¤ ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì„\n",
    "3. ì™„ì „ ì¼ì¹˜ìœ¨ì—ì„œ ê°€ì¥ ë“œë¼ë§ˆí‹±í•œ ê°œì„ ì„ í™•ì¸ (ì •í™•í•œ ë³µì› ëŠ¥ë ¥ í–¥ìƒ)\n",
    "4. ì¶”ë¡  ì‹œê°„ ì¦ê°€ëŠ” ë¯¸ë¯¸í•˜ì—¬ íš¨ìœ¨ì„± ì €í•˜ ì—†ì´ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±\n",
    "5. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í–¥ìƒ í™•ì¸\n",
    "\n",
    "ğŸ’¡ **ê¶Œì¥ ì‚¬í•­**\n",
    "- ë‚œë…í™” í•´ì œ ì‘ì—…ì—ëŠ” 30k ë°ì´í„°ì…‹ ëª¨ë¸ ì‚¬ìš© ê°•ë ¥ ê¶Œì¥\n",
    "- ë” ë§ì€ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ ì‹œ ë” í° ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€ ê°€ëŠ¥\n",
    "- ì›ë³¸ ëª¨ë¸ì˜ ì œí•œì  ì„±ëŠ¥ì„ ê³ ë ¤í•  ë•Œ ë¯¸ì„¸ì¡°ì •ì˜ íš¨ê³¼ê°€ ë§¤ìš° ì˜ë¯¸ ìˆìŒ\n",
    "- ë„ë©”ì¸ íŠ¹í™” ì‘ì—…ì—ì„œ ë¯¸ì„¸ì¡°ì •ì˜ ì¤‘ìš”ì„± ì…ì¦\n",
    "\"\"\") \n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd32b3",
   "metadata": {},
   "source": [
    "## 12. ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "results_summary = pd.DataFrame({\n",
    "    'ëª¨ë¸': ['ì›ë³¸ ëª¨ë¸', '10k ë°ì´í„°ì…‹ ëª¨ë¸', '30k ë°ì´í„°ì…‹ ëª¨ë¸'],\n",
    "    'BLEU ì ìˆ˜': [results_base['bleu'], results_10k['bleu'], results_30k['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_10k['rouge1'], results_30k['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_10k['rouge2'], results_30k['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_10k['rougeL'], results_30k['rougeL']],\n",
    "    'ë¬¸ì ì •í™•ë„': [results_base['char_accuracy'], results_10k['char_accuracy'], results_30k['char_accuracy']],\n",
    "    'ì •í™• ì¼ì¹˜ìœ¨': [results_base['exact_match'], results_10k['exact_match'], results_30k['exact_match']],\n",
    "    'í‰ê·  ì¶”ë¡  ì‹œê°„': [results_base['avg_inference_time'], results_10k['avg_inference_time'], results_30k['avg_inference_time']],\n",
    "    'ì´ ì¶”ë¡  ì‹œê°„': [results_base['total_inference_time'], results_10k['total_inference_time'], results_30k['total_inference_time']]\n",
    "})\n",
    "\n",
    "# ìƒì„¸ ê²°ê³¼ë„ ì €ì¥\n",
    "detailed_results = pd.DataFrame({\n",
    "    'ì¸ë±ìŠ¤': range(len(results_base['predictions'])),\n",
    "    'ì›ë³¸': results_base['references'],\n",
    "    'ë‚œë…í™”': results_base['test_data']['obfuscated'].tolist(),\n",
    "    'ì˜ˆì¸¡_ì›ë³¸': results_base['predictions'],\n",
    "    'ì˜ˆì¸¡_10k': results_10k['predictions'],\n",
    "    'ì˜ˆì¸¡_30k': results_30k['predictions'],\n",
    "    'ë¬¸ì_ì •í™•ë„_ì›ë³¸': results_base['char_accuracies'],\n",
    "    'ë¬¸ì_ì •í™•ë„_10k': results_10k['char_accuracies'],\n",
    "    'ë¬¸ì_ì •í™•ë„_30k': results_30k['char_accuracies'],\n",
    "    'ì •í™•_ì¼ì¹˜_ì›ë³¸': results_base['exact_matches'],\n",
    "    'ì •í™•_ì¼ì¹˜_10k': results_10k['exact_matches'],\n",
    "    'ì •í™•_ì¼ì¹˜_30k': results_30k['exact_matches'],\n",
    "    'ì¶”ë¡ _ì‹œê°„_ì›ë³¸': results_base['inference_times'],\n",
    "    'ì¶”ë¡ _ì‹œê°„_10k': results_10k['inference_times'],\n",
    "    'ì¶”ë¡ _ì‹œê°„_30k': results_30k['inference_times']\n",
    "})\n",
    "\n",
    "# ë¯¸ì„¸ì¡°ì • íš¨ê³¼ ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "finetuning_analysis = pd.DataFrame({\n",
    "    'ëª¨ë¸': ['10k vs ì›ë³¸', '30k vs ì›ë³¸', '30k vs 10k'],\n",
    "    'BLEU_ê°œì„ ìœ¨': [\n",
    "        ((results_10k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_30k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_30k['bleu'] - results_10k['bleu']) / results_10k['bleu'] * 100) if results_10k['bleu'] > 0 else 0\n",
    "    ],\n",
    "    'ë¬¸ìì •í™•ë„_ê°œì„ ìœ¨': [\n",
    "        ((results_10k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_30k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_30k['char_accuracy'] - results_10k['char_accuracy']) / results_10k['char_accuracy'] * 100)\n",
    "    ],\n",
    "    'ì™„ì „ì¼ì¹˜ìœ¨_ê°œì„ ìœ¨': [\n",
    "        ((results_10k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_30k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_30k['exact_match'] - results_10k['exact_match']) / results_10k['exact_match'] * 100) if results_10k['exact_match'] > 0 else float('inf')\n",
    "    ]\n",
    "})\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "results_summary.to_csv('model_performance_summary_with_base.csv', index=False, encoding='utf-8-sig')\n",
    "detailed_results.to_csv('detailed_model_comparison_with_base.csv', index=False, encoding='utf-8-sig')\n",
    "finetuning_analysis.to_csv('finetuning_effect_analysis.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ:\")\n",
    "print(\"- model_performance_summary_with_base.csv: ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½ (ì›ë³¸ ëª¨ë¸ í¬í•¨)\")\n",
    "print(\"- detailed_model_comparison_with_base.csv: ìƒì„¸ ë¹„êµ ê²°ê³¼ (ì›ë³¸ ëª¨ë¸ í¬í•¨)\")\n",
    "print(\"- finetuning_effect_analysis.csv: ë¯¸ì„¸ì¡°ì • íš¨ê³¼ ë¶„ì„\")\n",
    "\n",
    "# Google Colabì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('model_performance_summary_with_base.csv')\n",
    "    files.download('detailed_model_comparison_with_base.csv')\n",
    "    files.download('finetuning_effect_analysis.csv')\n",
    "    print(\"ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’¾ ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "print(\"\\n=== ìµœì¢… ì„±ëŠ¥ ìš”ì•½ ===\")\n",
    "print(results_summary.round(4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
