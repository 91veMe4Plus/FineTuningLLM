{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bbf116",
   "metadata": {},
   "source": [
    "# HyperCLOVAX 모델 성능 비교 분석\n",
    "\n",
    "이 노트북은 원본 모델과 10k 및 30k 데이터셋으로 미세조정된 HyperCLOVAX 모델의 성능을 비교합니다.\n",
    "\n",
    "## 분석 목표\n",
    "- 세 모델 간 정량적 성능 비교 (BLEU, ROUGE, 문자 정확도)\n",
    "- 정성적 분석 (실제 출력 예시 비교)\n",
    "- 카테고리별 성능 분석\n",
    "- 추론 시간 및 효율성 비교\n",
    "- 미세조정 효과 분석\n",
    "- 결과 시각화\n",
    "\n",
    "## 모델 정보\n",
    "- **원본 모델**: `naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B`\n",
    "- **10k 모델**: `hyperclova-deobfuscation-lora-with-10k-datasets`\n",
    "- **30k 모델**: `hyperclova-deobfuscation-lora-with-30k-datasets`\n",
    "- **테스트 데이터**: `testdata.csv` (1,002 샘플)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb29ec6",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge-score\n",
    "!pip install -q sacrebleu\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"패키지 설치 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib 및 seaborn 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5b03f",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 연결 (Colab에서 실행 시)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # 경로 설정\n",
    "    BASE_PATH = '/content/drive/MyDrive/'\n",
    "    MODEL_10K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = BASE_PATH + 'hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = BASE_PATH + 'testdata.csv'\n",
    "    \n",
    "except ImportError:\n",
    "    # 로컬 실행 시\n",
    "    BASE_PATH = './'\n",
    "    MODEL_10K_PATH = './hyperclova-deobfuscation-lora-with-10k-datasets'\n",
    "    MODEL_30K_PATH = './hyperclova-deobfuscation-lora-with-30k-datasets'\n",
    "    TEST_DATA_PATH = './testdata.csv'\n",
    "\n",
    "print(f\"경로 설정 완료:\")\n",
    "print(f\"10k 모델 경로: {MODEL_10K_PATH}\")\n",
    "print(f\"30k 모델 경로: {MODEL_30K_PATH}\")\n",
    "print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"테스트 데이터 크기: {len(test_df)} 샘플\")\n",
    "print(f\"컬럼 목록: {test_df.columns.tolist()}\")\n",
    "print(\"\\n첫 5개 샘플:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 데이터 통계\n",
    "print(\"\\n데이터 통계:\")\n",
    "print(f\"- 총 샘플 수: {len(test_df)}\")\n",
    "print(f\"- 원본 텍스트 평균 길이: {test_df['original'].str.len().mean():.1f}\")\n",
    "print(f\"- 난독화 텍스트 평균 길이: {test_df['obfuscated'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c660f8",
   "metadata": {},
   "source": [
    "## 3. 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 이름 설정\n",
    "BASE_MODEL_NAME = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_10K_PATH)\n",
    "print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83913712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name):\n",
    "    \"\"\"로라 모델을 로드합니다\"\"\"\n",
    "    print(f\"\\n{model_name} 모델 로딩 중...\")\n",
    "    \n",
    "    # 베이스 모델 로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 어댑터 적용\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"{model_name} 모델 로딩 완료\")\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"원본 베이스 모델을 로드합니다\"\"\"\n",
    "    print(\"\\n원본 모델 로딩 중...\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    print(\"원본 모델 로딩 완료\")\n",
    "    return base_model\n",
    "\n",
    "# 모델 로드\n",
    "base_model = load_base_model()\n",
    "model_10k = load_model(MODEL_10K_PATH, \"10k 데이터셋 모델\")\n",
    "model_30k = load_model(MODEL_30K_PATH, \"30k 데이터셋 모델\")\n",
    "\n",
    "print(\"모든 모델 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb373",
   "metadata": {},
   "source": [
    "## 4. 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deobfuscated_text(model, obfuscated_text, max_length=256):\n",
    "    \"\"\"난독화된 텍스트를 입력받아 원본 텍스트 생성\"\"\"\n",
    "    prompt = f\"\"\"### 지시사항:\n",
    "다음 난독화된 한국어 텍스트를 원래 텍스트로 복원해주세요.\n",
    "\n",
    "난독화된 텍스트: {obfuscated_text}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 응답 부분만 추출\n",
    "    if \"### 응답:\" in response:\n",
    "        response = response.split(\"### 응답:\")[1].strip()\n",
    "        # 불필요한 부분 제거\n",
    "        if \"<|endoftext|>\" in response:\n",
    "            response = response.split(\"<|endoftext|>\")[0].strip()\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "print(\"추론 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaac22",
   "metadata": {},
   "source": [
    "## 5. 성능 평가 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 메트릭 로드\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def calculate_character_accuracy(pred, ref):\n",
    "    \"\"\"문자 단위 정확도 계산\"\"\"\n",
    "    if len(ref) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    \n",
    "    # 정확히 일치하는 문자 수 계산\n",
    "    matches = sum(1 for i, char in enumerate(pred) if i < len(ref) and char == ref[i])\n",
    "    return matches / len(ref)\n",
    "\n",
    "def calculate_exact_match(pred, ref):\n",
    "    \"\"\"완전 일치 여부\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"모든 메트릭 계산\"\"\"\n",
    "    # BLEU 계산\n",
    "    try:\n",
    "        bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    try:\n",
    "        rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    except:\n",
    "        rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    \n",
    "    # 문자 정확도 계산\n",
    "    char_accuracies = [calculate_character_accuracy(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    avg_char_accuracy = np.mean(char_accuracies)\n",
    "    \n",
    "    # 완전 일치율 계산\n",
    "    exact_matches = [calculate_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    exact_match_rate = np.mean(exact_matches)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'char_accuracy': avg_char_accuracy,\n",
    "        'exact_match': exact_match_rate,\n",
    "        'char_accuracies': char_accuracies,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "print(\"평가 메트릭 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3322",
   "metadata": {},
   "source": [
    "## 6. 모델 성능 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_df, sample_size=None):\n",
    "    \"\"\"모델 성능 평가\"\"\"\n",
    "    if sample_size:\n",
    "        test_data = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        test_data = test_df.copy()\n",
    "    \n",
    "    print(f\"\\n{model_name} 평가 시작 ({len(test_data)}개 샘플)\")\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=f\"{model_name} 평가\"):\n",
    "        obfuscated = row['obfuscated']\n",
    "        pred, inf_time = generate_deobfuscated_text(model, obfuscated)\n",
    "        predictions.append(pred)\n",
    "        inference_times.append(inf_time)\n",
    "    \n",
    "    # 참조 텍스트\n",
    "    references = test_data['original'].tolist()\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # 추론 시간 통계\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    total_inference_time = np.sum(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'inference_times': inference_times,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'total_inference_time': total_inference_time,\n",
    "        'test_data': test_data,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} 평가 완료\")\n",
    "    print(f\"평균 추론 시간: {avg_inference_time:.3f}초\")\n",
    "    print(f\"총 추론 시간: {total_inference_time:.1f}초\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 평가 실행 (전체 데이터셋 또는 샘플)\n",
    "SAMPLE_SIZE = 200  # 전체 평가를 원하면 None으로 설정\n",
    "\n",
    "print(\"모델 성능 평가를 시작합니다...\")\n",
    "results_base = evaluate_model(base_model, \"원본 모델\", test_df, SAMPLE_SIZE)\n",
    "results_10k = evaluate_model(model_10k, \"10k 모델\", test_df, SAMPLE_SIZE)\n",
    "results_30k = evaluate_model(model_30k, \"30k 모델\", test_df, SAMPLE_SIZE)\n",
    "\n",
    "print(\"\\n모든 모델 평가 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18542a41",
   "metadata": {},
   "source": [
    "## 7. 성능 비교 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 표 생성\n",
    "comparison_df = pd.DataFrame({\n",
    "    '메트릭': ['BLEU 점수', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
    "               '문자 정확도', '정확 일치율', '평균 추론 시간(초)'],\n",
    "    '원본 모델': [\n",
    "        f\"{results_base['bleu']:.4f}\",\n",
    "        f\"{results_base['rouge1']:.4f}\",\n",
    "        f\"{results_base['rouge2']:.4f}\",\n",
    "        f\"{results_base['rougeL']:.4f}\",\n",
    "        f\"{results_base['char_accuracy']:.4f}\",\n",
    "        f\"{results_base['exact_match']:.4f}\",\n",
    "        f\"{results_base['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '10k 모델': [\n",
    "        f\"{results_10k['bleu']:.4f}\",\n",
    "        f\"{results_10k['rouge1']:.4f}\",\n",
    "        f\"{results_10k['rouge2']:.4f}\",\n",
    "        f\"{results_10k['rougeL']:.4f}\",\n",
    "        f\"{results_10k['char_accuracy']:.4f}\",\n",
    "        f\"{results_10k['exact_match']:.4f}\",\n",
    "        f\"{results_10k['avg_inference_time']:.3f}\"\n",
    "    ],\n",
    "    '30k 모델': [\n",
    "        f\"{results_30k['bleu']:.4f}\",\n",
    "        f\"{results_30k['rouge1']:.4f}\",\n",
    "        f\"{results_30k['rouge2']:.4f}\",\n",
    "        f\"{results_30k['rougeL']:.4f}\",\n",
    "        f\"{results_30k['char_accuracy']:.4f}\",\n",
    "        f\"{results_30k['exact_match']:.4f}\",\n",
    "        f\"{results_30k['avg_inference_time']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== 모델 성능 비교 결과 ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 원본 모델 대비 성능 개선률 계산\n",
    "print(\"\\n=== 원본 모델 대비 성능 개선율 ===\")\n",
    "metrics_to_compare = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'char_accuracy', 'exact_match']\n",
    "print(f\"{'Metric':<15} {'10k 모델':<15} {'30k 모델':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in metrics_to_compare:\n",
    "    improvement_10k = ((results_10k[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    improvement_30k = ((results_30k[metric] - results_base[metric]) / results_base[metric]) * 100 if results_base[metric] > 0 else 0\n",
    "    print(f\"{metric.upper():<15} {improvement_10k:+7.2f}%      {improvement_30k:+7.2f}%\")\n",
    "\n",
    "# 10k vs 30k 모델 비교\n",
    "print(\"\\n=== 30k 모델 vs 10k 모델 성능 개선율 ===\")\n",
    "for metric in metrics_to_compare:\n",
    "    improvement = ((results_30k[metric] - results_10k[metric]) / results_10k[metric]) * 100 if results_10k[metric] > 0 else 0\n",
    "    print(f\"{metric.upper()}: {improvement:+.2f}%\")\n",
    "\n",
    "# 추론 시간 비율 비교\n",
    "time_ratio_base_10k = results_10k['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_base_30k = results_30k['avg_inference_time'] / results_base['avg_inference_time']\n",
    "time_ratio_10k_30k = results_30k['avg_inference_time'] / results_10k['avg_inference_time']\n",
    "\n",
    "print(f\"\\n=== 추론 시간 비율 비교 ===\")\n",
    "print(f\"추론 시간 비율 (10k/원본): {time_ratio_base_10k:.2f}x\")\n",
    "print(f\"추론 시간 비율 (30k/원본): {time_ratio_base_30k:.2f}x\")\n",
    "print(f\"추론 시간 비율 (30k/10k): {time_ratio_10k_30k:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540b5d9",
   "metadata": {},
   "source": [
    "## 8. 시각화 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metric Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Analysis (Base vs Fine-tuned)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_data = {\n",
    "    'BLEU': [results_base['bleu'], results_10k['bleu'], results_30k['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_10k['rouge1'], results_30k['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_10k['rouge2'], results_30k['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_10k['rougeL'], results_30k['rougeL']],\n",
    "    'Character Accuracy': [results_base['char_accuracy'], results_10k['char_accuracy'], results_30k['char_accuracy']],\n",
    "    'Exact Match': [results_base['exact_match'], results_10k['exact_match'], results_30k['exact_match']]\n",
    "}\n",
    "\n",
    "models = ['Base Model', '10k Model', '30k Model']\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "\n",
    "for idx, (metric, values) in enumerate(metrics_data.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    bars = axes[row, col].bar(models, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{metric}', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Score')\n",
    "    axes[row, col].set_ylim(0, max(values) * 1.1)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Display values\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Character Accuracy Distribution Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 원본 모델\n",
    "axes[0].hist(results_base['char_accuracies'], bins=20, alpha=0.7, color='lightgray', edgecolor='black')\n",
    "axes[0].set_title('Base Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Character Accuracy')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(results_base['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_base[\"char_accuracy\"]:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# 10k 모델\n",
    "axes[1].hist(results_10k['char_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('10k Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Character Accuracy')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(results_10k['char_accuracy'], color='red', linestyle='--', \n",
    "                label=f'Mean: {results_10k[\"char_accuracy\"]:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# 30k 모델\n",
    "axes[2].hist(results_30k['char_accuracies'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title('30k Model - Character Accuracy Distribution', fontweight='bold')\n",
    "axes[2].set_xlabel('Character Accuracy')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(results_30k['char_accuracy'], color='red', linestyle='--',\n",
    "                label=f'Mean: {results_30k[\"char_accuracy\"]:.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inference Time Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Inference time distribution\n",
    "inference_data = [results_base['inference_times'], results_10k['inference_times'], results_30k['inference_times']]\n",
    "labels = ['Base Model', '10k Model', '30k Model']\n",
    "\n",
    "axes[0].boxplot(inference_data, labels=labels, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[0].set_title('Inference Time Distribution Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('Inference Time (seconds)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average inference time bar chart\n",
    "avg_times = [results_base['avg_inference_time'], results_10k['avg_inference_time'], results_30k['avg_inference_time']]\n",
    "colors = ['lightgray', 'skyblue', 'lightcoral']\n",
    "bars = axes[1].bar(labels, avg_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Inference Time Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('Average Inference Time (seconds)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, avg_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c6546",
   "metadata": {},
   "source": [
    "## 9. 질적 분석 - 예시 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 성능 차이가 큰 샘플들 찾기\n",
    "def find_performance_difference_samples(results_base, results_10k, results_30k, n_samples=5):\n",
    "    \"\"\"세 모델 간 성능 차이가 큰 샘플들 찾기\"\"\"\n",
    "    char_acc_base = np.array(results_base['char_accuracies'])\n",
    "    char_acc_10k = np.array(results_10k['char_accuracies'])\n",
    "    char_acc_30k = np.array(results_30k['char_accuracies'])\n",
    "    \n",
    "    # 미세조정 효과 (최고 성능 모델 vs 원본)\n",
    "    max_finetuned = np.maximum(char_acc_10k, char_acc_30k)\n",
    "    finetuning_improvement = max_finetuned - char_acc_base\n",
    "    \n",
    "    # 30k vs 10k 비교\n",
    "    diff_30k_10k = char_acc_30k - char_acc_10k\n",
    "    \n",
    "    # 가장 미세조정 효과가 큰 인덱스들\n",
    "    best_finetuning_idx = np.argsort(finetuning_improvement)[-n_samples:][::-1]\n",
    "    worst_finetuning_idx = np.argsort(finetuning_improvement)[:n_samples]\n",
    "    \n",
    "    # 30k가 10k보다 훨씬 좋은/나쁜 경우\n",
    "    best_30k_vs_10k_idx = np.argsort(diff_30k_10k)[-n_samples:][::-1]\n",
    "    worst_30k_vs_10k_idx = np.argsort(diff_30k_10k)[:n_samples]\n",
    "    \n",
    "    return best_finetuning_idx, worst_finetuning_idx, best_30k_vs_10k_idx, worst_30k_vs_10k_idx\n",
    "\n",
    "best_ft_idx, worst_ft_idx, best_30k_idx, worst_30k_idx = find_performance_difference_samples(results_base, results_10k, results_30k)\n",
    "\n",
    "print(\"=== 미세조정이 원본 모델 대비 가장 효과적이었던 예시 ===\")\n",
    "for i, idx in enumerate(best_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"10k 모델 예측: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k 모델 예측: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 30k 모델이 10k 모델 대비 크게 우수했던 예시 ===\")\n",
    "for i, idx in enumerate(best_30k_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_10k['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_10k['references'][idx]}\")\n",
    "    print(f\"10k 모델 예측: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k 모델 예측: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n=== 미세조정 효과가 제한적이었던 예시 ===\")\n",
    "for i, idx in enumerate(worst_ft_idx):\n",
    "    print(f\"\\n[예시 {i+1}]\")\n",
    "    print(f\"난독화 텍스트: {results_base['test_data'].iloc[idx]['obfuscated']}\")\n",
    "    print(f\"정답 텍스트: {results_base['references'][idx]}\")\n",
    "    print(f\"원본 모델 예측: {results_base['predictions'][idx]}\")\n",
    "    print(f\"10k 모델 예측: {results_10k['predictions'][idx]}\")\n",
    "    print(f\"30k 모델 예측: {results_30k['predictions'][idx]}\")\n",
    "    print(f\"문자 정확도 - 원본: {results_base['char_accuracies'][idx]:.3f}, 10k: {results_10k['char_accuracies'][idx]:.3f}, 30k: {results_30k['char_accuracies'][idx]:.3f}\")\n",
    "    print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91017b0",
   "metadata": {},
   "source": [
    "## 10. 상세 분석 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이별 성능 분석\n",
    "def analyze_by_text_length(results, model_name):\n",
    "    \"\"\"텍스트 길이별 성능 분석\"\"\"\n",
    "    test_data = results['test_data']\n",
    "    char_accuracies = results['char_accuracies']\n",
    "    \n",
    "    # 텍스트 길이 계산\n",
    "    text_lengths = test_data['original'].str.len()\n",
    "    \n",
    "    # 길이 구간별로 분류\n",
    "    length_bins = [0, 20, 50, 100, 200, float('inf')]\n",
    "    length_labels = ['≤20 chars', '21-50 chars', '51-100 chars', '101-200 chars', '200+ chars']\n",
    "    \n",
    "    length_categories = pd.cut(text_lengths, bins=length_bins, labels=length_labels, right=False)\n",
    "    \n",
    "    # 구간별 평균 성능\n",
    "    performance_by_length = []\n",
    "    for category in length_labels:\n",
    "        mask = length_categories == category\n",
    "        if mask.sum() > 0:\n",
    "            avg_acc = np.mean(np.array(char_accuracies)[mask])\n",
    "            count = mask.sum()\n",
    "            performance_by_length.append({\n",
    "                'length_category': category,\n",
    "                'avg_char_accuracy': avg_acc,\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(performance_by_length)\n",
    "\n",
    "# 세 모델의 길이별 성능 분석\n",
    "length_analysis_base = analyze_by_text_length(results_base, \"원본 모델\")\n",
    "length_analysis_10k = analyze_by_text_length(results_10k, \"10k 모델\")\n",
    "length_analysis_30k = analyze_by_text_length(results_30k, \"30k 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(length_analysis_base))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, length_analysis_base['avg_char_accuracy'], width, \n",
    "               label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, length_analysis_10k['avg_char_accuracy'], width,\n",
    "               label='10k Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, length_analysis_30k['avg_char_accuracy'], width,\n",
    "               label='30k Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Text Length Category')\n",
    "ax.set_ylabel('Average Character Accuracy')\n",
    "ax.set_title('Model Performance Comparison by Text Length (Base vs Fine-tuned)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(length_analysis_base['length_category'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 텍스트 길이별 성능 분석 결과 ===\")\n",
    "combined_length_analysis = pd.merge(\n",
    "    pd.merge(length_analysis_base, length_analysis_10k, on='length_category', suffixes=('_base', '_10k')),\n",
    "    length_analysis_30k, on='length_category'\n",
    ")\n",
    "combined_length_analysis.columns = ['length_category', 'avg_char_accuracy_base', 'count_base', \n",
    "                                   'avg_char_accuracy_10k', 'count_10k', 'avg_char_accuracy_30k', 'count_30k']\n",
    "print(combined_length_analysis[['length_category', 'avg_char_accuracy_base', 'avg_char_accuracy_10k', 'avg_char_accuracy_30k']])\n",
    "\n",
    "# 길이별 미세조정 효과 분석\n",
    "print(\"\\n=== 길이별 미세조정 효과 ===\")\n",
    "for _, row in combined_length_analysis.iterrows():\n",
    "    category = row['length_category']\n",
    "    base_acc = row['avg_char_accuracy_base']\n",
    "    acc_10k = row['avg_char_accuracy_10k']\n",
    "    acc_30k = row['avg_char_accuracy_30k']\n",
    "    \n",
    "    improvement_10k = ((acc_10k - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    improvement_30k = ((acc_30k - base_acc) / base_acc * 100) if base_acc > 0 else 0\n",
    "    \n",
    "    print(f\"{category}: 10k 개선 {improvement_10k:+.1f}%, 30k 개선 {improvement_30k:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 일치 및 부분 일치 분석\n",
    "def analyze_match_types(results, model_name):\n",
    "    \"\"\"완전 일치 및 부분 일치 분석\"\"\"\n",
    "    predictions = results['predictions']\n",
    "    references = results['references']\n",
    "    \n",
    "    perfect_matches = 0\n",
    "    high_accuracy = 0  # 90% 이상\n",
    "    medium_accuracy = 0  # 70-90%\n",
    "    low_accuracy = 0  # 70% 미만\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        char_acc = calculate_character_accuracy(pred, ref)\n",
    "        \n",
    "        if pred.strip() == ref.strip():\n",
    "            perfect_matches += 1\n",
    "        elif char_acc >= 0.9:\n",
    "            high_accuracy += 1\n",
    "        elif char_acc >= 0.7:\n",
    "            medium_accuracy += 1\n",
    "        else:\n",
    "            low_accuracy += 1\n",
    "    \n",
    "    total = len(predictions)\n",
    "    \n",
    "    return {\n",
    "        'perfect_match': perfect_matches,\n",
    "        'high_accuracy': high_accuracy,\n",
    "        'medium_accuracy': medium_accuracy,\n",
    "        'low_accuracy': low_accuracy,\n",
    "        'perfect_match_rate': perfect_matches / total,\n",
    "        'high_accuracy_rate': high_accuracy / total,\n",
    "        'medium_accuracy_rate': medium_accuracy / total,\n",
    "        'low_accuracy_rate': low_accuracy / total\n",
    "    }\n",
    "\n",
    "match_analysis_base = analyze_match_types(results_base, \"원본 모델\")\n",
    "match_analysis_10k = analyze_match_types(results_10k, \"10k 모델\")\n",
    "match_analysis_30k = analyze_match_types(results_30k, \"30k 모델\")\n",
    "\n",
    "# 결과 시각화\n",
    "categories = ['Perfect Match', 'High Accuracy\\n(90%+)', 'Medium Accuracy\\n(70-90%)', 'Low Accuracy\\n(<70%)']\n",
    "values_base = [match_analysis_base['perfect_match_rate'], \n",
    "               match_analysis_base['high_accuracy_rate'],\n",
    "               match_analysis_base['medium_accuracy_rate'], \n",
    "               match_analysis_base['low_accuracy_rate']]\n",
    "values_10k = [match_analysis_10k['perfect_match_rate'], \n",
    "              match_analysis_10k['high_accuracy_rate'],\n",
    "              match_analysis_10k['medium_accuracy_rate'], \n",
    "              match_analysis_10k['low_accuracy_rate']]\n",
    "values_30k = [match_analysis_30k['perfect_match_rate'], \n",
    "              match_analysis_30k['high_accuracy_rate'],\n",
    "              match_analysis_30k['medium_accuracy_rate'], \n",
    "              match_analysis_30k['low_accuracy_rate']]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, values_base, width, label='Base Model', color='lightgray', alpha=0.7)\n",
    "bars2 = ax.bar(x, values_10k, width, label='10k Model', color='skyblue', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, values_30k, width, label='30k Model', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Accuracy Category')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Sample Distribution by Accuracy Category (Base vs Fine-tuned)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Display values\n",
    "for bars, values in zip([bars1, bars2, bars3], [values_base, values_10k, values_30k]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{value:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== 정확도 카테고리별 분석 결과 ===\")\n",
    "print(f\"{'Category':<20} {'Base Model':<15} {'10k Model':<15} {'30k Model':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for i, category in enumerate(categories):\n",
    "    print(f\"{category:<20} {values_base[i]:<15.1%} {values_10k[i]:<15.1%} {values_30k[i]:<15.1%}\")\n",
    "\n",
    "print(\"\\n=== 원본 모델 대비 개선율 ===\")\n",
    "print(f\"{'Category':<20} {'10k Improvement':<20} {'30k Improvement':<20}\")\n",
    "print(\"-\" * 65)\n",
    "for i, category in enumerate(categories):\n",
    "    improvement_10k = ((values_10k[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_10k[i] > 0 else 0\n",
    "    improvement_30k = ((values_30k[i] - values_base[i]) / values_base[i] * 100) if values_base[i] > 0 else float('inf') if values_30k[i] > 0 else 0\n",
    "    \n",
    "    if improvement_10k == float('inf'):\n",
    "        imp_10k_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_10k_str = f\"{improvement_10k:+.1f}%\"\n",
    "        \n",
    "    if improvement_30k == float('inf'):\n",
    "        imp_30k_str = \"N/A (0→+)\"\n",
    "    else:\n",
    "        imp_30k_str = f\"{improvement_30k:+.1f}%\"\n",
    "    \n",
    "    print(f\"{category:<20} {imp_10k_str:<20} {imp_30k_str:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5877e",
   "metadata": {},
   "source": [
    "## 11. 종합 결론 및 인사이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dcacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"🔍 HyperCLOVAX 모델 성능 비교 분석 - 종합 결론 (원본 vs 미세조정)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 **원본 모델 대비 미세조정 모델 성능 비교**\n",
    "\n",
    "📈 **BLEU 점수**\n",
    "- 원본: {results_base['bleu']:.4f}\n",
    "- 10k: {results_10k['bleu']:.4f} (개선율: {((results_10k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['bleu']:.4f} (개선율: {((results_30k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100):+.2f}%)\n",
    "\n",
    "📈 **ROUGE-L 점수**\n",
    "- 원본: {results_base['rougeL']:.4f}\n",
    "- 10k: {results_10k['rougeL']:.4f} (개선율: {((results_10k['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['rougeL']:.4f} (개선율: {((results_30k['rougeL'] - results_base['rougeL']) / results_base['rougeL'] * 100):+.2f}%)\n",
    "\n",
    "📈 **문자 정확도**\n",
    "- 원본: {results_base['char_accuracy']:.4f}\n",
    "- 10k: {results_10k['char_accuracy']:.4f} (개선율: {((results_10k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "- 30k: {results_30k['char_accuracy']:.4f} (개선율: {((results_30k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100):+.2f}%)\n",
    "\n",
    "📈 **완전 일치율**\n",
    "- 원본: {results_base['exact_match']:.4f}\n",
    "- 10k: {results_10k['exact_match']:.4f} (개선율: {((results_10k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "- 30k: {results_30k['exact_match']:.4f} (개선율: {((results_30k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'):+.2f}%)\n",
    "\n",
    "⏱️ **효율성 분석**\n",
    "- 원본 모델 평균 추론 시간: {results_base['avg_inference_time']:.3f}초\n",
    "- 10k 모델 평균 추론 시간: {results_10k['avg_inference_time']:.3f}초 ({results_10k['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "- 30k 모델 평균 추론 시간: {results_30k['avg_inference_time']:.3f}초 ({results_30k['avg_inference_time'] / results_base['avg_inference_time']:.2f}x)\n",
    "\n",
    "💯 **복원 품질 분석**\n",
    "- 원본 모델 완전 일치: {match_analysis_base['perfect_match']}개 ({match_analysis_base['perfect_match_rate']:.1%})\n",
    "- 10k 모델 완전 일치: {match_analysis_10k['perfect_match']}개 ({match_analysis_10k['perfect_match_rate']:.1%})\n",
    "- 30k 모델 완전 일치: {match_analysis_30k['perfect_match']}개 ({match_analysis_30k['perfect_match_rate']:.1%})\n",
    "\n",
    "🎯 **핵심 인사이트**\n",
    "1. 미세조정이 원본 모델 대비 모든 지표에서 현저한 성능 향상을 가져옴\n",
    "2. 30k 데이터셋 모델이 10k 데이터셋 모델보다 추가 성능 향상을 보임\n",
    "3. 완전 일치율에서 가장 드라마틱한 개선을 확인 (정확한 복원 능력 향상)\n",
    "4. 추론 시간 증가는 미미하여 효율성 저하 없이 성능 향상 달성\n",
    "5. 다양한 텍스트 길이에서 안정적인 성능 향상 확인\n",
    "\n",
    "💡 **권장 사항**\n",
    "- 난독화 해제 작업에는 30k 데이터셋 모델 사용 강력 권장\n",
    "- 더 많은 데이터로 추가 학습 시 더 큰 성능 향상 기대 가능\n",
    "- 원본 모델의 제한적 성능을 고려할 때 미세조정의 효과가 매우 의미 있음\n",
    "- 도메인 특화 작업에서 미세조정의 중요성 입증\n",
    "\"\"\") \n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd32b3",
   "metadata": {},
   "source": [
    "## 12. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV 파일로 저장\n",
    "results_summary = pd.DataFrame({\n",
    "    '모델': ['원본 모델', '10k 데이터셋 모델', '30k 데이터셋 모델'],\n",
    "    'BLEU 점수': [results_base['bleu'], results_10k['bleu'], results_30k['bleu']],\n",
    "    'ROUGE-1': [results_base['rouge1'], results_10k['rouge1'], results_30k['rouge1']],\n",
    "    'ROUGE-2': [results_base['rouge2'], results_10k['rouge2'], results_30k['rouge2']],\n",
    "    'ROUGE-L': [results_base['rougeL'], results_10k['rougeL'], results_30k['rougeL']],\n",
    "    '문자 정확도': [results_base['char_accuracy'], results_10k['char_accuracy'], results_30k['char_accuracy']],\n",
    "    '정확 일치율': [results_base['exact_match'], results_10k['exact_match'], results_30k['exact_match']],\n",
    "    '평균 추론 시간': [results_base['avg_inference_time'], results_10k['avg_inference_time'], results_30k['avg_inference_time']],\n",
    "    '총 추론 시간': [results_base['total_inference_time'], results_10k['total_inference_time'], results_30k['total_inference_time']]\n",
    "})\n",
    "\n",
    "# 상세 결과도 저장\n",
    "detailed_results = pd.DataFrame({\n",
    "    '인덱스': range(len(results_base['predictions'])),\n",
    "    '원본': results_base['references'],\n",
    "    '난독화': results_base['test_data']['obfuscated'].tolist(),\n",
    "    '예측_원본': results_base['predictions'],\n",
    "    '예측_10k': results_10k['predictions'],\n",
    "    '예측_30k': results_30k['predictions'],\n",
    "    '문자_정확도_원본': results_base['char_accuracies'],\n",
    "    '문자_정확도_10k': results_10k['char_accuracies'],\n",
    "    '문자_정확도_30k': results_30k['char_accuracies'],\n",
    "    '정확_일치_원본': results_base['exact_matches'],\n",
    "    '정확_일치_10k': results_10k['exact_matches'],\n",
    "    '정확_일치_30k': results_30k['exact_matches'],\n",
    "    '추론_시간_원본': results_base['inference_times'],\n",
    "    '추론_시간_10k': results_10k['inference_times'],\n",
    "    '추론_시간_30k': results_30k['inference_times']\n",
    "})\n",
    "\n",
    "# 미세조정 효과 분석 결과 저장\n",
    "finetuning_analysis = pd.DataFrame({\n",
    "    '모델': ['10k vs 원본', '30k vs 원본', '30k vs 10k'],\n",
    "    'BLEU_개선율': [\n",
    "        ((results_10k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_30k['bleu'] - results_base['bleu']) / results_base['bleu'] * 100) if results_base['bleu'] > 0 else 0,\n",
    "        ((results_30k['bleu'] - results_10k['bleu']) / results_10k['bleu'] * 100) if results_10k['bleu'] > 0 else 0\n",
    "    ],\n",
    "    '문자정확도_개선율': [\n",
    "        ((results_10k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_30k['char_accuracy'] - results_base['char_accuracy']) / results_base['char_accuracy'] * 100),\n",
    "        ((results_30k['char_accuracy'] - results_10k['char_accuracy']) / results_10k['char_accuracy'] * 100)\n",
    "    ],\n",
    "    '완전일치율_개선율': [\n",
    "        ((results_10k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_30k['exact_match'] - results_base['exact_match']) / results_base['exact_match'] * 100) if results_base['exact_match'] > 0 else float('inf'),\n",
    "        ((results_30k['exact_match'] - results_10k['exact_match']) / results_10k['exact_match'] * 100) if results_10k['exact_match'] > 0 else float('inf')\n",
    "    ]\n",
    "})\n",
    "\n",
    "# 파일 저장\n",
    "results_summary.to_csv('model_performance_summary_with_base.csv', index=False, encoding='utf-8-sig')\n",
    "detailed_results.to_csv('detailed_model_comparison_with_base.csv', index=False, encoding='utf-8-sig')\n",
    "finetuning_analysis.to_csv('finetuning_effect_analysis.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"📁 결과 파일 저장 완료:\")\n",
    "print(\"- model_performance_summary_with_base.csv: 모델별 성능 요약 (원본 모델 포함)\")\n",
    "print(\"- detailed_model_comparison_with_base.csv: 상세 비교 결과 (원본 모델 포함)\")\n",
    "print(\"- finetuning_effect_analysis.csv: 미세조정 효과 분석\")\n",
    "\n",
    "# Google Colab에서 다운로드 시도\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('model_performance_summary_with_base.csv')\n",
    "    files.download('detailed_model_comparison_with_base.csv')\n",
    "    files.download('finetuning_effect_analysis.csv')\n",
    "    print(\"📥 파일 다운로드 완료\")\n",
    "except ImportError:\n",
    "    print(\"💾 로컬 환경에서는 현재 디렉토리에 파일이 저장되었습니다.\")\n",
    "\n",
    "# 결과 요약 출력\n",
    "print(\"\\n=== 최종 성능 요약 ===\")\n",
    "print(results_summary.round(4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
