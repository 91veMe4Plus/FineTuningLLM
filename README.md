# π€ HyperCLOVAX Korean Text De-obfuscation Fine-tuning Project

μ΄ ν”„λ΅μ νΈλ” **Naver HyperCLOVAX-SEED-Text-Instruct-0.5B** λ¨λΈμ„ ν•κµ­μ–΄ ν…μ¤νΈ λΉ„λ‚λ…ν™”(De-obfuscation) μ‘μ—…μ„ μ„ν•΄ fine-tuningν•λ” μ—°κµ¬ ν”„λ΅μ νΈμ…λ‹λ‹¤.

## π“‹ ν”„λ΅μ νΈ κ°μ”

### λ©ν‘
- λ‚λ…ν™”λ ν•κµ­μ–΄ ν…μ¤νΈλ¥Ό μ›λ³Έ ν…μ¤νΈλ΅ λ³µμ›ν•λ” λ¨λΈ κ°λ°
- **LoRA (Low-Rank Adaptation)**λ¥Ό μ‚¬μ©ν• ν¨μ¨μ μΈ fine-tuning
- λ‹¤μ–‘ν• ν…μ¤νΈ μ ν•μ— λ€ν• μ„±λ¥ ν‰κ°€ λ° λΉ„κµ
- λ°μ΄ν„°μ…‹ ν¬κΈ°μ— λ”°λ¥Έ λ¨λΈ μ„±λ¥ λ¶„μ„

### μ£Όμ” νΉμ§•
- π― **Target Model**: HyperCLOVAX-SEED-Text-Instruct-0.5B
- π”§ **Training Method**: LoRA Fine-tuning
- π“ **Dataset Sizes**: 10K, 30K μƒν”
- π“ **Performance Metrics**: BLEU, ROUGE, Character Accuracy
- π—‚οΈ **Text Categories**: κµ¬μ–΄μ²΄, λ‰΄μ¤, λ¬Έν™”, μ „λ¬Έλ¶„μ•Ό, μ΅°λ΅€, μ§€μμ²΄μ›Ήμ‚¬μ΄νΈ

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
FineTuningLLM/
β”β”€β”€ π““ hyperclova_deobfuscation_finetuning.ipynb    # λ©”μΈ fine-tuning λ…ΈνΈλ¶
β”β”€β”€ π“ model_performance_analysis.ipynb             # λ¨λΈ μ„±λ¥ λΉ„κµ λ¶„μ„
β”β”€β”€ π“„ testdata.csv                                 # ν…μ¤νΈ λ°μ΄ν„° (1,002 μƒν”)
β”β”€β”€ π“ κµ¬μ–΄μ²΄_λ€ν™”μ²΄_16878_sample_λ‚λ…ν™”κ²°κ³Ό.csv          # κµ¬μ–΄μ²΄ ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π“ λ‰΄μ¤λ¬Έμ–΄μ²΄_281932_sample_λ‚λ…ν™”κ²°κ³Ό.csv           # λ‰΄μ¤ ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π“ λ¬Έν™”λ¬Έμ–΄μ²΄_25628_sample_λ‚λ…ν™”κ²°κ³Ό.csv            # λ¬Έν™” ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π“ μ „λ¬Έλ¶„μ•Ό λ¬Έμ–΄μ²΄_306542_sample_λ‚λ…ν™”κ²°κ³Ό.csv        # μ „λ¬Έλ¶„μ•Ό ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π“ μ΅°λ΅€λ¬Έμ–΄μ²΄_36339_sample_λ‚λ…ν™”κ²°κ³Ό.csv            # μ΅°λ΅€ ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π“ μ§€μμ²΄μ›Ήμ‚¬μ΄νΈ λ¬Έμ–΄μ²΄_28705_sample_λ‚λ…ν™”κ²°κ³Ό.csv    # μ§€μμ²΄μ›Ήμ‚¬μ΄νΈ ν›λ ¨ λ°μ΄ν„°
β”β”€β”€ π¤– hyperclova-deobfuscation-lora-with-10k-datasets/  # 10K λ¨λΈ
β””β”€β”€ π¤– hyperclova-deobfuscation-lora-with-30k-datasets/  # 30K λ¨λΈ
```

## π› οΈ ν™κ²½ μ„¤μ •

### μ‹μ¤ν… μ”κµ¬μ‚¬ν•­
- **GPU**: NVIDIA GPU (8GB VRAM μ΄μƒ κ¶μ¥)
- **Python**: 3.8+
- **CUDA**: 11.8+
- **Environment**: Google Colab λλ” λ΅μ»¬ ν™κ²½

### ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ

```bash
pip install transformers>=4.35.0
pip install peft>=0.6.0
pip install trl>=0.7.0
pip install datasets>=2.14.0
pip install bitsandbytes>=0.41.0
pip install accelerate>=0.24.0
pip install evaluate>=0.4.0
pip install rouge-score>=0.1.2
pip install sentencepiece>=0.1.99
pip install scikit-learn>=1.3.0
pip install matplotlib seaborn plotly
```

## π“ λ°μ΄ν„°μ…‹

### ν›λ ¨ λ°μ΄ν„°
μ΄ 6κ° μΉ΄ν…κ³ λ¦¬μ ν•κµ­μ–΄ ν…μ¤νΈ λ°μ΄ν„°:

| μΉ΄ν…κ³ λ¦¬ | μƒν” μ | μ„¤λ… |
|---------|---------|------|
| κµ¬μ–΄μ²΄ λ€ν™”μ²΄ | 16,878 | μΌμƒ λ€ν™” λ° κµ¬μ–΄μ²΄ ν…μ¤νΈ |
| λ‰΄μ¤ λ¬Έμ–΄μ²΄ | 281,932 | λ‰΄μ¤ κΈ°μ‚¬ λ° λ³΄λ„ μλ£ |
| λ¬Έν™” λ¬Έμ–΄μ²΄ | 25,628 | λ¬Έν™” κ΄€λ ¨ ν…μ¤νΈ |
| μ „λ¬Έλ¶„μ•Ό λ¬Έμ–΄μ²΄ | 306,542 | μ „λ¬Έ λ¶„μ•Ό λ¬Έμ„ |
| μ΅°λ΅€ λ¬Έμ–΄μ²΄ | 36,339 | λ²•λ¥  λ° μ΅°λ΅€ λ¬Έμ„ |
| μ§€μμ²΄μ›Ήμ‚¬μ΄νΈ λ¬Έμ–΄μ²΄ | 28,705 | κ³µκ³µκΈ°κ΄€ μ›Ήμ‚¬μ΄νΈ ν…μ¤νΈ |

### ν…μ¤νΈ λ°μ΄ν„°
- **νμΌ**: `testdata.csv`
- **μƒν” μ**: 1,002κ°
- **κµ¬μ΅°**: `index`, `original`, `obfuscated` μ»¬λΌ

## π€ μ‚¬μ©λ²•

### 1. Fine-tuning μ‹¤ν–‰

```python
# hyperclova_deobfuscation_finetuning.ipynb μ‹¤ν–‰
# - λ°μ΄ν„° λ΅λ”© λ° μ „μ²λ¦¬
# - LoRA μ„¤μ • λ° λ¨λΈ ν›λ ¨
# - μ²΄ν¬ν¬μΈνΈ μ €μ¥
```

### 2. λ¨λΈ μ„±λ¥ λ¶„μ„

```python
# model_performance_analysis.ipynb μ‹¤ν–‰
# - μ›λ³Έ λ¨λΈ vs Fine-tuned λ¨λΈ λΉ„κµ
# - μ •λ‰μ /μ •μ„±μ  μ„±λ¥ ν‰κ°€
# - κ²°κ³Ό μ‹κ°ν™”
```

### 3. μ¶”λ΅  μμ‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# λ² μ΄μ¤ λ¨λΈ λ΅λ“
model = AutoModelForCausalLM.from_pretrained("naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B")
tokenizer = AutoTokenizer.from_pretrained("naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B")

# LoRA μ–΄λ‘ν„° λ΅λ“
model = PeftModel.from_pretrained(model, "./hyperclova-deobfuscation-lora-with-30k-datasets")

# μ¶”λ΅ 
obfuscated_text = "λ³„ ν• κ²ν†  μ•κΉλ•€. μ™ μ‹Έλλ“―λ¦­ νΌ 1μΊλ¥Ό μ¥°λμ§•..."
prompt = f"λ‹¤μ λ‚λ…ν™”λ ν…μ¤νΈλ¥Ό μ›λλ€λ΅ λ³µμ›ν•΄μ£Όμ„Έμ”.\n\nλ‚λ…ν™”λ ν…μ¤νΈ: {obfuscated_text}\n\nλ³µμ›λ ν…μ¤νΈ:"

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=512, temperature=0.7)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## π“ μ„±λ¥ κ²°κ³Ό

### μ •λ‰μ  ν‰κ°€ μ§€ν‘

| λ¨λΈ | BLEU Score | ROUGE-L | Character Accuracy |
|------|------------|---------|-------------------|
| μ›λ³Έ λ¨λΈ | - | - | - |
| 10K Fine-tuned | - | - | - |
| 30K Fine-tuned | - | - | - |

### μ£Όμ” λ°κ²¬μ‚¬ν•­
- **λ°μ΄ν„°μ…‹ ν¬κΈ°**: 30K > 10K λ°μ΄ν„°μ…‹μΌλ΅ ν›λ ¨λ λ¨λΈμ΄ λ” μ°μν• μ„±λ¥
- **μΉ΄ν…κ³ λ¦¬λ³„ μ„±λ¥**: λ‰΄μ¤ λ¬Έμ–΄μ²΄μ™€ μ „λ¬Έλ¶„μ•Ό λ¬Έμ–΄μ²΄μ—μ„ λ†’μ€ μ„±λ¥
- **ν›λ ¨ ν¨μ¨μ„±**: LoRA λ°©λ²•μΌλ΅ ν¨μ¨μ μΈ fine-tuning λ‹¬μ„±

## π”§ λ¨λΈ μ•„ν‚¤ν…μ²

### LoRA μ„¤μ •
```python
lora_config = LoraConfig(
    r=64,                    # rank
    lora_alpha=16,          # scaling parameter
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
```

### ν›λ ¨ νλΌλ―Έν„°
- **Learning Rate**: 2e-4
- **Batch Size**: 4 (gradient accumulation steps: 4)
- **Max Length**: 512 tokens
- **Epochs**: 3
- **Optimizer**: AdamW
- **Scheduler**: Linear with warmup

## π“ μ‹¤ν— κ²°κ³Ό

### ν›λ ¨ μ§„ν–‰μƒν™©
- **10K λ¨λΈ**: 1,686 steps (μµμΆ… μ²΄ν¬ν¬μΈνΈ)
- **30K λ¨λΈ**: 5,061 steps (μµμΆ… μ²΄ν¬ν¬μΈνΈ)
- **μ¤‘κ°„ μ²΄ν¬ν¬μΈνΈ**: κ° λ¨λΈλ³„ λ‹¤μ μ €μ¥

### μ„±λ¥ λΉ„κµ
μƒμ„Έν• μ„±λ¥ λ¶„μ„ κ²°κ³Όλ” `model_performance_analysis.ipynb`μ—μ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

## π”® ν–¥ν›„ κ³„ν

- [ ] λ” ν° λ°μ΄ν„°μ…‹ (50K, 100K)μΌλ΅ ν™•μ¥ μ‹¤ν—
- [ ] λ‹¤λ¥Έ LoRA μ„¤μ • (rank, alpha) μµμ ν™”
- [ ] λ‹¤μ–‘ν• λ² μ΄μ¤ λ¨λΈ λΉ„κµ μ‹¤ν—
- [ ] μ‹¤μ‹κ°„ μ¶”λ΅  API κ°λ°
- [ ] μ›Ή μΈν„°νμ΄μ¤ κµ¬μ¶•

## π¤ κΈ°μ—¬ν•κΈ°

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## π“ λΌμ΄μ„Όμ¤

μ΄ ν”„λ΅μ νΈλ” [LICENSE](LICENSE) νμΌμ— λ…μ‹λ λΌμ΄μ„Όμ¤λ¥Ό λ”°λ¦…λ‹λ‹¤.

## π™ κ°μ‚¬μ λ§

- **Naver HyperCLOVAX** ν€μ μ°μν• λ² μ΄μ¤ λ¨λΈ μ κ³µ
- **Hugging Face** μ»¤λ®¤λ‹ν‹°μ Transformers λ° PEFT λΌμ΄λΈλ¬λ¦¬
- **Google Colab**μ λ¬΄λ£ GPU ν™κ²½ μ κ³µ

## π“ μ—°λ½μ²

ν”„λ΅μ νΈ κ΄€λ ¨ λ¬Έμμ‚¬ν•­μ΄ μμΌμ‹λ©΄ μ΄μλ¥Ό λ“±λ΅ν•΄μ£Όμ„Έμ”.

---

*μ΄ ν”„λ΅μ νΈλ” ν•κµ­μ–΄ ν…μ¤νΈ λΉ„λ‚λ…ν™” μ—°κµ¬λ¥Ό μ„ν• κµμ΅ λ° μ—°κµ¬ λ©μ μΌλ΅ κ°λ°λμ—μµλ‹λ‹¤.*
