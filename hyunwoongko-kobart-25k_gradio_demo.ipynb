{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08793e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio>=4.0.0 transformers>=4.35.0 torch sentencepiece accelerate protobuf>=3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb48dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # GPT2 기반 모델이므로 CausalLM 사용\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 모델 설정 ---\n",
    "# 팀에서 파인튜닝하여 Hugging Face에 업로드한 모델 사용\n",
    "MODEL_NAME = \"91veMe4Plus-Project/hyunwoongko-kobart-25k\" # 팀에서 파인튜닝한 모델\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "device = None\n",
    "\n",
    "print(\"환경 설정 및 모델 로딩을 시작합니다...\")\n",
    "\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "    print(f\"토크나이저 ({MODEL_NAME}) 로딩 중...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # pad_token 설정 (GPT2 기반 모델은 기본적으로 pad_token이 없음)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"모델 ({MODEL_NAME}) 로딩 중...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME) # CausalLM으로 변경\n",
    "    model.to(device) # 모델을 해당 디바이스로 이동\n",
    "    model.eval() # 추론 모드로 설정\n",
    "    \n",
    "    print(f\"모델 및 토크나이저 로딩 완료. Device: {device}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 로딩 중 심각한 오류 발생: {e}\")\n",
    "    print(\"Gradio 앱이 정상적으로 작동하지 않을 수 있습니다. 모델 이름 및 인터넷 연결을 확인해주세요.\")\n",
    "    # 오류 발생 시 model이나 tokenizer가 None으로 유지되어 아래 함수에서 처리됩니다.\n",
    "\n",
    "def deobfuscate_interface(obfuscated_text):\n",
    "    \"\"\"\n",
    "    팀에서 파인튜닝한 hyunwoongko-kobart-25k 모델을 사용하여 텍스트를 비난독화합니다.\n",
    "    \"\"\"\n",
    "    if not model or not tokenizer:\n",
    "        return \"오류: 모델 또는 토크나이저가 로드되지 않았습니다. Colab 셀 실행 로그를 확인해주세요.\"\n",
    "\n",
    "    if not obfuscated_text.strip():\n",
    "        return \"난독화된 텍스트를 입력해주세요.\"\n",
    "\n",
    "    # GPT2 기반 Causal LM을 위한 프롬프트 형식\n",
    "    # 파인튜닝 방식에 따라 프롬프트를 조정할 수 있습니다.\n",
    "    prompt = f\"난독화된 텍스트: {obfuscated_text}\\n복원된 텍스트:\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Causal LM을 위한 생성 파라미터\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_new_tokens=128, # 새로 생성할 토큰 수\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # 생성된 전체 텍스트에서 입력 프롬프트 부분 제거\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 프롬프트 부분을 제거하여 생성된 부분만 추출\n",
    "        if \"복원된 텍스트:\" in generated_text:\n",
    "            response = generated_text.split(\"복원된 텍스트:\")[1].strip()\n",
    "        else:\n",
    "            # 프롬프트 길이만큼 제거\n",
    "            prompt_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "            response = generated_text[prompt_length:].strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"텍스트 생성 중 오류 발생: {e}\")\n",
    "        return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "iface = gr.Interface(\n",
    "    fn=deobfuscate_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"난독화된 한국어 텍스트\",\n",
    "        placeholder=\"난독화된 텍스트를 입력하세요...\",\n",
    "        lines=5\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"복원된 원본 텍스트\",\n",
    "        lines=5\n",
    "    ),\n",
    "    title=\"hyunwoongko-kobart-25k 한국어 텍스트 비난독화\", # 모델 이름으로 제목 수정\n",
    "    description=\"팀에서 파인튜닝한 `hyunwoongko-kobart-25k` 모델을 사용하여 난독화된 한국어 텍스트를 원본 텍스트로 복원합니다.\", # 모델 이름으로 설명 수정\n",
    "    examples=[ # 예제는 동일한 작업이므로 유지\n",
    "        [\"안녀하쎼요, 반갑쏘니댜!\"],\n",
    "        [\"오늬 날씨갸 맆이 좆네욘.\"],\n",
    "        [\"한큿어 쳬연어 처륄예 댕햔 연귝을 해보갰습닏댜.\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Gradio 인터페이스를 시작합니다...\")\n",
    "    # share=True로 설정하여 외부 접속 링크 생성\n",
    "    iface.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
