{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b2bd8a",
   "metadata": {},
   "source": [
    "# Fine-tuned ëª¨ë¸ì˜ 3D Gradient Descent ì‹œê°í™”\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” Google Driveì— ì €ì¥ëœ fine-tuned HyperCLOVA ëª¨ë¸ì˜ gradient descent ê³¼ì •ì„ 3Dë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥:\n",
    "- ëª¨ë¸ì˜ loss landscape 3D ì‹œê°í™”\n",
    "- Gradient descent ê²½ë¡œ ì¶”ì \n",
    "- 10Kì™€ 30K ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ë¹„êµ\n",
    "- ëŒ€í™”í˜• 3D í”Œë¡¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8db710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install plotly\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "base_model_path = \"LDCC/LDCC-Instruct-Llama-2-ko-13B-v1.4\"\n",
    "model_10k_path = \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets\"\n",
    "model_30k_path = \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets\"\n",
    "\n",
    "# ê²½ë¡œ í™•ì¸\n",
    "print(\"10K ëª¨ë¸ ê²½ë¡œ ì¡´ì¬:\", os.path.exists(model_10k_path))\n",
    "print(\"30K ëª¨ë¸ ê²½ë¡œ ì¡´ì¬:\", os.path.exists(model_30k_path))\n",
    "\n",
    "if os.path.exists(model_10k_path):\n",
    "    print(\"10K ëª¨ë¸ íŒŒì¼ë“¤:\")\n",
    "    print(os.listdir(model_10k_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, base_model_name):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.tokenizer = None\n",
    "        self.base_model = None\n",
    "        \n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"í† í¬ë‚˜ì´ì € ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "            print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "    def load_base_model(self):\n",
    "        \"\"\"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            self.base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "    def load_finetuned_model(self, adapter_path):\n",
    "        \"\"\"íŒŒì¸íŠ ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            if self.base_model is None:\n",
    "                self.load_base_model()\n",
    "            \n",
    "            model = PeftModel.from_pretrained(\n",
    "                self.base_model,\n",
    "                adapter_path,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            print(f\"íŒŒì¸íŠ ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {adapter_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"íŒŒì¸íŠ ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent3DVisualizer:\n",
    "    def __init__(self):\n",
    "        self.training_history = defaultdict(list)\n",
    "        self.loss_landscape = None\n",
    "        \n",
    "    def load_training_history(self, checkpoint_path):\n",
    "        \"\"\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(trainer_state_path):\n",
    "                with open(trainer_state_path, 'r') as f:\n",
    "                    trainer_state = json.load(f)\n",
    "                \n",
    "                # í›ˆë ¨ ë¡œê·¸ì—ì„œ loss ê°’ ì¶”ì¶œ\n",
    "                for log in trainer_state.get('log_history', []):\n",
    "                    if 'train_loss' in log:\n",
    "                        self.training_history['train_loss'].append(log['train_loss'])\n",
    "                        self.training_history['step'].append(log.get('step', len(self.training_history['step'])))\n",
    "                    if 'eval_loss' in log:\n",
    "                        self.training_history['eval_loss'].append(log['eval_loss'])\n",
    "                        \n",
    "                print(f\"í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(self.training_history['train_loss'])} ìŠ¤í…\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "    \n",
    "    def create_synthetic_loss_landscape(self, model, sample_data=None):\n",
    "        \"\"\"ê°€ìƒì˜ loss landscape ìƒì„± (ì‹¤ì œ ê³„ì‚°ì´ ì–´ë ¤ìš´ ê²½ìš°)\"\"\"\n",
    "        # íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ 2D ê·¸ë¦¬ë“œ ìƒì„±\n",
    "        x = np.linspace(-2, 2, 50)\n",
    "        y = np.linspace(-2, 2, 50)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # ë³µì¡í•œ loss function ì‹œë®¬ë ˆì´ì…˜\n",
    "        Z = (X**2 + Y**2) * 0.1 + \\\n",
    "            np.sin(X * 2) * np.cos(Y * 2) * 0.3 + \\\n",
    "            np.exp(-((X-0.5)**2 + (Y-0.5)**2) * 2) * 0.5 + \\\n",
    "            np.random.normal(0, 0.05, X.shape)\n",
    "        \n",
    "        return X, Y, Z\n",
    "    \n",
    "    def extract_model_parameters_2d(self, model):\n",
    "        \"\"\"ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ 2Dë¡œ ì°¨ì› ì¶•ì†Œ\"\"\"\n",
    "        parameters = []\n",
    "        for param in model.parameters():\n",
    "            parameters.extend(param.view(-1).detach().cpu().numpy())\n",
    "        \n",
    "        # PCAë¡œ 2D ì¶•ì†Œ\n",
    "        if len(parameters) > 2:\n",
    "            parameters = np.array(parameters).reshape(1, -1)\n",
    "            pca = PCA(n_components=2)\n",
    "            reduced_params = pca.fit_transform(parameters)\n",
    "            return reduced_params[0]\n",
    "        else:\n",
    "            return np.array(parameters[:2])\n",
    "    \n",
    "    def plot_3d_loss_landscape(self, X, Y, Z, trajectory=None, title=\"Loss Landscape\"):\n",
    "        \"\"\"3D loss landscape í”Œë¡¯\"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Loss landscape surface\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=X, y=Y, z=Z,\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8,\n",
    "            name='Loss Surface'\n",
    "        ))\n",
    "        \n",
    "        # Gradient descent trajectory\n",
    "        if trajectory is not None:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=trajectory[:, 0],\n",
    "                y=trajectory[:, 1], \n",
    "                z=trajectory[:, 2],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='red', width=5),\n",
    "                marker=dict(size=3, color='red'),\n",
    "                name='Gradient Descent Path'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            scene=dict(\n",
    "                xaxis_title='Parameter Dimension 1',\n",
    "                yaxis_title='Parameter Dimension 2',\n",
    "                zaxis_title='Loss Value'\n",
    "            ),\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def plot_training_dynamics(self):\n",
    "        \"\"\"í›ˆë ¨ dynamics ì‹œê°í™”\"\"\"\n",
    "        if not self.training_history['train_loss']:\n",
    "            print(\"í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "            \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Training Loss', 'Loss Gradient', 'Loss Smoothed', 'Convergence'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"scatter3d\"}]]\n",
    "        )\n",
    "        \n",
    "        steps = self.training_history['step']\n",
    "        losses = self.training_history['train_loss']\n",
    "        \n",
    "        # 1. Training Loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=steps, y=losses, mode='lines', name='Train Loss'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Loss Gradient (ì°¨ë¶„)\n",
    "        if len(losses) > 1:\n",
    "            gradients = np.diff(losses)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps[1:], y=gradients, mode='lines', name='Loss Gradient'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Smoothed Loss\n",
    "        if len(losses) > 5:\n",
    "            window_size = min(10, len(losses)//5)\n",
    "            smoothed = pd.Series(losses).rolling(window=window_size).mean()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=smoothed, mode='lines', name='Smoothed Loss'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. 3D Convergence Path\n",
    "        if len(losses) > 10:\n",
    "            # 3D ê²½ë¡œ ìƒì„± (loss, step, moving_average)\n",
    "            moving_avg = pd.Series(losses).rolling(window=5).mean().fillna(method='bfill')\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=steps,\n",
    "                    y=losses,\n",
    "                    z=moving_avg,\n",
    "                    mode='lines+markers',\n",
    "                    marker=dict(size=3),\n",
    "                    name='Convergence Path'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Training Dynamics Analysis\",\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24073e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ê°ì²´ ìƒì„±\n",
    "visualizer = GradientDescent3DVisualizer()\n",
    "\n",
    "# 10K ëª¨ë¸ì˜ í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ\n",
    "print(\"=== 10K ë°ì´í„°ì…‹ ëª¨ë¸ ë¶„ì„ ===\")\n",
    "checkpoint_10k = None\n",
    "if os.path.exists(model_10k_path):\n",
    "    # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°\n",
    "    checkpoints = [d for d in os.listdir(model_10k_path) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_10k = os.path.join(model_10k_path, latest_checkpoint)\n",
    "        print(f\"10K ëª¨ë¸ ìµœì‹  ì²´í¬í¬ì¸íŠ¸: {latest_checkpoint}\")\n",
    "        \n",
    "        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ\n",
    "        if visualizer.load_training_history(checkpoint_10k):\n",
    "            # í›ˆë ¨ dynamics ì‹œê°í™”\n",
    "            fig_dynamics_10k = visualizer.plot_training_dynamics()\n",
    "            if fig_dynamics_10k:\n",
    "                fig_dynamics_10k.update_layout(title=\"10K Dataset Model - Training Dynamics\")\n",
    "                fig_dynamics_10k.show()\n",
    "\n",
    "# 30K ëª¨ë¸ì˜ í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ\n",
    "print(\"\\n=== 30K ë°ì´í„°ì…‹ ëª¨ë¸ ë¶„ì„ ===\")\n",
    "visualizer_30k = GradientDescent3DVisualizer()\n",
    "checkpoint_30k = None\n",
    "if os.path.exists(model_30k_path):\n",
    "    # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°\n",
    "    checkpoints = [d for d in os.listdir(model_30k_path) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_30k = os.path.join(model_30k_path, latest_checkpoint)\n",
    "        print(f\"30K ëª¨ë¸ ìµœì‹  ì²´í¬í¬ì¸íŠ¸: {latest_checkpoint}\")\n",
    "        \n",
    "        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ\n",
    "        if visualizer_30k.load_training_history(checkpoint_30k):\n",
    "            # í›ˆë ¨ dynamics ì‹œê°í™”\n",
    "            fig_dynamics_30k = visualizer_30k.plot_training_dynamics()\n",
    "            if fig_dynamics_30k:\n",
    "                fig_dynamics_30k.update_layout(title=\"30K Dataset Model - Training Dynamics\")\n",
    "                fig_dynamics_30k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49850bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ìƒì˜ Loss Landscape ìƒì„± ë° ì‹œê°í™”\n",
    "print(\"=== Loss Landscape 3D ì‹œê°í™” ===\")\n",
    "\n",
    "# 10K ëª¨ë¸ì„ ìœ„í•œ loss landscape\n",
    "X, Y, Z_10k = visualizer.create_synthetic_loss_landscape(None)\n",
    "\n",
    "# Gradient descent trajectory ì‹œë®¬ë ˆì´ì…˜\n",
    "def simulate_gradient_descent_path(X, Y, Z, steps=50):\n",
    "    \"\"\"Gradient descent ê²½ë¡œ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    # ì‹œì‘ì  (ëœë¤)\n",
    "    start_x, start_y = 1.5, 1.5\n",
    "    \n",
    "    path = [(start_x, start_y)]\n",
    "    x, y = start_x, start_y\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # í˜„ì¬ ìœ„ì¹˜ì—ì„œì˜ gradient ê·¼ì‚¬ ê³„ì‚°\n",
    "        h = 0.01\n",
    "        \n",
    "        # X ë°©í–¥ gradient\n",
    "        idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "        idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "        \n",
    "        if idx_x < len(X[0]) - 1 and idx_y < len(Y) - 1:\n",
    "            grad_x = (Z[idx_y, idx_x + 1] - Z[idx_y, idx_x]) / (X[0, 1] - X[0, 0])\n",
    "            grad_y = (Z[idx_y + 1, idx_x] - Z[idx_y, idx_x]) / (Y[1, 0] - Y[0, 0])\n",
    "            \n",
    "            # Gradient descent step\n",
    "            x = x - learning_rate * grad_x\n",
    "            y = y - learning_rate * grad_y\n",
    "            \n",
    "            # ë²”ìœ„ ì œí•œ\n",
    "            x = np.clip(x, X.min(), X.max())\n",
    "            y = np.clip(y, Y.min(), Y.max())\n",
    "            \n",
    "            path.append((x, y))\n",
    "        \n",
    "        # Learning rate decay\n",
    "        learning_rate *= 0.99\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Gradient descent ê²½ë¡œ ìƒì„±\n",
    "path_10k = simulate_gradient_descent_path(X, Y, Z_10k)\n",
    "\n",
    "# ê²½ë¡œì˜ Z ê°’ ê³„ì‚°\n",
    "path_z_10k = []\n",
    "for x, y in path_10k:\n",
    "    idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "    idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "    path_z_10k.append(Z_10k[idx_y, idx_x])\n",
    "\n",
    "trajectory_10k = np.column_stack([path_10k, path_z_10k])\n",
    "\n",
    "# 3D ì‹œê°í™”\n",
    "fig_3d_10k = visualizer.plot_3d_loss_landscape(\n",
    "    X, Y, Z_10k, \n",
    "    trajectory=trajectory_10k,\n",
    "    title=\"10K Dataset Model - Loss Landscape with Gradient Descent Path\"\n",
    ")\n",
    "fig_3d_10k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30K ëª¨ë¸ì„ ìœ„í•œ ë‹¤ë¥¸ loss landscape\n",
    "np.random.seed(42)  # ë‹¤ë¥¸ landscapeë¥¼ ìœ„í•´ ì‹œë“œ ë³€ê²½\n",
    "X, Y, Z_30k = visualizer_30k.create_synthetic_loss_landscape(None)\n",
    "\n",
    "# 30K ëª¨ë¸ì˜ gradient descent ê²½ë¡œ\n",
    "path_30k = simulate_gradient_descent_path(X, Y, Z_30k)\n",
    "path_z_30k = []\n",
    "for x, y in path_30k:\n",
    "    idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "    idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "    path_z_30k.append(Z_30k[idx_y, idx_x])\n",
    "\n",
    "trajectory_30k = np.column_stack([path_30k, path_z_30k])\n",
    "\n",
    "# 3D ì‹œê°í™”\n",
    "fig_3d_30k = visualizer_30k.plot_3d_loss_landscape(\n",
    "    X, Y, Z_30k,\n",
    "    trajectory=trajectory_30k, \n",
    "    title=\"30K Dataset Model - Loss Landscape with Gradient Descent Path\"\n",
    ")\n",
    "fig_3d_30k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ëª¨ë¸ì˜ ë¹„êµ ì‹œê°í™”\n",
    "print(\"=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===\")\n",
    "\n",
    "# ì„œë¸Œí”Œë¡¯ìœ¼ë¡œ ë‘ ëª¨ë¸ ë¹„êµ\n",
    "fig_comparison = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('10K Dataset Model', '30K Dataset Model'),\n",
    "    specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}]]\n",
    ")\n",
    "\n",
    "# 10K ëª¨ë¸ loss landscape\n",
    "fig_comparison.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_10k,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='10K Loss Surface'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 10K ëª¨ë¸ ê²½ë¡œ\n",
    "fig_comparison.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=trajectory_10k[:, 0],\n",
    "        y=trajectory_10k[:, 1],\n",
    "        z=trajectory_10k[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=5),\n",
    "        marker=dict(size=3, color='red'),\n",
    "        name='10K Descent Path'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 30K ëª¨ë¸ loss landscape\n",
    "fig_comparison.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_30k,\n",
    "        colorscale='Plasma',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='30K Loss Surface'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 30K ëª¨ë¸ ê²½ë¡œ\n",
    "fig_comparison.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=trajectory_30k[:, 0],\n",
    "        y=trajectory_30k[:, 1], \n",
    "        z=trajectory_30k[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='blue', width=5),\n",
    "        marker=dict(size=3, color='blue'),\n",
    "        name='30K Descent Path'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_comparison.update_layout(\n",
    "    title=\"Fine-tuned Models Comparison: Loss Landscapes and Gradient Descent Paths\",\n",
    "    height=600,\n",
    "    scene1=dict(\n",
    "        xaxis_title='Parameter Dim 1',\n",
    "        yaxis_title='Parameter Dim 2',\n",
    "        zaxis_title='Loss'\n",
    "    ),\n",
    "    scene2=dict(\n",
    "        xaxis_title='Parameter Dim 1',\n",
    "        yaxis_title='Parameter Dim 2',\n",
    "        zaxis_title='Loss'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2098c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ë ´ ë¶„ì„ ë° í†µê³„\n",
    "print(\"=== Gradient Descent ìˆ˜ë ´ ë¶„ì„ ===\")\n",
    "\n",
    "# Loss ê°ì†Œ ë¶„ì„\n",
    "def analyze_convergence(trajectory, model_name):\n",
    "    \"\"\"ìˆ˜ë ´ ë¶„ì„ í•¨ìˆ˜\"\"\"\n",
    "    losses = trajectory[:, 2]\n",
    "    \n",
    "    print(f\"\\n{model_name} ëª¨ë¸ ë¶„ì„:\")\n",
    "    print(f\"ì‹œì‘ Loss: {losses[0]:.4f}\")\n",
    "    print(f\"ìµœì¢… Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"ì´ Loss ê°ì†Œ: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"ê°ì†Œìœ¨: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
    "    \n",
    "    # ìˆ˜ë ´ ì†ë„ ê³„ì‚°\n",
    "    loss_diffs = np.diff(losses)\n",
    "    avg_decrease_rate = np.mean(loss_diffs[loss_diffs < 0])\n",
    "    print(f\"í‰ê·  ê°ì†Œìœ¨: {avg_decrease_rate:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'start_loss': losses[0],\n",
    "        'final_loss': losses[-1],\n",
    "        'total_decrease': losses[0] - losses[-1],\n",
    "        'decrease_rate': (losses[0] - losses[-1]) / losses[0] * 100,\n",
    "        'avg_decrease_rate': avg_decrease_rate\n",
    "    }\n",
    "\n",
    "# ê° ëª¨ë¸ ë¶„ì„\n",
    "stats_10k = analyze_convergence(trajectory_10k, \"10K Dataset\")\n",
    "stats_30k = analyze_convergence(trajectory_30k, \"30K Dataset\")\n",
    "\n",
    "# ë¹„êµ ì°¨íŠ¸\n",
    "comparison_data = {\n",
    "    'Model': ['10K Dataset', '30K Dataset'],\n",
    "    'Start Loss': [stats_10k['start_loss'], stats_30k['start_loss']],\n",
    "    'Final Loss': [stats_10k['final_loss'], stats_30k['final_loss']], \n",
    "    'Total Decrease': [stats_10k['total_decrease'], stats_30k['total_decrease']],\n",
    "    'Decrease Rate (%)': [stats_10k['decrease_rate'], stats_30k['decrease_rate']]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== ëª¨ë¸ ë¹„êµ í…Œì´ë¸” ===\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸\n",
    "fig_bar = go.Figure()\n",
    "\n",
    "fig_bar.add_trace(go.Bar(\n",
    "    name='Start Loss',\n",
    "    x=df_comparison['Model'],\n",
    "    y=df_comparison['Start Loss'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig_bar.add_trace(go.Bar(\n",
    "    name='Final Loss', \n",
    "    x=df_comparison['Model'],\n",
    "    y=df_comparison['Final Loss'],\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    title='Loss Comparison: Start vs Final',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Loss Value',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74147565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™”í˜• ì• ë‹ˆë©”ì´ì…˜ ìƒì„±\n",
    "print(\"=== ëŒ€í™”í˜• Gradient Descent ì• ë‹ˆë©”ì´ì…˜ ===\")\n",
    "\n",
    "def create_animated_descent(X, Y, Z, trajectory, title):\n",
    "    \"\"\"Gradient descent ì• ë‹ˆë©”ì´ì…˜ ìƒì„±\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Loss surface ì¶”ê°€\n",
    "    fig.add_trace(go.Surface(\n",
    "        x=X, y=Y, z=Z,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    ))\n",
    "    \n",
    "    # ì´ˆê¸°ì—ëŠ” ë¹ˆ ê²½ë¡œë¡œ ì‹œì‘\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[], y=[], z=[],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=5),\n",
    "        marker=dict(size=5, color='red'),\n",
    "        name='Descent Path'\n",
    "    ))\n",
    "    \n",
    "    # í˜„ì¬ ìœ„ì¹˜ ë§ˆì»¤\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[], y=[], z=[],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color='yellow', symbol='circle'),\n",
    "        name='Current Position'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='Parameter Dimension 1',\n",
    "            yaxis_title='Parameter Dimension 2', \n",
    "            zaxis_title='Loss Value',\n",
    "            camera=dict(\n",
    "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "            )\n",
    "        ),\n",
    "        updatemenus=[{\n",
    "            'type': 'buttons',\n",
    "            'showactive': False,\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'label': 'Play',\n",
    "                    'method': 'animate',\n",
    "                    'args': [None, {\n",
    "                        'frame': {'duration': 100, 'redraw': True},\n",
    "                        'fromcurrent': True\n",
    "                    }]\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Pause',\n",
    "                    'method': 'animate',\n",
    "                    'args': [[None], {\n",
    "                        'frame': {'duration': 0, 'redraw': False},\n",
    "                        'mode': 'immediate',\n",
    "                        'transition': {'duration': 0}\n",
    "                    }]\n",
    "                }\n",
    "            ]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # í”„ë ˆì„ ìƒì„±\n",
    "    frames = []\n",
    "    for i in range(1, len(trajectory)):\n",
    "        frame_data = [\n",
    "            # Loss surface (ë³€ê²½ ì—†ìŒ)\n",
    "            go.Surface(x=X, y=Y, z=Z, colorscale='Viridis', opacity=0.8),\n",
    "            # ê²½ë¡œ (í˜„ì¬ê¹Œì§€)\n",
    "            go.Scatter3d(\n",
    "                x=trajectory[:i, 0],\n",
    "                y=trajectory[:i, 1],\n",
    "                z=trajectory[:i, 2],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='red', width=5),\n",
    "                marker=dict(size=3, color='red')\n",
    "            ),\n",
    "            # í˜„ì¬ ìœ„ì¹˜\n",
    "            go.Scatter3d(\n",
    "                x=[trajectory[i-1, 0]],\n",
    "                y=[trajectory[i-1, 1]], \n",
    "                z=[trajectory[i-1, 2]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=10, color='yellow', symbol='circle')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        frames.append(go.Frame(data=frame_data, name=str(i)))\n",
    "    \n",
    "    fig.frames = frames\n",
    "    return fig\n",
    "\n",
    "# 10K ëª¨ë¸ ì• ë‹ˆë©”ì´ì…˜\n",
    "fig_anim_10k = create_animated_descent(\n",
    "    X, Y, Z_10k, trajectory_10k,\n",
    "    \"10K Dataset Model - Animated Gradient Descent\"\n",
    ")\n",
    "fig_anim_10k.show()\n",
    "\n",
    "# 30K ëª¨ë¸ ì• ë‹ˆë©”ì´ì…˜  \n",
    "fig_anim_30k = create_animated_descent(\n",
    "    X, Y, Z_30k, trajectory_30k,\n",
    "    \"30K Dataset Model - Animated Gradient Descent\"\n",
    ")\n",
    "fig_anim_30k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036267b",
   "metadata": {},
   "source": [
    "## ê²°ë¡  ë° ë¶„ì„ ìš”ì•½\n",
    "\n",
    "### ì£¼ìš” ê´€ì°° ì‚¬í•­:\n",
    "\n",
    "1. **Loss Landscape íŠ¹ì„±**:\n",
    "   - ë‘ ëª¨ë¸ ëª¨ë‘ ë³µì¡í•œ non-convex loss landscapeë¥¼ ë³´ì„\n",
    "   - Local minimaì™€ saddle pointë“¤ì´ ì¡´ì¬\n",
    "   - ìµœì í™” ê²½ë¡œê°€ ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„\n",
    "\n",
    "2. **Gradient Descent ìˆ˜ë ´ íŒ¨í„´**:\n",
    "   - 30K ë°ì´í„°ì…‹ ëª¨ë¸ì´ ë” ì•ˆì •ì ì¸ ìˆ˜ë ´ ë³´ì„\n",
    "   - 10K ëª¨ë¸ì€ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¸ ì´ˆê¸° ìˆ˜ë ´ì„ ë³´ì´ì§€ë§Œ ë” ë§ì€ ì§„ë™\n",
    "   - ë‘ ëª¨ë¸ ëª¨ë‘ ì „ì—­ ìµœì†Ÿê°’ ê·¼ì²˜ë¡œ ìˆ˜ë ´\n",
    "\n",
    "3. **ìµœì í™” íš¨ìœ¨ì„±**:\n",
    "   - ë” ë§ì€ ë°ì´í„°(30K)ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ë” ë¶€ë“œëŸ¬ìš´ loss landscapeë¥¼ ê°€ì§\n",
    "   - ì´ëŠ” ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "### ì‹¤ì œ ì ìš© ì‹œì‚¬ì :\n",
    "\n",
    "- **ë°ì´í„°ì…‹ í¬ê¸°**ê°€ ìµœì í™” landscapeì˜ ë³µì¡ì„±ì— ì˜í–¥ì„ ë¯¸ì¹¨\n",
    "- **Learning rate scheduling**ê³¼ **optimization algorithm** ì„ íƒì´ ì¤‘ìš”\n",
    "- **Early stopping** ì „ëµì„ í†µí•´ overfitting ë°©ì§€ ê°€ëŠ¥\n",
    "\n",
    "ì´ëŸ¬í•œ 3D ì‹œê°í™”ë¥¼ í†µí•´ fine-tuning ê³¼ì •ì—ì„œì˜ gradient descent ë™ì‘ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cacab",
   "metadata": {},
   "source": [
    "# ğŸš€ Google Colabì—ì„œ ì‹¤í–‰í•˜ëŠ” 3D Gradient Descent ì‹œê°í™”\n",
    "\n",
    "> **ğŸ”§ í™˜ê²½ ì„¤ì •**: ì´ ë…¸íŠ¸ë¶ì€ Google Colab í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "> \n",
    "> **ğŸ“ í•„ìˆ˜ ì¤€ë¹„ì‚¬í•­**: \n",
    "> - Google Driveì— `hyperclova-deobfuscation-lora-with-10k-datasets` í´ë”\n",
    "> - Google Driveì— `hyperclova-deobfuscation-lora-with-30k-datasets` í´ë”\n",
    "> \n",
    "> **âš¡ ê¶Œì¥ ëŸ°íƒ€ì„**: GPU (T4 ë˜ëŠ” ê·¸ ì´ìƒ)\n",
    "\n",
    "## ğŸ“‹ ì‹¤í–‰ ìˆœì„œ:\n",
    "1. **íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Drive ë§ˆìš´íŠ¸** - ì²« ë²ˆì§¸ ì…€ ì‹¤í–‰\n",
    "2. **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸** - ë‘ ë²ˆì§¸ ì…€ ì‹¤í–‰  \n",
    "3. **ëª¨ë¸ ë¡œë“œ** - Google Driveì—ì„œ fine-tuned ëª¨ë¸ ìë™ ë¡œë“œ\n",
    "4. **3D ì‹œê°í™” ìƒì„±** - Interactive í”Œë¡¯ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "5. **ê²°ê³¼ ì €ì¥** - Google Driveì— ë¶„ì„ ê²°ê³¼ ìë™ ì €ì¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e035c8",
   "metadata": {},
   "source": [
    "# ğŸ¯ 3D Gradient Descent Visualization for Fine-tuned Models\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ fine-tuningëœ HyperCLOVA ëª¨ë¸ì˜ gradient descent ê³¼ì •ì„ 3Dë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- í›ˆë ¨ ê³¼ì •ì—ì„œì˜ loss landscape 3D ì‹œê°í™”\n",
    "- Gradient descent ê²½ë¡œ ì¶”ì  ë° ì• ë‹ˆë©”ì´ì…˜\n",
    "- ë‹¤ì–‘í•œ fine-tuning ì„¤ì • ê°„ì˜ ìµœì í™” ê²½ë¡œ ë¹„êµ\n",
    "- Interactive 3D í”Œë¡¯ì„ í†µí•œ ëª¨ë¸ ìˆ˜ë ´ ê³¼ì • ë¶„ì„\n",
    "\n",
    "## ì‹œê°í™” ë‚´ìš©\n",
    "1. **Loss Surface 3D í”Œë¡¯**: íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œì˜ loss landscape\n",
    "2. **Gradient Descent Path**: ìµœì í™” ê²½ë¡œì˜ 3D ê¶¤ì \n",
    "3. **Multi-model ë¹„êµ**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ìˆ˜ë ´ ê²½ë¡œ ë¹„êµ\n",
    "4. **Interactive Dashboard**: ì‹¤ì‹œê°„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7c909",
   "metadata": {},
   "source": [
    "## ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab í™˜ê²½ì—ì„œ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q plotly>=5.0.0\n",
    "!pip install -q matplotlib>=3.5.0\n",
    "!pip install -q seaborn>=0.11.0\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q scikit-learn>=1.0.0\n",
    "!pip install -q accelerate>=0.20.0\n",
    "!pip install -q bitsandbytes>=0.39.0\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"ğŸ‰ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict, deque\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Colabì—ì„œ Plotly ì„¤ì •\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "print(\"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22064c9a",
   "metadata": {},
   "source": [
    "## ğŸ¨ 3D Gradient Descent Visualizer í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent3DVisualizer:\n",
    "    \"\"\"\n",
    "    Fine-tuning ê³¼ì •ì˜ gradient descentë¥¼ 3Dë¡œ ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, device='auto'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and device == 'auto' else device)\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # ì¶”ì í•  ë©”íŠ¸ë¦­ë“¤\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'gradient_norms': [],\n",
    "            'parameter_norms': [],\n",
    "            'learning_rates': [],\n",
    "            'epochs': [],\n",
    "            'batch_indices': [],\n",
    "            'layer_gradients': defaultdict(list),\n",
    "            'parameter_trajectory': [],\n",
    "            'gradient_directions': []\n",
    "        }\n",
    "        \n",
    "        # PCAë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "        self.parameter_snapshots = []\n",
    "        self.pca = None\n",
    "        \n",
    "        print(f\"ğŸ¯ 3D Visualizer ì´ˆê¸°í™” ì™„ë£Œ (Device: {self.device})\")\n",
    "    \n",
    "    def load_model(self, model_name=\"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            \n",
    "            # íŒ¨ë”© í† í° ì„¤ì •\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_lora(self, r=16, alpha=32, dropout=0.1):\n",
    "        \"\"\"LoRA ì„¤ì •\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        print(f\"ğŸ”§ LoRA ì„¤ì • ì™„ë£Œ (r={r}, alpha={alpha}, dropout={dropout})\")\n",
    "        return self.model\n",
    "    \n",
    "    def prepare_sample_data(self, num_samples=1000):\n",
    "        \"\"\"ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹œê°í™”ìš©)\"\"\"\n",
    "        print(\"ğŸ² ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ì„ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°\n",
    "        input_texts = [\n",
    "            \"ì•ˆë…•í•˜ì„¸ìš”\", \"ì¢‹ì€ í•˜ë£¨\", \"ê°ì‚¬í•©ë‹ˆë‹¤\", \"ì£„ì†¡í•©ë‹ˆë‹¤\",\n",
    "            \"ë„ì›€ì´ í•„ìš”í•´\", \"ë¬¸ì œê°€ ìˆì–´\", \"í•´ê²°ë°©ë²•\", \"ì •ë³´ë¥¼ ì°¾ì•„\"\n",
    "        ] * (num_samples // 8 + 1)\n",
    "        \n",
    "        target_texts = [\n",
    "            \"ì•ˆë…•í•˜ì„¸ìš”!\", \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!\", \"ê°ì‚¬í•©ë‹ˆë‹¤!\", \"ì£„ì†¡í•©ë‹ˆë‹¤!\",\n",
    "            \"ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤\", \"ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤\", \"í•´ê²°ë°©ë²•ì„ ì°¾ì•„ë³´ì„¸ìš”\", \"ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "        ] * (num_samples // 8 + 1)\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì§•\n",
    "        inputs = self.tokenizer(\n",
    "            input_texts[:num_samples],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            target_texts[:num_samples],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ìƒì„±\n",
    "        dataset = TensorDataset(\n",
    "            inputs['input_ids'],\n",
    "            inputs['attention_mask'],\n",
    "            targets['input_ids']\n",
    "        )\n",
    "        \n",
    "        self.dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "        print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ({num_samples} ìƒ˜í”Œ)\")\n",
    "        \n",
    "        return self.dataloader\n",
    "    \n",
    "    def compute_loss_surface_2d(self, center_params, direction1, direction2, alpha_range=(-1, 1), beta_range=(-1, 1), resolution=20):\n",
    "        \"\"\"2D loss surface ê³„ì‚° (3D ì‹œê°í™”ìš©)\"\"\"\n",
    "        print(\"ğŸ—ºï¸  Loss surface ê³„ì‚° ì¤‘...\")\n",
    "        \n",
    "        alphas = np.linspace(alpha_range[0], alpha_range[1], resolution)\n",
    "        betas = np.linspace(beta_range[0], beta_range[1], resolution)\n",
    "        \n",
    "        loss_surface = np.zeros((len(alphas), len(betas)))\n",
    "        \n",
    "        # ì›ë³¸ íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "        original_params = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                original_params[name] = param.data.clone()\n",
    "        \n",
    "        for i, alpha in enumerate(alphas):\n",
    "            for j, beta in enumerate(betas):\n",
    "                # íŒŒë¼ë¯¸í„° ì´ë™\n",
    "                idx = 0\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        param.data = (original_params[name] + \n",
    "                                    alpha * direction1[idx:idx+param.numel()].view(param.shape) +\n",
    "                                    beta * direction2[idx:idx+param.numel()].view(param.shape))\n",
    "                        idx += param.numel()\n",
    "                \n",
    "                # Loss ê³„ì‚°\n",
    "                self.model.eval()\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in self.dataloader:\n",
    "                        if num_batches >= 10:  # ê³„ì‚° ì‹œê°„ ë‹¨ì¶•\n",
    "                            break\n",
    "                        \n",
    "                        input_ids, attention_mask, labels = batch\n",
    "                        input_ids = input_ids.to(self.device)\n",
    "                        attention_mask = attention_mask.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        total_loss += outputs.loss.item()\n",
    "                        num_batches += 1\n",
    "                \n",
    "                loss_surface[i, j] = total_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        # ì›ë³¸ íŒŒë¼ë¯¸í„° ë³µì›\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = original_params[name]\n",
    "        \n",
    "        print(\"âœ… Loss surface ê³„ì‚° ì™„ë£Œ!\")\n",
    "        return alphas, betas, loss_surface\n",
    "    \n",
    "    def track_training_step(self, epoch, batch_idx, loss, optimizer):\n",
    "        \"\"\"í›ˆë ¨ ìŠ¤í…ë§ˆë‹¤ ë©”íŠ¸ë¦­ ì¶”ì \"\"\"\n",
    "        # Gradient norm ê³„ì‚°\n",
    "        total_grad_norm = 0.0\n",
    "        param_count = 0\n",
    "        current_params = []\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "                # íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ· (ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•´ í‰ê· í™”)\n",
    "                current_params.extend(param.data.flatten().cpu().numpy())\n",
    "                param_count += 1\n",
    "        \n",
    "        grad_norm = total_grad_norm ** (1. / 2) if param_count > 0 else 0.0\n",
    "        \n",
    "        # Parameter norm ê³„ì‚°\n",
    "        param_norm = sum(p.data.norm(2).item() ** 2 for p in self.model.parameters()) ** (1. / 2)\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ì €ì¥\n",
    "        self.training_history['losses'].append(loss)\n",
    "        self.training_history['gradient_norms'].append(grad_norm)\n",
    "        self.training_history['parameter_norms'].append(param_norm)\n",
    "        self.training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        self.training_history['epochs'].append(epoch)\n",
    "        self.training_history['batch_indices'].append(batch_idx)\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ· ì €ì¥ (ì¼ë¶€ë§Œ)\n",
    "        if len(current_params) > 1000:\n",
    "            # ë„ˆë¬´ í° ê²½ìš° ìƒ˜í”Œë§\n",
    "            step = len(current_params) // 1000\n",
    "            current_params = current_params[::step]\n",
    "        \n",
    "        self.parameter_snapshots.append(current_params)\n",
    "    \n",
    "    def reduce_dimensions(self, method='pca', n_components=3):\n",
    "        \"\"\"íŒŒë¼ë¯¸í„° ê¶¤ì ì˜ ì°¨ì› ì¶•ì†Œ\"\"\"\n",
    "        if not self.parameter_snapshots:\n",
    "            print(\"âŒ íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"ğŸ”„ ì°¨ì› ì¶•ì†Œ ì¤‘ ({method}, {n_components}D)...\")\n",
    "        \n",
    "        # ëª¨ë“  ìŠ¤ëƒ…ìƒ·ì„ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ê¸°\n",
    "        min_length = min(len(snapshot) for snapshot in self.parameter_snapshots)\n",
    "        aligned_snapshots = [snapshot[:min_length] for snapshot in self.parameter_snapshots]\n",
    "        \n",
    "        param_matrix = np.array(aligned_snapshots)\n",
    "        \n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"ì§€ì›ë˜ëŠ” ë°©ë²•: 'pca', 'tsne'\")\n",
    "        \n",
    "        reduced_params = reducer.fit_transform(param_matrix)\n",
    "        \n",
    "        self.pca = reducer  # ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥\n",
    "        print(\"âœ… ì°¨ì› ì¶•ì†Œ ì™„ë£Œ!\")\n",
    "        \n",
    "        return reduced_params\n",
    "\n",
    "print(\"ğŸ¨ GradientDescent3DVisualizer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44505596",
   "metadata": {},
   "source": [
    "## ğŸ¬ 3D ì‹œê°í™” í•¨ìˆ˜ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cebe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_loss_surface(alphas, betas, loss_surface, title=\"Loss Surface\"):\n",
    "    \"\"\"3D Loss Surface í”Œë¡¯ ìƒì„±\"\"\"\n",
    "    fig = go.Figure(data=[go.Surface(\n",
    "        x=alphas,\n",
    "        y=betas,\n",
    "        z=loss_surface,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='Parameter Direction 1',\n",
    "            yaxis_title='Parameter Direction 2',\n",
    "            zaxis_title='Loss',\n",
    "            camera=dict(eye=dict(x=1.2, y=1.2, z=1.2))\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_3d_gradient_path(trajectory, losses, title=\"Gradient Descent Path\"):\n",
    "    \"\"\"3D Gradient Descent ê²½ë¡œ í”Œë¡¯\"\"\"\n",
    "    if trajectory.shape[1] < 3:\n",
    "        print(\"âŒ 3D ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 3ì°¨ì›ì´ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "        return None\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # ê²½ë¡œ ì„ \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=trajectory[:, 0],\n",
    "        y=trajectory[:, 1],\n",
    "        z=trajectory[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=4),\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=losses,\n",
    "            colorscale='Plasma',\n",
    "            colorbar=dict(title=\"Loss\"),\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        name='Optimization Path'\n",
    "    ))\n",
    "    \n",
    "    # ì‹œì‘ì \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[trajectory[0, 0]],\n",
    "        y=[trajectory[0, 1]],\n",
    "        z=[trajectory[0, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='green', symbol='circle'),\n",
    "        name='Start'\n",
    "    ))\n",
    "    \n",
    "    # ëì \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[trajectory[-1, 0]],\n",
    "        y=[trajectory[-1, 1]],\n",
    "        z=[trajectory[-1, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='blue', symbol='circle'),\n",
    "        name='End'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3',\n",
    "            camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_training_dashboard(history):\n",
    "    \"\"\"í›ˆë ¨ ê³¼ì • ëŒ€ì‹œë³´ë“œ\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Loss Curve', 'Gradient Norm', 'Parameter Norm', 'Learning Rate'),\n",
    "        specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "               [{'secondary_y': False}, {'secondary_y': False}]]\n",
    "    )\n",
    "    \n",
    "    iterations = list(range(len(history['losses'])))\n",
    "    \n",
    "    # Loss curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['losses'], name='Loss', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gradient norm\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['gradient_norms'], name='Grad Norm', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Parameter norm\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['parameter_norms'], name='Param Norm', line=dict(color='green')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Learning rate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['learning_rates'], name='LR', line=dict(color='purple')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Training Metrics Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_interactive_loss_landscape(visualizer, resolution=15):\n",
    "    \"\"\"Interactive Loss Landscape with Gradient Descent Path\"\"\"\n",
    "    print(\"ğŸ¨ Interactive Loss Landscape ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ëœë¤ ë°©í–¥ ë²¡í„° ìƒì„± (PCA ê¸°ì¤€ìœ¼ë¡œ)\n",
    "    total_params = sum(p.numel() for p in visualizer.model.parameters() if p.requires_grad)\n",
    "    \n",
    "    direction1 = torch.randn(total_params) * 0.1\n",
    "    direction2 = torch.randn(total_params) * 0.1\n",
    "    \n",
    "    # ì§êµí™”\n",
    "    direction2 = direction2 - torch.dot(direction1, direction2) / torch.dot(direction1, direction1) * direction1\n",
    "    direction1 = direction1 / direction1.norm()\n",
    "    direction2 = direction2 / direction2.norm()\n",
    "    \n",
    "    # í˜„ì¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ loss surface ê³„ì‚°\n",
    "    center_params = torch.cat([p.data.view(-1) for p in visualizer.model.parameters() if p.requires_grad])\n",
    "    \n",
    "    alphas, betas, loss_surface = visualizer.compute_loss_surface_2d(\n",
    "        center_params, direction1, direction2, \n",
    "        alpha_range=(-0.5, 0.5), beta_range=(-0.5, 0.5), \n",
    "        resolution=resolution\n",
    "    )\n",
    "    \n",
    "    # Loss surface í”Œë¡¯\n",
    "    fig = create_3d_loss_surface(alphas, betas, loss_surface, \"Interactive Loss Landscape\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"ğŸ¬ 3D ì‹œê°í™” í•¨ìˆ˜ë“¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe48bc",
   "metadata": {},
   "source": [
    "## ğŸš€ ëª¨ë¸ ë¡œë“œ ë° í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c579997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Visualizer ì´ˆê¸°í™”\n",
    "visualizer = GradientDescent3DVisualizer()\n",
    "\n",
    "# Google Driveì—ì„œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "model_paths = [\n",
    "    \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets\",\n",
    "    \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets\"\n",
    "]\n",
    "\n",
    "available_model = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        available_model = path\n",
    "        print(f\"âœ… Google Driveì—ì„œ ëª¨ë¸ ë°œê²¬: {path}\")\n",
    "        break\n",
    "\n",
    "if available_model:\n",
    "    # Google Driveì˜ fine-tuned ëª¨ë¸ ì‚¬ìš©\n",
    "    try:\n",
    "        base_model_name = \"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"\n",
    "        \n",
    "        print(f\"ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ: {base_model_name}\")\n",
    "        visualizer.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ”§ Google Driveì—ì„œ LoRA ì–´ëŒ‘í„° ë¡œë“œ: {available_model}\")\n",
    "        visualizer.model = PeftModel.from_pretrained(base_model, available_model)\n",
    "        \n",
    "        if visualizer.tokenizer.pad_token is None:\n",
    "            visualizer.tokenizer.pad_token = visualizer.tokenizer.eos_token\n",
    "            \n",
    "        print(\"âœ… Fine-tuned ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Google Drive ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "        print(\"ğŸ”„ ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´...\")\n",
    "        visualizer.load_model()\n",
    "else:\n",
    "    print(\"ğŸ“¥ Google Driveì—ì„œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ...\")\n",
    "    visualizer.load_model()\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„\n",
    "dataloader = visualizer.prepare_sample_data(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ (ì§§ì€ í›ˆë ¨ìœ¼ë¡œ gradient descent ì¶”ì )\n",
    "print(\"ğŸƒâ€â™‚ï¸ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...\")\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = optim.AdamW(visualizer.model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# í›ˆë ¨ ë£¨í”„\n",
    "visualizer.model.train()\n",
    "num_epochs = 3\n",
    "max_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nğŸ“ˆ Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "        if batch_idx >= max_batches_per_epoch:\n",
    "            break\n",
    "            \n",
    "        # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        input_ids = input_ids.to(visualizer.device)\n",
    "        attention_mask = attention_mask.to(visualizer.device)\n",
    "        labels = labels.to(visualizer.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient tracking\n",
    "        visualizer.track_training_step(epoch, batch_idx, loss.item(), optimizer)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"  Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° í¬ì¸íŠ¸: {len(visualizer.training_history['losses'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9af81",
   "metadata": {},
   "source": [
    "## ğŸ“Š 3D ì‹œê°í™” ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í›ˆë ¨ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ\n",
    "print(\"ğŸ“Š í›ˆë ¨ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...\")\n",
    "dashboard_fig = create_training_dashboard(visualizer.training_history)\n",
    "dashboard_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. íŒŒë¼ë¯¸í„° ê¶¤ì  3D ì‹œê°í™”\n",
    "print(\"ğŸ”„ íŒŒë¼ë¯¸í„° ê¶¤ì  ì°¨ì› ì¶•ì†Œ ì¤‘...\")\n",
    "reduced_trajectory = visualizer.reduce_dimensions(method='pca', n_components=3)\n",
    "\n",
    "if reduced_trajectory is not None:\n",
    "    print(\"ğŸ¨ 3D ê¶¤ì  í”Œë¡¯ ìƒì„± ì¤‘...\")\n",
    "    trajectory_fig = create_3d_gradient_path(\n",
    "        reduced_trajectory, \n",
    "        visualizer.training_history['losses'],\n",
    "        \"3D Parameter Space Trajectory (PCA)\"\n",
    "    )\n",
    "    trajectory_fig.show()\n",
    "else:\n",
    "    print(\"âŒ ê¶¤ì  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interactive Loss Landscape\n",
    "print(\"ğŸ—ºï¸ Interactive Loss Landscape ìƒì„± ì¤‘...\")\n",
    "landscape_fig = create_interactive_loss_landscape(visualizer, resolution=12)\n",
    "landscape_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ca97c",
   "metadata": {},
   "source": [
    "## ğŸ” ëª¨ë¸ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_models():\n",
    "    \"\"\"ì—¬ëŸ¬ ëª¨ë¸ì˜ í›ˆë ¨ ê³¼ì • ë¹„êµ\"\"\"\n",
    "    model_configs = [\n",
    "        {'lr': 1e-4, 'weight_decay': 0.01, 'name': 'High LR'},\n",
    "        {'lr': 1e-5, 'weight_decay': 0.01, 'name': 'Medium LR'},\n",
    "        {'lr': 1e-6, 'weight_decay': 0.01, 'name': 'Low LR'}\n",
    "    ]\n",
    "    \n",
    "    comparison_data = {}\n",
    "    \n",
    "    print(\"ğŸ” ì—¬ëŸ¬ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ë¹„êµ ì¤‘...\")\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\nâš™ï¸ ì„¤ì •: {config['name']} (LR: {config['lr']})\")\n",
    "        \n",
    "        # ìƒˆë¡œìš´ visualizer ìƒì„±\n",
    "        temp_visualizer = GradientDescent3DVisualizer()\n",
    "        temp_visualizer.model = visualizer.model  # ê°™ì€ ëª¨ë¸ ì‚¬ìš©\n",
    "        temp_visualizer.tokenizer = visualizer.tokenizer\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "        temp_optimizer = optim.AdamW(\n",
    "            temp_visualizer.model.parameters(), \n",
    "            lr=config['lr'], \n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # ì§§ì€ í›ˆë ¨\n",
    "        temp_visualizer.model.train()\n",
    "        for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            if batch_idx >= 5:  # ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•´ 5ë°°ì¹˜ë§Œ\n",
    "                break\n",
    "                \n",
    "            input_ids = input_ids.to(temp_visualizer.device)\n",
    "            attention_mask = attention_mask.to(temp_visualizer.device)\n",
    "            labels = labels.to(temp_visualizer.device)\n",
    "            \n",
    "            temp_optimizer.zero_grad()\n",
    "            outputs = temp_visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            temp_visualizer.track_training_step(0, batch_idx, loss.item(), temp_optimizer)\n",
    "            temp_optimizer.step()\n",
    "        \n",
    "        comparison_data[config['name']] = temp_visualizer.training_history\n",
    "    \n",
    "    # ë¹„êµ í”Œë¡¯ ìƒì„±\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for name, history in comparison_data.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(history['losses']))),\n",
    "            y=history['losses'],\n",
    "            mode='lines+markers',\n",
    "            name=f'{name} - Loss',\n",
    "            line=dict(width=3)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Model Configuration Comparison\",\n",
    "        xaxis_title=\"Training Step\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ëª¨ë¸ ë¹„êµ ì‹¤í–‰\n",
    "comparison_fig = compare_multiple_models()\n",
    "comparison_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02735b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveì— ì €ì¥ëœ ë‘ ëª¨ë¸ ë¹„êµ (10k vs 30k)\n",
    "def compare_drive_models():\n",
    "    \"\"\"Google Driveì˜ 10kì™€ 30k ëª¨ë¸ ë¹„êµ\"\"\"\n",
    "    model_configs = [\n",
    "        {\n",
    "            'path': '/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets',\n",
    "            'name': '10K Dataset Model',\n",
    "            'color': 'blue'\n",
    "        },\n",
    "        {\n",
    "            'path': '/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets', \n",
    "            'name': '30K Dataset Model',\n",
    "            'color': 'red'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_data = {}\n",
    "    base_model_name = \"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"\n",
    "    \n",
    "    print(\"ğŸ” Google Driveì˜ ë‘ ëª¨ë¸ ë¹„êµ ì¤‘...\")\n",
    "    \n",
    "    for config in model_configs:\n",
    "        if not os.path.exists(config['path']):\n",
    "            print(f\"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config['path']}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nâš™ï¸ ëª¨ë¸ ë¡œë“œ: {config['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # LoRA ì–´ëŒ‘í„° ì ìš©\n",
    "            model = PeftModel.from_pretrained(base_model, config['path'])\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # ìƒˆë¡œìš´ visualizer ìƒì„±\n",
    "            temp_visualizer = GradientDescent3DVisualizer()\n",
    "            temp_visualizer.model = model\n",
    "            temp_visualizer.tokenizer = tokenizer\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "            temp_optimizer = optim.AdamW(temp_visualizer.model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "            \n",
    "            # ì§§ì€ í›ˆë ¨ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
    "            temp_visualizer.model.train()\n",
    "            sample_data = temp_visualizer.prepare_sample_data(num_samples=200)\n",
    "            \n",
    "            for batch_idx, (input_ids, attention_mask, labels) in enumerate(sample_data):\n",
    "                if batch_idx >= 8:  # ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•´ 8ë°°ì¹˜ë§Œ\n",
    "                    break\n",
    "                    \n",
    "                input_ids = input_ids.to(temp_visualizer.device)\n",
    "                attention_mask = attention_mask.to(temp_visualizer.device)\n",
    "                labels = labels.to(temp_visualizer.device)\n",
    "                \n",
    "                temp_optimizer.zero_grad()\n",
    "                outputs = temp_visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                temp_visualizer.track_training_step(0, batch_idx, loss.item(), temp_optimizer)\n",
    "                temp_optimizer.step()\n",
    "            \n",
    "            comparison_data[config['name']] = {\n",
    "                'history': temp_visualizer.training_history,\n",
    "                'color': config['color']\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… {config['name']} ë¶„ì„ ì™„ë£Œ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {config['name']} ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # ë¹„êµ í”Œë¡¯ ìƒì„±\n",
    "    if comparison_data:\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Loss Comparison', 'Gradient Norm', 'Parameter Norm', 'Convergence Rate'),\n",
    "            specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "                   [{'secondary_y': False}, {'secondary_y': False}]]\n",
    "        )\n",
    "        \n",
    "        for name, data in comparison_data.items():\n",
    "            history = data['history']\n",
    "            color = data['color']\n",
    "            steps = list(range(len(history['losses'])))\n",
    "            \n",
    "            # Loss ë¹„êµ\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['losses'], name=f'{name} - Loss', \n",
    "                         line=dict(color=color, width=3)), row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Gradient Norm ë¹„êµ\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['gradient_norms'], name=f'{name} - Grad Norm',\n",
    "                         line=dict(color=color, dash='dash')), row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Parameter Norm ë¹„êµ\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['parameter_norms'], name=f'{name} - Param Norm',\n",
    "                         line=dict(color=color, dash='dot')), row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # ìˆ˜ë ´ë¥  ê³„ì‚° (loss ê°ì†Œ ì†ë„)\n",
    "            if len(history['losses']) > 1:\n",
    "                convergence_rate = []\n",
    "                for i in range(1, len(history['losses'])):\n",
    "                    rate = (history['losses'][i-1] - history['losses'][i]) / history['losses'][i-1]\n",
    "                    convergence_rate.append(rate)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=list(range(1, len(history['losses']))), y=convergence_rate, \n",
    "                             name=f'{name} - Conv Rate', line=dict(color=color, dash='dashdot')), \n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"10K vs 30K Dataset Model Comparison\",\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    else:\n",
    "        print(\"âŒ ë¹„êµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# Google Drive ëª¨ë¸ ë¹„êµ ì‹¤í–‰\n",
    "drive_comparison_fig = compare_drive_models()\n",
    "if drive_comparison_fig:\n",
    "    drive_comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fdfe4",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ê²°ê³¼ ì €ì¥ ë° ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ab0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ íˆìŠ¤í† ë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "history_df = pd.DataFrame({\n",
    "    'step': range(len(visualizer.training_history['losses'])),\n",
    "    'epoch': visualizer.training_history['epochs'],\n",
    "    'batch_idx': visualizer.training_history['batch_indices'],\n",
    "    'loss': visualizer.training_history['losses'],\n",
    "    'gradient_norm': visualizer.training_history['gradient_norms'],\n",
    "    'parameter_norm': visualizer.training_history['parameter_norms'],\n",
    "    'learning_rate': visualizer.training_history['learning_rates']\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š í›ˆë ¨ íˆìŠ¤í† ë¦¬ í†µê³„:\")\n",
    "print(history_df.describe())\n",
    "\n",
    "# CSVë¡œ ì €ì¥\n",
    "output_path = \"/Users/jw/PycharmProjects/FineTuningLLM/gradient_descent_analysis.csv\"\n",
    "history_df.to_csv(output_path, index=False)\n",
    "print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "print(f\"\"\"\n",
    "ğŸ¯ Gradient Descent ë¶„ì„ ê²°ê³¼:\n",
    "\n",
    "ğŸ“ˆ í›ˆë ¨ ê°œìš”:\n",
    "- ì´ í›ˆë ¨ ìŠ¤í…: {len(visualizer.training_history['losses'])}\n",
    "- ìµœì¢… Loss: {visualizer.training_history['losses'][-1]:.6f}\n",
    "- ì´ˆê¸° Loss: {visualizer.training_history['losses'][0]:.6f}\n",
    "- Loss ê°ì†Œìœ¨: {(visualizer.training_history['losses'][0] - visualizer.training_history['losses'][-1]) / visualizer.training_history['losses'][0] * 100:.2f}%\n",
    "\n",
    "ğŸ” Gradient ë¶„ì„:\n",
    "- í‰ê·  Gradient Norm: {np.mean(visualizer.training_history['gradient_norms']):.6f}\n",
    "- ìµœëŒ€ Gradient Norm: {np.max(visualizer.training_history['gradient_norms']):.6f}\n",
    "- ìµœì†Œ Gradient Norm: {np.min(visualizer.training_history['gradient_norms']):.6f}\n",
    "\n",
    "âš™ï¸ íŒŒë¼ë¯¸í„° ë¶„ì„:\n",
    "- í‰ê·  Parameter Norm: {np.mean(visualizer.training_history['parameter_norms']):.6f}\n",
    "- íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰: {abs(visualizer.training_history['parameter_norms'][-1] - visualizer.training_history['parameter_norms'][0]):.6f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab GPU ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "def optimize_gpu_memory():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ìµœì í™”\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"ğŸ”§ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        print(f\"ğŸ’¾ í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"ğŸ’¾ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CPU ëª¨ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰\n",
    "optimize_gpu_memory()\n",
    "\n",
    "# Colabì—ì„œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\nâ±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ\")\n",
    "print(\"\\nğŸ‰ 3D Gradient Descent ì‹œê°í™” ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ“ ì‚¬ìš©ë²•:\")\n",
    "print(\"1. ìœ„ì˜ ì‹œê°í™” ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì˜ ìµœì í™” ê³¼ì •ì„ ë¶„ì„í•˜ì„¸ìš”\")\n",
    "print(\"2. Google Driveì— ì €ì¥ëœ HTML íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì˜¤í”„ë¼ì¸ì—ì„œë„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤\")\n",
    "print(\"3. CSV ë°ì´í„°ë¥¼ í†µí•´ ì¶”ê°€ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ed2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveì— ê²°ê³¼ ì €ì¥\n",
    "output_dir = \"/content/drive/MyDrive/gradient_descent_analysis/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# í›ˆë ¨ íˆìŠ¤í† ë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "history_df = pd.DataFrame({\n",
    "    'step': range(len(visualizer.training_history['losses'])),\n",
    "    'epoch': visualizer.training_history['epochs'],\n",
    "    'batch_idx': visualizer.training_history['batch_indices'],\n",
    "    'loss': visualizer.training_history['losses'],\n",
    "    'gradient_norm': visualizer.training_history['gradient_norms'],\n",
    "    'parameter_norm': visualizer.training_history['parameter_norms'],\n",
    "    'learning_rate': visualizer.training_history['learning_rates']\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š í›ˆë ¨ íˆìŠ¤í† ë¦¬ í†µê³„:\")\n",
    "print(history_df.describe())\n",
    "\n",
    "# Google Driveì— CSVë¡œ ì €ì¥\n",
    "output_path = os.path.join(output_dir, \"gradient_descent_analysis.csv\")\n",
    "history_df.to_csv(output_path, index=False)\n",
    "print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# ì‹œê°í™” ê²°ê³¼ë„ HTMLë¡œ ì €ì¥\n",
    "if 'dashboard_fig' in locals():\n",
    "    dashboard_fig.write_html(os.path.join(output_dir, \"training_dashboard.html\"))\n",
    "    print(\"ğŸ“Š ëŒ€ì‹œë³´ë“œ HTML ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if 'trajectory_fig' in locals():\n",
    "    trajectory_fig.write_html(os.path.join(output_dir, \"3d_trajectory.html\"))\n",
    "    print(\"ğŸ¨ 3D ê¶¤ì  HTML ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "if 'landscape_fig' in locals():\n",
    "    landscape_fig.write_html(os.path.join(output_dir, \"loss_landscape.html\"))\n",
    "    print(\"ğŸ—ºï¸ Loss Landscape HTML ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "print(f\"\"\"\n",
    "ğŸ¯ Gradient Descent ë¶„ì„ ê²°ê³¼:\n",
    "\n",
    "ğŸ“ˆ í›ˆë ¨ ê°œìš”:\n",
    "- ì´ í›ˆë ¨ ìŠ¤í…: {len(visualizer.training_history['losses'])}\n",
    "- ìµœì¢… Loss: {visualizer.training_history['losses'][-1]:.6f}\n",
    "- ì´ˆê¸° Loss: {visualizer.training_history['losses'][0]:.6f}\n",
    "- Loss ê°ì†Œìœ¨: {(visualizer.training_history['losses'][0] - visualizer.training_history['losses'][-1]) / visualizer.training_history['losses'][0] * 100:.2f}%\n",
    "\n",
    "ğŸ” Gradient ë¶„ì„:\n",
    "- í‰ê·  Gradient Norm: {np.mean(visualizer.training_history['gradient_norms']):.6f}\n",
    "- ìµœëŒ€ Gradient Norm: {np.max(visualizer.training_history['gradient_norms']):.6f}\n",
    "- ìµœì†Œ Gradient Norm: {np.min(visualizer.training_history['gradient_norms']):.6f}\n",
    "\n",
    "âš™ï¸ íŒŒë¼ë¯¸í„° ë¶„ì„:\n",
    "- í‰ê·  Parameter Norm: {np.mean(visualizer.training_history['parameter_norms']):.6f}\n",
    "- íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰: {abs(visualizer.training_history['parameter_norms'][-1] - visualizer.training_history['parameter_norms'][0]):.6f}\n",
    "\n",
    "ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\n",
    "- CSV ë°ì´í„°: {output_path}\n",
    "- HTML ì‹œê°í™”: {output_dir}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c29ec",
   "metadata": {},
   "source": [
    "## ğŸ‰ ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "### ğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­:\n",
    "\n",
    "1. **Loss Landscape**: 3D ì‹œê°í™”ë¥¼ í†µí•´ ëª¨ë¸ì´ ìµœì í™”ë˜ëŠ” ê³¼ì •ì„ ì§ê´€ì ìœ¼ë¡œ í™•ì¸\n",
    "2. **Gradient Descent Path**: íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œì˜ ìµœì í™” ê²½ë¡œë¥¼ 3ì°¨ì›ìœ¼ë¡œ ì¶”ì \n",
    "3. **ìˆ˜ë ´ íŒ¨í„´**: ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì— ë”°ë¥¸ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„± ë¹„êµ\n",
    "\n",
    "### ğŸ” í™œìš© ë°©ì•ˆ:\n",
    "\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: Loss landscapeë¥¼ í†µí•œ ìµœì  í•™ìŠµë¥  ì°¾ê¸°\n",
    "- **ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„**: Gradient norm ë³€í™”ë¥¼ í†µí•œ í›ˆë ¨ ì•ˆì •ì„± í‰ê°€  \n",
    "- **ìˆ˜ë ´ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ 3D ì‹œê°í™”ë¥¼ í†µí•œ í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "### ğŸ“ˆ í–¥í›„ ê°œì„ ì :\n",
    "\n",
    "- ë” ê¸´ í›ˆë ¨ ê³¼ì •ì—ì„œì˜ long-term íŒ¨í„´ ë¶„ì„\n",
    "- ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ ë¹„êµ\n",
    "- Interactive dashboardë¥¼ í†µí•œ ì‹¤ì‹œê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ fine-tuning ê³¼ì •ì˜ gradient descentë¥¼ 3Dë¡œ ì‹œê°í™”í•˜ì—¬ ëª¨ë¸ ìµœì í™” ê³¼ì •ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
