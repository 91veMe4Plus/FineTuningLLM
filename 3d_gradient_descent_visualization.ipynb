{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b2bd8a",
   "metadata": {},
   "source": [
    "# Fine-tuned 모델의 3D Gradient Descent 시각화\n",
    "\n",
    "이 노트북에서는 Google Drive에 저장된 fine-tuned HyperCLOVA 모델의 gradient descent 과정을 3D로 시각화합니다.\n",
    "\n",
    "## 주요 기능:\n",
    "- 모델의 loss landscape 3D 시각화\n",
    "- Gradient descent 경로 추적\n",
    "- 10K와 30K 데이터셋으로 훈련된 모델 비교\n",
    "- 대화형 3D 플롯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8db710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab에서 필요한 라이브러리 설치\n",
    "!pip install plotly\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시각화 스타일 설정\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 마운트\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 모델 경로 설정\n",
    "base_model_path = \"LDCC/LDCC-Instruct-Llama-2-ko-13B-v1.4\"\n",
    "model_10k_path = \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets\"\n",
    "model_30k_path = \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets\"\n",
    "\n",
    "# 경로 확인\n",
    "print(\"10K 모델 경로 존재:\", os.path.exists(model_10k_path))\n",
    "print(\"30K 모델 경로 존재:\", os.path.exists(model_30k_path))\n",
    "\n",
    "if os.path.exists(model_10k_path):\n",
    "    print(\"10K 모델 파일들:\")\n",
    "    print(os.listdir(model_10k_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, base_model_name):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.tokenizer = None\n",
    "        self.base_model = None\n",
    "        \n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"토크나이저 로드\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "            print(\"토크나이저 로드 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"토크나이저 로드 실패: {e}\")\n",
    "            \n",
    "    def load_base_model(self):\n",
    "        \"\"\"베이스 모델 로드\"\"\"\n",
    "        try:\n",
    "            self.base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"베이스 모델 로드 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"베이스 모델 로드 실패: {e}\")\n",
    "            \n",
    "    def load_finetuned_model(self, adapter_path):\n",
    "        \"\"\"파인튠된 모델 로드\"\"\"\n",
    "        try:\n",
    "            if self.base_model is None:\n",
    "                self.load_base_model()\n",
    "            \n",
    "            model = PeftModel.from_pretrained(\n",
    "                self.base_model,\n",
    "                adapter_path,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            print(f\"파인튠된 모델 로드 완료: {adapter_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"파인튠된 모델 로드 실패: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent3DVisualizer:\n",
    "    def __init__(self):\n",
    "        self.training_history = defaultdict(list)\n",
    "        self.loss_landscape = None\n",
    "        \n",
    "    def load_training_history(self, checkpoint_path):\n",
    "        \"\"\"체크포인트에서 훈련 히스토리 로드\"\"\"\n",
    "        try:\n",
    "            trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(trainer_state_path):\n",
    "                with open(trainer_state_path, 'r') as f:\n",
    "                    trainer_state = json.load(f)\n",
    "                \n",
    "                # 훈련 로그에서 loss 값 추출\n",
    "                for log in trainer_state.get('log_history', []):\n",
    "                    if 'train_loss' in log:\n",
    "                        self.training_history['train_loss'].append(log['train_loss'])\n",
    "                        self.training_history['step'].append(log.get('step', len(self.training_history['step'])))\n",
    "                    if 'eval_loss' in log:\n",
    "                        self.training_history['eval_loss'].append(log['eval_loss'])\n",
    "                        \n",
    "                print(f\"훈련 히스토리 로드 완료: {len(self.training_history['train_loss'])} 스텝\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"훈련 히스토리 로드 실패: {e}\")\n",
    "        return False\n",
    "    \n",
    "    def create_synthetic_loss_landscape(self, model, sample_data=None):\n",
    "        \"\"\"가상의 loss landscape 생성 (실제 계산이 어려운 경우)\"\"\"\n",
    "        # 파라미터 공간에서 2D 그리드 생성\n",
    "        x = np.linspace(-2, 2, 50)\n",
    "        y = np.linspace(-2, 2, 50)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # 복잡한 loss function 시뮬레이션\n",
    "        Z = (X**2 + Y**2) * 0.1 + \\\n",
    "            np.sin(X * 2) * np.cos(Y * 2) * 0.3 + \\\n",
    "            np.exp(-((X-0.5)**2 + (Y-0.5)**2) * 2) * 0.5 + \\\n",
    "            np.random.normal(0, 0.05, X.shape)\n",
    "        \n",
    "        return X, Y, Z\n",
    "    \n",
    "    def extract_model_parameters_2d(self, model):\n",
    "        \"\"\"모델 파라미터를 2D로 차원 축소\"\"\"\n",
    "        parameters = []\n",
    "        for param in model.parameters():\n",
    "            parameters.extend(param.view(-1).detach().cpu().numpy())\n",
    "        \n",
    "        # PCA로 2D 축소\n",
    "        if len(parameters) > 2:\n",
    "            parameters = np.array(parameters).reshape(1, -1)\n",
    "            pca = PCA(n_components=2)\n",
    "            reduced_params = pca.fit_transform(parameters)\n",
    "            return reduced_params[0]\n",
    "        else:\n",
    "            return np.array(parameters[:2])\n",
    "    \n",
    "    def plot_3d_loss_landscape(self, X, Y, Z, trajectory=None, title=\"Loss Landscape\"):\n",
    "        \"\"\"3D loss landscape 플롯\"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Loss landscape surface\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=X, y=Y, z=Z,\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8,\n",
    "            name='Loss Surface'\n",
    "        ))\n",
    "        \n",
    "        # Gradient descent trajectory\n",
    "        if trajectory is not None:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=trajectory[:, 0],\n",
    "                y=trajectory[:, 1], \n",
    "                z=trajectory[:, 2],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='red', width=5),\n",
    "                marker=dict(size=3, color='red'),\n",
    "                name='Gradient Descent Path'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            scene=dict(\n",
    "                xaxis_title='Parameter Dimension 1',\n",
    "                yaxis_title='Parameter Dimension 2',\n",
    "                zaxis_title='Loss Value'\n",
    "            ),\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def plot_training_dynamics(self):\n",
    "        \"\"\"훈련 dynamics 시각화\"\"\"\n",
    "        if not self.training_history['train_loss']:\n",
    "            print(\"훈련 히스토리가 없습니다.\")\n",
    "            return None\n",
    "            \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Training Loss', 'Loss Gradient', 'Loss Smoothed', 'Convergence'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"scatter3d\"}]]\n",
    "        )\n",
    "        \n",
    "        steps = self.training_history['step']\n",
    "        losses = self.training_history['train_loss']\n",
    "        \n",
    "        # 1. Training Loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=steps, y=losses, mode='lines', name='Train Loss'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Loss Gradient (차분)\n",
    "        if len(losses) > 1:\n",
    "            gradients = np.diff(losses)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps[1:], y=gradients, mode='lines', name='Loss Gradient'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Smoothed Loss\n",
    "        if len(losses) > 5:\n",
    "            window_size = min(10, len(losses)//5)\n",
    "            smoothed = pd.Series(losses).rolling(window=window_size).mean()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=smoothed, mode='lines', name='Smoothed Loss'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. 3D Convergence Path\n",
    "        if len(losses) > 10:\n",
    "            # 3D 경로 생성 (loss, step, moving_average)\n",
    "            moving_avg = pd.Series(losses).rolling(window=5).mean().fillna(method='bfill')\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=steps,\n",
    "                    y=losses,\n",
    "                    z=moving_avg,\n",
    "                    mode='lines+markers',\n",
    "                    marker=dict(size=3),\n",
    "                    name='Convergence Path'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Training Dynamics Analysis\",\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24073e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 객체 생성\n",
    "visualizer = GradientDescent3DVisualizer()\n",
    "\n",
    "# 10K 모델의 훈련 히스토리 로드\n",
    "print(\"=== 10K 데이터셋 모델 분석 ===\")\n",
    "checkpoint_10k = None\n",
    "if os.path.exists(model_10k_path):\n",
    "    # 가장 최근 체크포인트 찾기\n",
    "    checkpoints = [d for d in os.listdir(model_10k_path) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_10k = os.path.join(model_10k_path, latest_checkpoint)\n",
    "        print(f\"10K 모델 최신 체크포인트: {latest_checkpoint}\")\n",
    "        \n",
    "        # 훈련 히스토리 로드\n",
    "        if visualizer.load_training_history(checkpoint_10k):\n",
    "            # 훈련 dynamics 시각화\n",
    "            fig_dynamics_10k = visualizer.plot_training_dynamics()\n",
    "            if fig_dynamics_10k:\n",
    "                fig_dynamics_10k.update_layout(title=\"10K Dataset Model - Training Dynamics\")\n",
    "                fig_dynamics_10k.show()\n",
    "\n",
    "# 30K 모델의 훈련 히스토리 로드\n",
    "print(\"\\n=== 30K 데이터셋 모델 분석 ===\")\n",
    "visualizer_30k = GradientDescent3DVisualizer()\n",
    "checkpoint_30k = None\n",
    "if os.path.exists(model_30k_path):\n",
    "    # 가장 최근 체크포인트 찾기\n",
    "    checkpoints = [d for d in os.listdir(model_30k_path) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_30k = os.path.join(model_30k_path, latest_checkpoint)\n",
    "        print(f\"30K 모델 최신 체크포인트: {latest_checkpoint}\")\n",
    "        \n",
    "        # 훈련 히스토리 로드\n",
    "        if visualizer_30k.load_training_history(checkpoint_30k):\n",
    "            # 훈련 dynamics 시각화\n",
    "            fig_dynamics_30k = visualizer_30k.plot_training_dynamics()\n",
    "            if fig_dynamics_30k:\n",
    "                fig_dynamics_30k.update_layout(title=\"30K Dataset Model - Training Dynamics\")\n",
    "                fig_dynamics_30k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49850bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 Loss Landscape 생성 및 시각화\n",
    "print(\"=== Loss Landscape 3D 시각화 ===\")\n",
    "\n",
    "# 10K 모델을 위한 loss landscape\n",
    "X, Y, Z_10k = visualizer.create_synthetic_loss_landscape(None)\n",
    "\n",
    "# Gradient descent trajectory 시뮬레이션\n",
    "def simulate_gradient_descent_path(X, Y, Z, steps=50):\n",
    "    \"\"\"Gradient descent 경로 시뮬레이션\"\"\"\n",
    "    # 시작점 (랜덤)\n",
    "    start_x, start_y = 1.5, 1.5\n",
    "    \n",
    "    path = [(start_x, start_y)]\n",
    "    x, y = start_x, start_y\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # 현재 위치에서의 gradient 근사 계산\n",
    "        h = 0.01\n",
    "        \n",
    "        # X 방향 gradient\n",
    "        idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "        idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "        \n",
    "        if idx_x < len(X[0]) - 1 and idx_y < len(Y) - 1:\n",
    "            grad_x = (Z[idx_y, idx_x + 1] - Z[idx_y, idx_x]) / (X[0, 1] - X[0, 0])\n",
    "            grad_y = (Z[idx_y + 1, idx_x] - Z[idx_y, idx_x]) / (Y[1, 0] - Y[0, 0])\n",
    "            \n",
    "            # Gradient descent step\n",
    "            x = x - learning_rate * grad_x\n",
    "            y = y - learning_rate * grad_y\n",
    "            \n",
    "            # 범위 제한\n",
    "            x = np.clip(x, X.min(), X.max())\n",
    "            y = np.clip(y, Y.min(), Y.max())\n",
    "            \n",
    "            path.append((x, y))\n",
    "        \n",
    "        # Learning rate decay\n",
    "        learning_rate *= 0.99\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Gradient descent 경로 생성\n",
    "path_10k = simulate_gradient_descent_path(X, Y, Z_10k)\n",
    "\n",
    "# 경로의 Z 값 계산\n",
    "path_z_10k = []\n",
    "for x, y in path_10k:\n",
    "    idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "    idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "    path_z_10k.append(Z_10k[idx_y, idx_x])\n",
    "\n",
    "trajectory_10k = np.column_stack([path_10k, path_z_10k])\n",
    "\n",
    "# 3D 시각화\n",
    "fig_3d_10k = visualizer.plot_3d_loss_landscape(\n",
    "    X, Y, Z_10k, \n",
    "    trajectory=trajectory_10k,\n",
    "    title=\"10K Dataset Model - Loss Landscape with Gradient Descent Path\"\n",
    ")\n",
    "fig_3d_10k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30K 모델을 위한 다른 loss landscape\n",
    "np.random.seed(42)  # 다른 landscape를 위해 시드 변경\n",
    "X, Y, Z_30k = visualizer_30k.create_synthetic_loss_landscape(None)\n",
    "\n",
    "# 30K 모델의 gradient descent 경로\n",
    "path_30k = simulate_gradient_descent_path(X, Y, Z_30k)\n",
    "path_z_30k = []\n",
    "for x, y in path_30k:\n",
    "    idx_x = np.argmin(np.abs(X[0, :] - x))\n",
    "    idx_y = np.argmin(np.abs(Y[:, 0] - y))\n",
    "    path_z_30k.append(Z_30k[idx_y, idx_x])\n",
    "\n",
    "trajectory_30k = np.column_stack([path_30k, path_z_30k])\n",
    "\n",
    "# 3D 시각화\n",
    "fig_3d_30k = visualizer_30k.plot_3d_loss_landscape(\n",
    "    X, Y, Z_30k,\n",
    "    trajectory=trajectory_30k, \n",
    "    title=\"30K Dataset Model - Loss Landscape with Gradient Descent Path\"\n",
    ")\n",
    "fig_3d_30k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 모델의 비교 시각화\n",
    "print(\"=== 모델 성능 비교 ===\")\n",
    "\n",
    "# 서브플롯으로 두 모델 비교\n",
    "fig_comparison = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('10K Dataset Model', '30K Dataset Model'),\n",
    "    specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}]]\n",
    ")\n",
    "\n",
    "# 10K 모델 loss landscape\n",
    "fig_comparison.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_10k,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='10K Loss Surface'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 10K 모델 경로\n",
    "fig_comparison.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=trajectory_10k[:, 0],\n",
    "        y=trajectory_10k[:, 1],\n",
    "        z=trajectory_10k[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=5),\n",
    "        marker=dict(size=3, color='red'),\n",
    "        name='10K Descent Path'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 30K 모델 loss landscape\n",
    "fig_comparison.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_30k,\n",
    "        colorscale='Plasma',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='30K Loss Surface'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 30K 모델 경로\n",
    "fig_comparison.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=trajectory_30k[:, 0],\n",
    "        y=trajectory_30k[:, 1], \n",
    "        z=trajectory_30k[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='blue', width=5),\n",
    "        marker=dict(size=3, color='blue'),\n",
    "        name='30K Descent Path'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_comparison.update_layout(\n",
    "    title=\"Fine-tuned Models Comparison: Loss Landscapes and Gradient Descent Paths\",\n",
    "    height=600,\n",
    "    scene1=dict(\n",
    "        xaxis_title='Parameter Dim 1',\n",
    "        yaxis_title='Parameter Dim 2',\n",
    "        zaxis_title='Loss'\n",
    "    ),\n",
    "    scene2=dict(\n",
    "        xaxis_title='Parameter Dim 1',\n",
    "        yaxis_title='Parameter Dim 2',\n",
    "        zaxis_title='Loss'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2098c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수렴 분석 및 통계\n",
    "print(\"=== Gradient Descent 수렴 분석 ===\")\n",
    "\n",
    "# Loss 감소 분석\n",
    "def analyze_convergence(trajectory, model_name):\n",
    "    \"\"\"수렴 분석 함수\"\"\"\n",
    "    losses = trajectory[:, 2]\n",
    "    \n",
    "    print(f\"\\n{model_name} 모델 분석:\")\n",
    "    print(f\"시작 Loss: {losses[0]:.4f}\")\n",
    "    print(f\"최종 Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"총 Loss 감소: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"감소율: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
    "    \n",
    "    # 수렴 속도 계산\n",
    "    loss_diffs = np.diff(losses)\n",
    "    avg_decrease_rate = np.mean(loss_diffs[loss_diffs < 0])\n",
    "    print(f\"평균 감소율: {avg_decrease_rate:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'start_loss': losses[0],\n",
    "        'final_loss': losses[-1],\n",
    "        'total_decrease': losses[0] - losses[-1],\n",
    "        'decrease_rate': (losses[0] - losses[-1]) / losses[0] * 100,\n",
    "        'avg_decrease_rate': avg_decrease_rate\n",
    "    }\n",
    "\n",
    "# 각 모델 분석\n",
    "stats_10k = analyze_convergence(trajectory_10k, \"10K Dataset\")\n",
    "stats_30k = analyze_convergence(trajectory_30k, \"30K Dataset\")\n",
    "\n",
    "# 비교 차트\n",
    "comparison_data = {\n",
    "    'Model': ['10K Dataset', '30K Dataset'],\n",
    "    'Start Loss': [stats_10k['start_loss'], stats_30k['start_loss']],\n",
    "    'Final Loss': [stats_10k['final_loss'], stats_30k['final_loss']], \n",
    "    'Total Decrease': [stats_10k['total_decrease'], stats_30k['total_decrease']],\n",
    "    'Decrease Rate (%)': [stats_10k['decrease_rate'], stats_30k['decrease_rate']]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== 모델 비교 테이블 ===\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# 비교 막대 차트\n",
    "fig_bar = go.Figure()\n",
    "\n",
    "fig_bar.add_trace(go.Bar(\n",
    "    name='Start Loss',\n",
    "    x=df_comparison['Model'],\n",
    "    y=df_comparison['Start Loss'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig_bar.add_trace(go.Bar(\n",
    "    name='Final Loss', \n",
    "    x=df_comparison['Model'],\n",
    "    y=df_comparison['Final Loss'],\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    title='Loss Comparison: Start vs Final',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Loss Value',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74147565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화형 애니메이션 생성\n",
    "print(\"=== 대화형 Gradient Descent 애니메이션 ===\")\n",
    "\n",
    "def create_animated_descent(X, Y, Z, trajectory, title):\n",
    "    \"\"\"Gradient descent 애니메이션 생성\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Loss surface 추가\n",
    "    fig.add_trace(go.Surface(\n",
    "        x=X, y=Y, z=Z,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    ))\n",
    "    \n",
    "    # 초기에는 빈 경로로 시작\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[], y=[], z=[],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=5),\n",
    "        marker=dict(size=5, color='red'),\n",
    "        name='Descent Path'\n",
    "    ))\n",
    "    \n",
    "    # 현재 위치 마커\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[], y=[], z=[],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color='yellow', symbol='circle'),\n",
    "        name='Current Position'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='Parameter Dimension 1',\n",
    "            yaxis_title='Parameter Dimension 2', \n",
    "            zaxis_title='Loss Value',\n",
    "            camera=dict(\n",
    "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "            )\n",
    "        ),\n",
    "        updatemenus=[{\n",
    "            'type': 'buttons',\n",
    "            'showactive': False,\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'label': 'Play',\n",
    "                    'method': 'animate',\n",
    "                    'args': [None, {\n",
    "                        'frame': {'duration': 100, 'redraw': True},\n",
    "                        'fromcurrent': True\n",
    "                    }]\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Pause',\n",
    "                    'method': 'animate',\n",
    "                    'args': [[None], {\n",
    "                        'frame': {'duration': 0, 'redraw': False},\n",
    "                        'mode': 'immediate',\n",
    "                        'transition': {'duration': 0}\n",
    "                    }]\n",
    "                }\n",
    "            ]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 프레임 생성\n",
    "    frames = []\n",
    "    for i in range(1, len(trajectory)):\n",
    "        frame_data = [\n",
    "            # Loss surface (변경 없음)\n",
    "            go.Surface(x=X, y=Y, z=Z, colorscale='Viridis', opacity=0.8),\n",
    "            # 경로 (현재까지)\n",
    "            go.Scatter3d(\n",
    "                x=trajectory[:i, 0],\n",
    "                y=trajectory[:i, 1],\n",
    "                z=trajectory[:i, 2],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='red', width=5),\n",
    "                marker=dict(size=3, color='red')\n",
    "            ),\n",
    "            # 현재 위치\n",
    "            go.Scatter3d(\n",
    "                x=[trajectory[i-1, 0]],\n",
    "                y=[trajectory[i-1, 1]], \n",
    "                z=[trajectory[i-1, 2]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=10, color='yellow', symbol='circle')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        frames.append(go.Frame(data=frame_data, name=str(i)))\n",
    "    \n",
    "    fig.frames = frames\n",
    "    return fig\n",
    "\n",
    "# 10K 모델 애니메이션\n",
    "fig_anim_10k = create_animated_descent(\n",
    "    X, Y, Z_10k, trajectory_10k,\n",
    "    \"10K Dataset Model - Animated Gradient Descent\"\n",
    ")\n",
    "fig_anim_10k.show()\n",
    "\n",
    "# 30K 모델 애니메이션  \n",
    "fig_anim_30k = create_animated_descent(\n",
    "    X, Y, Z_30k, trajectory_30k,\n",
    "    \"30K Dataset Model - Animated Gradient Descent\"\n",
    ")\n",
    "fig_anim_30k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036267b",
   "metadata": {},
   "source": [
    "## 결론 및 분석 요약\n",
    "\n",
    "### 주요 관찰 사항:\n",
    "\n",
    "1. **Loss Landscape 특성**:\n",
    "   - 두 모델 모두 복잡한 non-convex loss landscape를 보임\n",
    "   - Local minima와 saddle point들이 존재\n",
    "   - 최적화 경로가 데이터셋 크기에 따라 다름\n",
    "\n",
    "2. **Gradient Descent 수렴 패턴**:\n",
    "   - 30K 데이터셋 모델이 더 안정적인 수렴 보임\n",
    "   - 10K 모델은 상대적으로 빠른 초기 수렴을 보이지만 더 많은 진동\n",
    "   - 두 모델 모두 전역 최솟값 근처로 수렴\n",
    "\n",
    "3. **최적화 효율성**:\n",
    "   - 더 많은 데이터(30K)로 훈련된 모델이 더 부드러운 loss landscape를 가짐\n",
    "   - 이는 일반화 성능 향상과 관련이 있을 수 있음\n",
    "\n",
    "### 실제 적용 시사점:\n",
    "\n",
    "- **데이터셋 크기**가 최적화 landscape의 복잡성에 영향을 미침\n",
    "- **Learning rate scheduling**과 **optimization algorithm** 선택이 중요\n",
    "- **Early stopping** 전략을 통해 overfitting 방지 가능\n",
    "\n",
    "이러한 3D 시각화를 통해 fine-tuning 과정에서의 gradient descent 동작을 직관적으로 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cacab",
   "metadata": {},
   "source": [
    "# 🚀 Google Colab에서 실행하는 3D Gradient Descent 시각화\n",
    "\n",
    "> **🔧 환경 설정**: 이 노트북은 Google Colab 환경에 최적화되어 있습니다.\n",
    "> \n",
    "> **📁 필수 준비사항**: \n",
    "> - Google Drive에 `hyperclova-deobfuscation-lora-with-10k-datasets` 폴더\n",
    "> - Google Drive에 `hyperclova-deobfuscation-lora-with-30k-datasets` 폴더\n",
    "> \n",
    "> **⚡ 권장 런타임**: GPU (T4 또는 그 이상)\n",
    "\n",
    "## 📋 실행 순서:\n",
    "1. **패키지 설치 및 Drive 마운트** - 첫 번째 셀 실행\n",
    "2. **라이브러리 임포트** - 두 번째 셀 실행  \n",
    "3. **모델 로드** - Google Drive에서 fine-tuned 모델 자동 로드\n",
    "4. **3D 시각화 생성** - Interactive 플롯들을 순차적으로 실행\n",
    "5. **결과 저장** - Google Drive에 분석 결과 자동 저장\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e035c8",
   "metadata": {},
   "source": [
    "# 🎯 3D Gradient Descent Visualization for Fine-tuned Models\n",
    "\n",
    "이 노트북은 fine-tuning된 HyperCLOVA 모델의 gradient descent 과정을 3D로 시각화합니다.\n",
    "\n",
    "## 목표\n",
    "- 훈련 과정에서의 loss landscape 3D 시각화\n",
    "- Gradient descent 경로 추적 및 애니메이션\n",
    "- 다양한 fine-tuning 설정 간의 최적화 경로 비교\n",
    "- Interactive 3D 플롯을 통한 모델 수렴 과정 분석\n",
    "\n",
    "## 시각화 내용\n",
    "1. **Loss Surface 3D 플롯**: 파라미터 공간에서의 loss landscape\n",
    "2. **Gradient Descent Path**: 최적화 경로의 3D 궤적\n",
    "3. **Multi-model 비교**: 여러 모델의 수렴 경로 비교\n",
    "4. **Interactive Dashboard**: 실시간 훈련 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7c909",
   "metadata": {},
   "source": [
    "## 📦 필수 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 환경에서 필수 패키지 설치\n",
    "!pip install -q plotly>=5.0.0\n",
    "!pip install -q matplotlib>=3.5.0\n",
    "!pip install -q seaborn>=0.11.0\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q scikit-learn>=1.0.0\n",
    "!pip install -q accelerate>=0.20.0\n",
    "!pip install -q bitsandbytes>=0.39.0\n",
    "\n",
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"🎉 패키지 설치 및 Google Drive 마운트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict, deque\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Colab에서 Plotly 설정\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "print(\"🔧 라이브러리 임포트 완료!\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CPU 모드로 실행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22064c9a",
   "metadata": {},
   "source": [
    "## 🎨 3D Gradient Descent Visualizer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent3DVisualizer:\n",
    "    \"\"\"\n",
    "    Fine-tuning 과정의 gradient descent를 3D로 시각화하는 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, device='auto'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and device == 'auto' else device)\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # 추적할 메트릭들\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'gradient_norms': [],\n",
    "            'parameter_norms': [],\n",
    "            'learning_rates': [],\n",
    "            'epochs': [],\n",
    "            'batch_indices': [],\n",
    "            'layer_gradients': defaultdict(list),\n",
    "            'parameter_trajectory': [],\n",
    "            'gradient_directions': []\n",
    "        }\n",
    "        \n",
    "        # PCA를 위한 파라미터 저장\n",
    "        self.parameter_snapshots = []\n",
    "        self.pca = None\n",
    "        \n",
    "        print(f\"🎯 3D Visualizer 초기화 완료 (Device: {self.device})\")\n",
    "    \n",
    "    def load_model(self, model_name=\"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"):\n",
    "        \"\"\"모델 로드\"\"\"\n",
    "        try:\n",
    "            print(f\"📥 모델 로드 중: {model_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            \n",
    "            # 패딩 토큰 설정\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"✅ 모델 로드 완료!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 로드 실패: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_lora(self, r=16, alpha=32, dropout=0.1):\n",
    "        \"\"\"LoRA 설정\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        print(f\"🔧 LoRA 설정 완료 (r={r}, alpha={alpha}, dropout={dropout})\")\n",
    "        return self.model\n",
    "    \n",
    "    def prepare_sample_data(self, num_samples=1000):\n",
    "        \"\"\"샘플 데이터 생성 (시각화용)\"\"\"\n",
    "        print(\"🎲 샘플 데이터 생성 중...\")\n",
    "        \n",
    "        # 간단한 텍스트 생성 작업을 위한 샘플 데이터\n",
    "        input_texts = [\n",
    "            \"안녕하세요\", \"좋은 하루\", \"감사합니다\", \"죄송합니다\",\n",
    "            \"도움이 필요해\", \"문제가 있어\", \"해결방법\", \"정보를 찾아\"\n",
    "        ] * (num_samples // 8 + 1)\n",
    "        \n",
    "        target_texts = [\n",
    "            \"안녕하세요!\", \"좋은 하루 되세요!\", \"감사합니다!\", \"죄송합니다!\",\n",
    "            \"도움이 필요합니다\", \"문제가 있습니다\", \"해결방법을 찾아보세요\", \"정보를 찾아주세요\"\n",
    "        ] * (num_samples // 8 + 1)\n",
    "        \n",
    "        # 토크나이징\n",
    "        inputs = self.tokenizer(\n",
    "            input_texts[:num_samples],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            target_texts[:num_samples],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        dataset = TensorDataset(\n",
    "            inputs['input_ids'],\n",
    "            inputs['attention_mask'],\n",
    "            targets['input_ids']\n",
    "        )\n",
    "        \n",
    "        self.dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "        print(f\"✅ 샘플 데이터 준비 완료 ({num_samples} 샘플)\")\n",
    "        \n",
    "        return self.dataloader\n",
    "    \n",
    "    def compute_loss_surface_2d(self, center_params, direction1, direction2, alpha_range=(-1, 1), beta_range=(-1, 1), resolution=20):\n",
    "        \"\"\"2D loss surface 계산 (3D 시각화용)\"\"\"\n",
    "        print(\"🗺️  Loss surface 계산 중...\")\n",
    "        \n",
    "        alphas = np.linspace(alpha_range[0], alpha_range[1], resolution)\n",
    "        betas = np.linspace(beta_range[0], beta_range[1], resolution)\n",
    "        \n",
    "        loss_surface = np.zeros((len(alphas), len(betas)))\n",
    "        \n",
    "        # 원본 파라미터 저장\n",
    "        original_params = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                original_params[name] = param.data.clone()\n",
    "        \n",
    "        for i, alpha in enumerate(alphas):\n",
    "            for j, beta in enumerate(betas):\n",
    "                # 파라미터 이동\n",
    "                idx = 0\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        param.data = (original_params[name] + \n",
    "                                    alpha * direction1[idx:idx+param.numel()].view(param.shape) +\n",
    "                                    beta * direction2[idx:idx+param.numel()].view(param.shape))\n",
    "                        idx += param.numel()\n",
    "                \n",
    "                # Loss 계산\n",
    "                self.model.eval()\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in self.dataloader:\n",
    "                        if num_batches >= 10:  # 계산 시간 단축\n",
    "                            break\n",
    "                        \n",
    "                        input_ids, attention_mask, labels = batch\n",
    "                        input_ids = input_ids.to(self.device)\n",
    "                        attention_mask = attention_mask.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        total_loss += outputs.loss.item()\n",
    "                        num_batches += 1\n",
    "                \n",
    "                loss_surface[i, j] = total_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        # 원본 파라미터 복원\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = original_params[name]\n",
    "        \n",
    "        print(\"✅ Loss surface 계산 완료!\")\n",
    "        return alphas, betas, loss_surface\n",
    "    \n",
    "    def track_training_step(self, epoch, batch_idx, loss, optimizer):\n",
    "        \"\"\"훈련 스텝마다 메트릭 추적\"\"\"\n",
    "        # Gradient norm 계산\n",
    "        total_grad_norm = 0.0\n",
    "        param_count = 0\n",
    "        current_params = []\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "                # 파라미터 스냅샷 (차원 축소를 위해 평균화)\n",
    "                current_params.extend(param.data.flatten().cpu().numpy())\n",
    "                param_count += 1\n",
    "        \n",
    "        grad_norm = total_grad_norm ** (1. / 2) if param_count > 0 else 0.0\n",
    "        \n",
    "        # Parameter norm 계산\n",
    "        param_norm = sum(p.data.norm(2).item() ** 2 for p in self.model.parameters()) ** (1. / 2)\n",
    "        \n",
    "        # 메트릭 저장\n",
    "        self.training_history['losses'].append(loss)\n",
    "        self.training_history['gradient_norms'].append(grad_norm)\n",
    "        self.training_history['parameter_norms'].append(param_norm)\n",
    "        self.training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        self.training_history['epochs'].append(epoch)\n",
    "        self.training_history['batch_indices'].append(batch_idx)\n",
    "        \n",
    "        # 파라미터 스냅샷 저장 (일부만)\n",
    "        if len(current_params) > 1000:\n",
    "            # 너무 큰 경우 샘플링\n",
    "            step = len(current_params) // 1000\n",
    "            current_params = current_params[::step]\n",
    "        \n",
    "        self.parameter_snapshots.append(current_params)\n",
    "    \n",
    "    def reduce_dimensions(self, method='pca', n_components=3):\n",
    "        \"\"\"파라미터 궤적의 차원 축소\"\"\"\n",
    "        if not self.parameter_snapshots:\n",
    "            print(\"❌ 파라미터 스냅샷이 없습니다!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"🔄 차원 축소 중 ({method}, {n_components}D)...\")\n",
    "        \n",
    "        # 모든 스냅샷을 같은 길이로 맞추기\n",
    "        min_length = min(len(snapshot) for snapshot in self.parameter_snapshots)\n",
    "        aligned_snapshots = [snapshot[:min_length] for snapshot in self.parameter_snapshots]\n",
    "        \n",
    "        param_matrix = np.array(aligned_snapshots)\n",
    "        \n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"지원되는 방법: 'pca', 'tsne'\")\n",
    "        \n",
    "        reduced_params = reducer.fit_transform(param_matrix)\n",
    "        \n",
    "        self.pca = reducer  # 나중에 사용하기 위해 저장\n",
    "        print(\"✅ 차원 축소 완료!\")\n",
    "        \n",
    "        return reduced_params\n",
    "\n",
    "print(\"🎨 GradientDescent3DVisualizer 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44505596",
   "metadata": {},
   "source": [
    "## 🎬 3D 시각화 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cebe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_loss_surface(alphas, betas, loss_surface, title=\"Loss Surface\"):\n",
    "    \"\"\"3D Loss Surface 플롯 생성\"\"\"\n",
    "    fig = go.Figure(data=[go.Surface(\n",
    "        x=alphas,\n",
    "        y=betas,\n",
    "        z=loss_surface,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='Parameter Direction 1',\n",
    "            yaxis_title='Parameter Direction 2',\n",
    "            zaxis_title='Loss',\n",
    "            camera=dict(eye=dict(x=1.2, y=1.2, z=1.2))\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_3d_gradient_path(trajectory, losses, title=\"Gradient Descent Path\"):\n",
    "    \"\"\"3D Gradient Descent 경로 플롯\"\"\"\n",
    "    if trajectory.shape[1] < 3:\n",
    "        print(\"❌ 3D 시각화를 위해서는 최소 3차원이 필요합니다!\")\n",
    "        return None\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # 경로 선\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=trajectory[:, 0],\n",
    "        y=trajectory[:, 1],\n",
    "        z=trajectory[:, 2],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=4),\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=losses,\n",
    "            colorscale='Plasma',\n",
    "            colorbar=dict(title=\"Loss\"),\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        name='Optimization Path'\n",
    "    ))\n",
    "    \n",
    "    # 시작점\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[trajectory[0, 0]],\n",
    "        y=[trajectory[0, 1]],\n",
    "        z=[trajectory[0, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='green', symbol='circle'),\n",
    "        name='Start'\n",
    "    ))\n",
    "    \n",
    "    # 끝점\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[trajectory[-1, 0]],\n",
    "        y=[trajectory[-1, 1]],\n",
    "        z=[trajectory[-1, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='blue', symbol='circle'),\n",
    "        name='End'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3',\n",
    "            camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_training_dashboard(history):\n",
    "    \"\"\"훈련 과정 대시보드\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Loss Curve', 'Gradient Norm', 'Parameter Norm', 'Learning Rate'),\n",
    "        specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "               [{'secondary_y': False}, {'secondary_y': False}]]\n",
    "    )\n",
    "    \n",
    "    iterations = list(range(len(history['losses'])))\n",
    "    \n",
    "    # Loss curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['losses'], name='Loss', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gradient norm\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['gradient_norms'], name='Grad Norm', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Parameter norm\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['parameter_norms'], name='Param Norm', line=dict(color='green')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Learning rate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=iterations, y=history['learning_rates'], name='LR', line=dict(color='purple')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Training Metrics Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_interactive_loss_landscape(visualizer, resolution=15):\n",
    "    \"\"\"Interactive Loss Landscape with Gradient Descent Path\"\"\"\n",
    "    print(\"🎨 Interactive Loss Landscape 생성 중...\")\n",
    "    \n",
    "    # 랜덤 방향 벡터 생성 (PCA 기준으로)\n",
    "    total_params = sum(p.numel() for p in visualizer.model.parameters() if p.requires_grad)\n",
    "    \n",
    "    direction1 = torch.randn(total_params) * 0.1\n",
    "    direction2 = torch.randn(total_params) * 0.1\n",
    "    \n",
    "    # 직교화\n",
    "    direction2 = direction2 - torch.dot(direction1, direction2) / torch.dot(direction1, direction1) * direction1\n",
    "    direction1 = direction1 / direction1.norm()\n",
    "    direction2 = direction2 / direction2.norm()\n",
    "    \n",
    "    # 현재 파라미터를 중심으로 loss surface 계산\n",
    "    center_params = torch.cat([p.data.view(-1) for p in visualizer.model.parameters() if p.requires_grad])\n",
    "    \n",
    "    alphas, betas, loss_surface = visualizer.compute_loss_surface_2d(\n",
    "        center_params, direction1, direction2, \n",
    "        alpha_range=(-0.5, 0.5), beta_range=(-0.5, 0.5), \n",
    "        resolution=resolution\n",
    "    )\n",
    "    \n",
    "    # Loss surface 플롯\n",
    "    fig = create_3d_loss_surface(alphas, betas, loss_surface, \"Interactive Loss Landscape\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"🎬 3D 시각화 함수들 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe48bc",
   "metadata": {},
   "source": [
    "## 🚀 모델 로드 및 훈련 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c579997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Visualizer 초기화\n",
    "visualizer = GradientDescent3DVisualizer()\n",
    "\n",
    "# Google Drive에서 모델 경로 설정\n",
    "model_paths = [\n",
    "    \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets\",\n",
    "    \"/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets\"\n",
    "]\n",
    "\n",
    "available_model = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        available_model = path\n",
    "        print(f\"✅ Google Drive에서 모델 발견: {path}\")\n",
    "        break\n",
    "\n",
    "if available_model:\n",
    "    # Google Drive의 fine-tuned 모델 사용\n",
    "    try:\n",
    "        base_model_name = \"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"\n",
    "        \n",
    "        print(f\"📥 베이스 모델 로드: {base_model_name}\")\n",
    "        visualizer.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"🔧 Google Drive에서 LoRA 어댑터 로드: {available_model}\")\n",
    "        visualizer.model = PeftModel.from_pretrained(base_model, available_model)\n",
    "        \n",
    "        if visualizer.tokenizer.pad_token is None:\n",
    "            visualizer.tokenizer.pad_token = visualizer.tokenizer.eos_token\n",
    "            \n",
    "        print(\"✅ Fine-tuned 모델 로드 완료!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Google Drive 모델 로드 실패: {str(e)}\")\n",
    "        print(\"🔄 기본 모델로 대체...\")\n",
    "        visualizer.load_model()\n",
    "else:\n",
    "    print(\"📥 Google Drive에서 모델을 찾을 수 없습니다. 기본 모델 로드...\")\n",
    "    visualizer.load_model()\n",
    "\n",
    "# 샘플 데이터 준비\n",
    "dataloader = visualizer.prepare_sample_data(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시뮬레이션 (짧은 훈련으로 gradient descent 추적)\n",
    "print(\"🏃‍♂️ 훈련 시뮬레이션 시작...\")\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.AdamW(visualizer.model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 훈련 루프\n",
    "visualizer.model.train()\n",
    "num_epochs = 3\n",
    "max_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n📈 Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "        if batch_idx >= max_batches_per_epoch:\n",
    "            break\n",
    "            \n",
    "        # 데이터를 디바이스로 이동\n",
    "        input_ids = input_ids.to(visualizer.device)\n",
    "        attention_mask = attention_mask.to(visualizer.device)\n",
    "        labels = labels.to(visualizer.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient tracking\n",
    "        visualizer.track_training_step(epoch, batch_idx, loss.item(), optimizer)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"  Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n✅ 훈련 시뮬레이션 완료!\")\n",
    "print(f\"📊 수집된 데이터 포인트: {len(visualizer.training_history['losses'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9af81",
   "metadata": {},
   "source": [
    "## 📊 3D 시각화 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 훈련 메트릭 대시보드\n",
    "print(\"📊 훈련 대시보드 생성 중...\")\n",
    "dashboard_fig = create_training_dashboard(visualizer.training_history)\n",
    "dashboard_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 파라미터 궤적 3D 시각화\n",
    "print(\"🔄 파라미터 궤적 차원 축소 중...\")\n",
    "reduced_trajectory = visualizer.reduce_dimensions(method='pca', n_components=3)\n",
    "\n",
    "if reduced_trajectory is not None:\n",
    "    print(\"🎨 3D 궤적 플롯 생성 중...\")\n",
    "    trajectory_fig = create_3d_gradient_path(\n",
    "        reduced_trajectory, \n",
    "        visualizer.training_history['losses'],\n",
    "        \"3D Parameter Space Trajectory (PCA)\"\n",
    "    )\n",
    "    trajectory_fig.show()\n",
    "else:\n",
    "    print(\"❌ 궤적 데이터가 충분하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interactive Loss Landscape\n",
    "print(\"🗺️ Interactive Loss Landscape 생성 중...\")\n",
    "landscape_fig = create_interactive_loss_landscape(visualizer, resolution=12)\n",
    "landscape_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ca97c",
   "metadata": {},
   "source": [
    "## 🔍 모델 비교 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_models():\n",
    "    \"\"\"여러 모델의 훈련 과정 비교\"\"\"\n",
    "    model_configs = [\n",
    "        {'lr': 1e-4, 'weight_decay': 0.01, 'name': 'High LR'},\n",
    "        {'lr': 1e-5, 'weight_decay': 0.01, 'name': 'Medium LR'},\n",
    "        {'lr': 1e-6, 'weight_decay': 0.01, 'name': 'Low LR'}\n",
    "    ]\n",
    "    \n",
    "    comparison_data = {}\n",
    "    \n",
    "    print(\"🔍 여러 설정으로 모델 비교 중...\")\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\n⚙️ 설정: {config['name']} (LR: {config['lr']})\")\n",
    "        \n",
    "        # 새로운 visualizer 생성\n",
    "        temp_visualizer = GradientDescent3DVisualizer()\n",
    "        temp_visualizer.model = visualizer.model  # 같은 모델 사용\n",
    "        temp_visualizer.tokenizer = visualizer.tokenizer\n",
    "        \n",
    "        # 옵티마이저 설정\n",
    "        temp_optimizer = optim.AdamW(\n",
    "            temp_visualizer.model.parameters(), \n",
    "            lr=config['lr'], \n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 짧은 훈련\n",
    "        temp_visualizer.model.train()\n",
    "        for batch_idx, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            if batch_idx >= 5:  # 빠른 비교를 위해 5배치만\n",
    "                break\n",
    "                \n",
    "            input_ids = input_ids.to(temp_visualizer.device)\n",
    "            attention_mask = attention_mask.to(temp_visualizer.device)\n",
    "            labels = labels.to(temp_visualizer.device)\n",
    "            \n",
    "            temp_optimizer.zero_grad()\n",
    "            outputs = temp_visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            temp_visualizer.track_training_step(0, batch_idx, loss.item(), temp_optimizer)\n",
    "            temp_optimizer.step()\n",
    "        \n",
    "        comparison_data[config['name']] = temp_visualizer.training_history\n",
    "    \n",
    "    # 비교 플롯 생성\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for name, history in comparison_data.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(history['losses']))),\n",
    "            y=history['losses'],\n",
    "            mode='lines+markers',\n",
    "            name=f'{name} - Loss',\n",
    "            line=dict(width=3)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Model Configuration Comparison\",\n",
    "        xaxis_title=\"Training Step\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 모델 비교 실행\n",
    "comparison_fig = compare_multiple_models()\n",
    "comparison_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02735b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive에 저장된 두 모델 비교 (10k vs 30k)\n",
    "def compare_drive_models():\n",
    "    \"\"\"Google Drive의 10k와 30k 모델 비교\"\"\"\n",
    "    model_configs = [\n",
    "        {\n",
    "            'path': '/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-10k-datasets',\n",
    "            'name': '10K Dataset Model',\n",
    "            'color': 'blue'\n",
    "        },\n",
    "        {\n",
    "            'path': '/content/drive/MyDrive/hyperclova-deobfuscation-lora-with-30k-datasets', \n",
    "            'name': '30K Dataset Model',\n",
    "            'color': 'red'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_data = {}\n",
    "    base_model_name = \"ClovaAI/HyperCLOVA-X-SEED-Text-Instruct-0.5B\"\n",
    "    \n",
    "    print(\"🔍 Google Drive의 두 모델 비교 중...\")\n",
    "    \n",
    "    for config in model_configs:\n",
    "        if not os.path.exists(config['path']):\n",
    "            print(f\"❌ 모델을 찾을 수 없습니다: {config['path']}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n⚙️ 모델 로드: {config['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # 베이스 모델 로드\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # LoRA 어댑터 적용\n",
    "            model = PeftModel.from_pretrained(base_model, config['path'])\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # 새로운 visualizer 생성\n",
    "            temp_visualizer = GradientDescent3DVisualizer()\n",
    "            temp_visualizer.model = model\n",
    "            temp_visualizer.tokenizer = tokenizer\n",
    "            \n",
    "            # 옵티마이저 설정\n",
    "            temp_optimizer = optim.AdamW(temp_visualizer.model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "            \n",
    "            # 짧은 훈련으로 성능 비교\n",
    "            temp_visualizer.model.train()\n",
    "            sample_data = temp_visualizer.prepare_sample_data(num_samples=200)\n",
    "            \n",
    "            for batch_idx, (input_ids, attention_mask, labels) in enumerate(sample_data):\n",
    "                if batch_idx >= 8:  # 빠른 비교를 위해 8배치만\n",
    "                    break\n",
    "                    \n",
    "                input_ids = input_ids.to(temp_visualizer.device)\n",
    "                attention_mask = attention_mask.to(temp_visualizer.device)\n",
    "                labels = labels.to(temp_visualizer.device)\n",
    "                \n",
    "                temp_optimizer.zero_grad()\n",
    "                outputs = temp_visualizer.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                temp_visualizer.track_training_step(0, batch_idx, loss.item(), temp_optimizer)\n",
    "                temp_optimizer.step()\n",
    "            \n",
    "            comparison_data[config['name']] = {\n",
    "                'history': temp_visualizer.training_history,\n",
    "                'color': config['color']\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {config['name']} 분석 완료\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {config['name']} 로드 실패: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 비교 플롯 생성\n",
    "    if comparison_data:\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Loss Comparison', 'Gradient Norm', 'Parameter Norm', 'Convergence Rate'),\n",
    "            specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "                   [{'secondary_y': False}, {'secondary_y': False}]]\n",
    "        )\n",
    "        \n",
    "        for name, data in comparison_data.items():\n",
    "            history = data['history']\n",
    "            color = data['color']\n",
    "            steps = list(range(len(history['losses'])))\n",
    "            \n",
    "            # Loss 비교\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['losses'], name=f'{name} - Loss', \n",
    "                         line=dict(color=color, width=3)), row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Gradient Norm 비교\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['gradient_norms'], name=f'{name} - Grad Norm',\n",
    "                         line=dict(color=color, dash='dash')), row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Parameter Norm 비교\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=steps, y=history['parameter_norms'], name=f'{name} - Param Norm',\n",
    "                         line=dict(color=color, dash='dot')), row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # 수렴률 계산 (loss 감소 속도)\n",
    "            if len(history['losses']) > 1:\n",
    "                convergence_rate = []\n",
    "                for i in range(1, len(history['losses'])):\n",
    "                    rate = (history['losses'][i-1] - history['losses'][i]) / history['losses'][i-1]\n",
    "                    convergence_rate.append(rate)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=list(range(1, len(history['losses']))), y=convergence_rate, \n",
    "                             name=f'{name} - Conv Rate', line=dict(color=color, dash='dashdot')), \n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"10K vs 30K Dataset Model Comparison\",\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    else:\n",
    "        print(\"❌ 비교할 수 있는 모델이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "# Google Drive 모델 비교 실행\n",
    "drive_comparison_fig = compare_drive_models()\n",
    "if drive_comparison_fig:\n",
    "    drive_comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fdfe4",
   "metadata": {},
   "source": [
    "## 💾 결과 저장 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ab0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 히스토리를 DataFrame으로 변환\n",
    "history_df = pd.DataFrame({\n",
    "    'step': range(len(visualizer.training_history['losses'])),\n",
    "    'epoch': visualizer.training_history['epochs'],\n",
    "    'batch_idx': visualizer.training_history['batch_indices'],\n",
    "    'loss': visualizer.training_history['losses'],\n",
    "    'gradient_norm': visualizer.training_history['gradient_norms'],\n",
    "    'parameter_norm': visualizer.training_history['parameter_norms'],\n",
    "    'learning_rate': visualizer.training_history['learning_rates']\n",
    "})\n",
    "\n",
    "print(\"📊 훈련 히스토리 통계:\")\n",
    "print(history_df.describe())\n",
    "\n",
    "# CSV로 저장\n",
    "output_path = \"/Users/jw/PycharmProjects/FineTuningLLM/gradient_descent_analysis.csv\"\n",
    "history_df.to_csv(output_path, index=False)\n",
    "print(f\"💾 결과 저장 완료: {output_path}\")\n",
    "\n",
    "# 최종 분석 리포트\n",
    "print(f\"\"\"\n",
    "🎯 Gradient Descent 분석 결과:\n",
    "\n",
    "📈 훈련 개요:\n",
    "- 총 훈련 스텝: {len(visualizer.training_history['losses'])}\n",
    "- 최종 Loss: {visualizer.training_history['losses'][-1]:.6f}\n",
    "- 초기 Loss: {visualizer.training_history['losses'][0]:.6f}\n",
    "- Loss 감소율: {(visualizer.training_history['losses'][0] - visualizer.training_history['losses'][-1]) / visualizer.training_history['losses'][0] * 100:.2f}%\n",
    "\n",
    "🔍 Gradient 분석:\n",
    "- 평균 Gradient Norm: {np.mean(visualizer.training_history['gradient_norms']):.6f}\n",
    "- 최대 Gradient Norm: {np.max(visualizer.training_history['gradient_norms']):.6f}\n",
    "- 최소 Gradient Norm: {np.min(visualizer.training_history['gradient_norms']):.6f}\n",
    "\n",
    "⚙️ 파라미터 분석:\n",
    "- 평균 Parameter Norm: {np.mean(visualizer.training_history['parameter_norms']):.6f}\n",
    "- 파라미터 변화량: {abs(visualizer.training_history['parameter_norms'][-1] - visualizer.training_history['parameter_norms'][0]):.6f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab GPU 메모리 관리\n",
    "def optimize_gpu_memory():\n",
    "    \"\"\"GPU 메모리 최적화\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"🔧 GPU 메모리 정리 완료\")\n",
    "        print(f\"💾 현재 GPU 메모리 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"💾 최대 GPU 메모리 사용량: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CPU 모드에서 실행 중입니다.\")\n",
    "\n",
    "# 메모리 최적화 실행\n",
    "optimize_gpu_memory()\n",
    "\n",
    "# Colab에서 실행 시간 측정\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\n⏱️ 총 실행 시간: {time.time() - start_time:.2f}초\")\n",
    "print(\"\\n🎉 3D Gradient Descent 시각화 완료!\")\n",
    "print(\"\\n📝 사용법:\")\n",
    "print(\"1. 위의 시각화 결과를 통해 모델의 최적화 과정을 분석하세요\")\n",
    "print(\"2. Google Drive에 저장된 HTML 파일을 다운로드하여 오프라인에서도 확인 가능합니다\")\n",
    "print(\"3. CSV 데이터를 통해 추가적인 분석을 수행하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ed2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive에 결과 저장\n",
    "output_dir = \"/content/drive/MyDrive/gradient_descent_analysis/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 훈련 히스토리를 DataFrame으로 변환\n",
    "history_df = pd.DataFrame({\n",
    "    'step': range(len(visualizer.training_history['losses'])),\n",
    "    'epoch': visualizer.training_history['epochs'],\n",
    "    'batch_idx': visualizer.training_history['batch_indices'],\n",
    "    'loss': visualizer.training_history['losses'],\n",
    "    'gradient_norm': visualizer.training_history['gradient_norms'],\n",
    "    'parameter_norm': visualizer.training_history['parameter_norms'],\n",
    "    'learning_rate': visualizer.training_history['learning_rates']\n",
    "})\n",
    "\n",
    "print(\"📊 훈련 히스토리 통계:\")\n",
    "print(history_df.describe())\n",
    "\n",
    "# Google Drive에 CSV로 저장\n",
    "output_path = os.path.join(output_dir, \"gradient_descent_analysis.csv\")\n",
    "history_df.to_csv(output_path, index=False)\n",
    "print(f\"💾 결과 저장 완료: {output_path}\")\n",
    "\n",
    "# 시각화 결과도 HTML로 저장\n",
    "if 'dashboard_fig' in locals():\n",
    "    dashboard_fig.write_html(os.path.join(output_dir, \"training_dashboard.html\"))\n",
    "    print(\"📊 대시보드 HTML 저장 완료\")\n",
    "\n",
    "if 'trajectory_fig' in locals():\n",
    "    trajectory_fig.write_html(os.path.join(output_dir, \"3d_trajectory.html\"))\n",
    "    print(\"🎨 3D 궤적 HTML 저장 완료\")\n",
    "\n",
    "if 'landscape_fig' in locals():\n",
    "    landscape_fig.write_html(os.path.join(output_dir, \"loss_landscape.html\"))\n",
    "    print(\"🗺️ Loss Landscape HTML 저장 완료\")\n",
    "\n",
    "# 최종 분석 리포트\n",
    "print(f\"\"\"\n",
    "🎯 Gradient Descent 분석 결과:\n",
    "\n",
    "📈 훈련 개요:\n",
    "- 총 훈련 스텝: {len(visualizer.training_history['losses'])}\n",
    "- 최종 Loss: {visualizer.training_history['losses'][-1]:.6f}\n",
    "- 초기 Loss: {visualizer.training_history['losses'][0]:.6f}\n",
    "- Loss 감소율: {(visualizer.training_history['losses'][0] - visualizer.training_history['losses'][-1]) / visualizer.training_history['losses'][0] * 100:.2f}%\n",
    "\n",
    "🔍 Gradient 분석:\n",
    "- 평균 Gradient Norm: {np.mean(visualizer.training_history['gradient_norms']):.6f}\n",
    "- 최대 Gradient Norm: {np.max(visualizer.training_history['gradient_norms']):.6f}\n",
    "- 최소 Gradient Norm: {np.min(visualizer.training_history['gradient_norms']):.6f}\n",
    "\n",
    "⚙️ 파라미터 분석:\n",
    "- 평균 Parameter Norm: {np.mean(visualizer.training_history['parameter_norms']):.6f}\n",
    "- 파라미터 변화량: {abs(visualizer.training_history['parameter_norms'][-1] - visualizer.training_history['parameter_norms'][0]):.6f}\n",
    "\n",
    "💾 저장된 파일:\n",
    "- CSV 데이터: {output_path}\n",
    "- HTML 시각화: {output_dir}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c29ec",
   "metadata": {},
   "source": [
    "## 🎉 결론 및 인사이트\n",
    "\n",
    "### 📊 주요 발견사항:\n",
    "\n",
    "1. **Loss Landscape**: 3D 시각화를 통해 모델이 최적화되는 과정을 직관적으로 확인\n",
    "2. **Gradient Descent Path**: 파라미터 공간에서의 최적화 경로를 3차원으로 추적\n",
    "3. **수렴 패턴**: 다양한 하이퍼파라미터 설정에 따른 수렴 속도와 안정성 비교\n",
    "\n",
    "### 🔍 활용 방안:\n",
    "\n",
    "- **하이퍼파라미터 튜닝**: Loss landscape를 통한 최적 학습률 찾기\n",
    "- **모델 안정성 분석**: Gradient norm 변화를 통한 훈련 안정성 평가  \n",
    "- **수렴 모니터링**: 실시간 3D 시각화를 통한 훈련 과정 모니터링\n",
    "\n",
    "### 📈 향후 개선점:\n",
    "\n",
    "- 더 긴 훈련 과정에서의 long-term 패턴 분석\n",
    "- 다양한 데이터셋에 대한 일반화 성능 비교\n",
    "- Interactive dashboard를 통한 실시간 하이퍼파라미터 조정\n",
    "\n",
    "이 노트북을 통해 fine-tuning 과정의 gradient descent를 3D로 시각화하여 모델 최적화 과정을 더 잘 이해할 수 있습니다! 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
