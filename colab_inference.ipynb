{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af584fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 최적화 설정 (반드시 최상단에 실행)\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(\"⚙️ 메모리 최적화 설정 완료\")\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True,max_split_size_mb:512\")\n",
    "print(\"TOKENIZERS_PARALLELISM: false\")\n",
    "print(\"CUDA_LAUNCH_BLOCKING: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b977",
   "metadata": {},
   "source": [
    "# 🔍 한국어 텍스트 교정 모델 추론 테스트 (Google Colab)\n",
    "\n",
    "훈련된 한국어 텍스트 교정 모델을 사용한 추론 및 성능 평가\n",
    "- Google Drive에서 모델 로드\n",
    "- 대화형 텍스트 교정 인터페이스\n",
    "- 배치 텍스트 교정\n",
    "- 성능 벤치마크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffcf41",
   "metadata": {},
   "source": [
    "## 🚀 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a21798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "print(\"\\nPyTorch CUDA 지원 여부:\", torch.cuda.is_available() if 'torch' in globals() else 'torch not imported yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치 (양자화 지원 포함)\n",
    "!pip install -q transformers datasets peft accelerate\n",
    "!pip install -q gradio sentencepiece protobuf\n",
    "!pip install -q evaluate rouge-score sacrebleu\n",
    "!pip install -q bitsandbytes  # 양자화를 위한 패키지\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"✅ 패키지 설치 완료 (양자화 지원 포함)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee60544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 효율성을 위한 환경 변수 설정\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from typing import List, Dict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    BitsAndBytesConfig,  # 양자화 지원\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "\n",
    "# GPU 메모리 정리 함수\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"GPU 메모리 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"GPU 메모리 사용량 모니터링\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used_percent = (allocated / total) * 100\n",
    "        \n",
    "        print(f\"🔍 GPU 메모리 상태:\")\n",
    "        print(f\"  - 할당됨: {allocated:.2f}GB ({used_percent:.1f}%)\")\n",
    "        print(f\"  - 예약됨: {reserved:.2f}GB\")\n",
    "        print(f\"  - 사용가능: {free:.2f}GB\")\n",
    "        print(f\"  - 전체: {total:.2f}GB\")\n",
    "        \n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'free_gb': free,\n",
    "            'total_gb': total,\n",
    "            'used_percent': used_percent\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"완전한 메모리 정리\"\"\"\n",
    "    # 가능한 모든 모델 변수 삭제\n",
    "    for var_name in ['model', 'base_model', 'tokenizer']:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    print(\"🧹 메모리 정리 완료\")\n",
    "    \n",
    "# 초기 메모리 정리\n",
    "clear_gpu_memory()\n",
    "\n",
    "# GPU 정보 출력\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    # 메모리 사용량 출력\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "    print(f\"할당된 GPU 메모리: {allocated:.2f}GB\")\n",
    "    print(f\"예약된 GPU 메모리: {reserved:.2f}GB\")\n",
    "    print(f\"사용 가능한 GPU 메모리: {free:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb2511",
   "metadata": {},
   "source": [
    "## 📦 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 경로 설정 (Google Drive에서)\n",
    "MODEL_PATH = \"/content/drive/MyDrive/korean-text-correction-model\"\n",
    "BASE_MODEL = \"google/mt5-small\"\n",
    "\n",
    "# 모델 파일 존재 확인\n",
    "print(f\"📁 모델 경로: {MODEL_PATH}\")\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"✅ 모델 디렉토리가 존재합니다.\")\n",
    "    print(\"📋 모델 파일들:\")\n",
    "    !ls -la {MODEL_PATH}\n",
    "else:\n",
    "    print(\"❌ 모델 디렉토리를 찾을 수 없습니다.\")\n",
    "    print(\"먼저 colab_training.ipynb를 실행하여 모델을 훈련해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2422a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화 설정\n",
    "print(\"⚙️ 양자화 설정 준비...\")\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit 양자화 설정 (메모리 사용량 ~75% 감소)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"📊 양자화 설정:\")\n",
    "print(f\"- 4-bit 양자화: {quantization_config.load_in_4bit}\")\n",
    "print(f\"- 계산 타입: {quantization_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"- 양자화 타입: {quantization_config.bnb_4bit_quant_type}\")\n",
    "print(f\"- 이중 양자화: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"\\n🔤 토크나이저 로딩...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "\n",
    "# 베이스 모델을 4-bit 양자화로 로드\n",
    "print(\"🤖 베이스 모델 로딩 (4-bit 양자화)...\")\n",
    "try:\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 양자화된 베이스 모델 로드 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 양자화 로드 실패, 기본 모드로 fallback: {e}\")\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "# PEFT 모델 로드\n",
    "print(\"🔧 PEFT 어댑터 로딩...\")\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 메모리 사용량 출력\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\n📊 모델 로드 후 메모리 사용량:\")\n",
    "    print(f\"- 할당된 GPU 메모리: {allocated:.2f}GB\")\n",
    "    print(f\"- 예약된 GPU 메모리: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"\\n✅ 모델 로드 완료!\")\n",
    "print(f\"모델 파라미터 수: {model.num_parameters():,}\")\n",
    "\n",
    "# 메모리 정리\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0397949",
   "metadata": {},
   "source": [
    "## 🔍 추론 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextCorrector:\n",
    "    \"\"\"GPU 메모리 최적화된 한국어 텍스트 교정 클래스 (양자화 지원)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device, max_length=256):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 메모리 최적화를 위한 설정\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # 모델을 eval 모드로 설정\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 양자화 상태 확인\n",
    "        is_quantized = hasattr(self.model, 'hf_quantizer') or any(\n",
    "            hasattr(getattr(self.model, name), 'weight') and \n",
    "            getattr(getattr(self.model, name), 'weight').dtype == torch.uint8\n",
    "            for name, _ in self.model.named_modules()\n",
    "            if hasattr(getattr(self.model, name), 'weight')\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ KoreanTextCorrector 초기화 완료\")\n",
    "        print(f\"- 디바이스: {device}\")\n",
    "        print(f\"- 최대 길이: {max_length}\")\n",
    "        print(f\"- 양자화 상태: {'활성화' if is_quantized else '비활성화'}\")\n",
    "        \n",
    "        # 초기 메모리 상태 출력\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    def correct_text(self, text: str, **generation_kwargs) -> str:\n",
    "        \"\"\"단일 텍스트 교정 (메모리 최적화)\"\"\"\n",
    "        if not text.strip():\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # 메모리 정리\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # 입력 텍스트 전처리\n",
    "            input_text = f\"교정: {text.strip()}\"\n",
    "            \n",
    "            # 토크나이제이션 (메모리 효율적)\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=False  # 단일 텍스트이므로 padding 비활성화\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # 생성 설정 (메모리 효율적)\n",
    "            default_kwargs = {\n",
    "                \"max_length\": self.max_length,\n",
    "                \"num_beams\": 2,  # beam 수 감소로 메모리 절약\n",
    "                \"temperature\": 0.7,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"early_stopping\": True,  # 조기 종료로 메모리 절약\n",
    "                \"use_cache\": False  # 캐시 비활성화로 메모리 절약\n",
    "            }\n",
    "            default_kwargs.update(generation_kwargs)\n",
    "            \n",
    "            # 텍스트 생성\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **default_kwargs)\n",
    "            \n",
    "            # 결과 디코딩\n",
    "            corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del inputs, outputs\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            return corrected.strip()\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"⚠️ GPU 메모리 부족: {e}\")\n",
    "            clear_gpu_memory()\n",
    "            # 더 작은 beam size로 재시도\n",
    "            return self.correct_text_fallback(text)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 추론 오류: {e}\")\n",
    "            clear_gpu_memory()\n",
    "            return text\n",
    "    \n",
    "    def correct_text_fallback(self, text: str) -> str:\n",
    "        \"\"\"메모리 부족 시 폴백 방법 (양자화 환경 최적화)\"\"\"\n",
    "        try:\n",
    "            print(\"🔄 메모리 절약 모드로 재시도...\")\n",
    "            \n",
    "            # 더 작은 최대 길이로 설정\n",
    "            fallback_max_length = min(96, self.max_length // 2)\n",
    "            \n",
    "            input_text = f\"교정: {text.strip()[:200]}\"  # 입력 텍스트도 줄임\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=fallback_max_length,\n",
    "                truncation=True,\n",
    "                padding=False\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # 최소한의 설정으로 생성 (양자화 모델에 최적화)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=fallback_max_length,\n",
    "                    num_beams=1,  # greedy decoding만 사용\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=False,\n",
    "                    return_dict_in_generate=False  # 딕셔너리 반환 비활성화\n",
    "                )\n",
    "            \n",
    "            corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # 즉시 메모리 정리\n",
    "            del inputs, outputs\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            print(\"✅ 메모리 절약 모드 성공\")\n",
    "            return corrected.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 폴백 방법도 실패: {e}\")\n",
    "            print(\"🔄 CPU 모드로 최종 시도...\")\n",
    "            \n",
    "            try:\n",
    "                # CPU로 이동하여 최종 시도\n",
    "                cpu_inputs = self.tokenizer(\n",
    "                    f\"교정: {text.strip()[:100]}\",\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=64,\n",
    "                    truncation=True,\n",
    "                    padding=False\n",
    "                )\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    cpu_outputs = self.model.cpu().generate(\n",
    "                        **cpu_inputs,\n",
    "                        max_length=64,\n",
    "                        num_beams=1,\n",
    "                        do_sample=False,\n",
    "                        use_cache=False\n",
    "                    )\n",
    "                \n",
    "                result = self.tokenizer.decode(cpu_outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # 모델을 다시 GPU로 이동\n",
    "                self.model.to(self.device)\n",
    "                clear_gpu_memory()\n",
    "                \n",
    "                return result.strip()\n",
    "                \n",
    "            except Exception as cpu_e:\n",
    "                print(f\"❌ CPU 모드도 실패: {cpu_e}\")\n",
    "                clear_gpu_memory()\n",
    "                return text\n",
    "    \n",
    "    def correct_batch(self, texts: List[str], batch_size: int = 1, **generation_kwargs) -> List[str]:\n",
    "        \"\"\"배치 텍스트 교정 (양자화 환경 최적화)\"\"\"\n",
    "        results = []\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"📝 배치 처리 시작: {len(texts)}개 텍스트, 배치 크기: {batch_size}\")\n",
    "        \n",
    "        # 양자화 모델에서는 배치 크기를 더 작게 설정\n",
    "        safe_batch_size = min(batch_size, 2)  # 양자화 환경에서는 최대 2개\n",
    "        \n",
    "        for i in range(0, len(texts), safe_batch_size):\n",
    "            batch_texts = texts[i:i + safe_batch_size]\n",
    "            batch_num = i // safe_batch_size + 1\n",
    "            \n",
    "            print(f\"🔄 배치 {batch_num}/{total_batches} 처리 중...\")\n",
    "            \n",
    "            # 배치 시작 전 메모리 상태 확인\n",
    "            if torch.cuda.is_available():\n",
    "                memory_info = monitor_gpu_memory()\n",
    "                if memory_info and memory_info['used_percent'] > 85:\n",
    "                    print(\"⚠️ 메모리 사용량이 높습니다. 정리 중...\")\n",
    "                    clear_gpu_memory()\n",
    "            \n",
    "            # 각 텍스트를 개별적으로 처리 (메모리 안전)\n",
    "            batch_results = []\n",
    "            for j, text in enumerate(batch_texts):\n",
    "                try:\n",
    "                    print(f\"  - 텍스트 {j+1}/{len(batch_texts)} 처리 중...\")\n",
    "                    corrected = self.correct_text(text, **generation_kwargs)\n",
    "                    batch_results.append(corrected)\n",
    "                    \n",
    "                    # 각 텍스트 처리 후 즉시 메모리 정리\n",
    "                    clear_gpu_memory()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ 텍스트 처리 실패: {e}\")\n",
    "                    batch_results.append(text)  # 원본 텍스트 반환\n",
    "                    clear_gpu_memory()\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # 배치 완료 후 메모리 정리\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            print(f\"✅ 배치 {batch_num} 완료 ({len(batch_results)}개 처리)\")\n",
    "        \n",
    "        print(f\"🎉 전체 배치 처리 완료: {len(results)}개 결과\")\n",
    "        return results\n",
    "    \n",
    "    def benchmark_correction(self, text: str, num_runs: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"양자화 모델의 메모리 효율적인 성능 벤치마크\"\"\"\n",
    "        print(f\"🔬 벤치마크 시작: {num_runs}회 실행\")\n",
    "        \n",
    "        times = []\n",
    "        memory_usage = []\n",
    "        corrected_text = \"\"\n",
    "        \n",
    "        # 초기 메모리 상태\n",
    "        initial_memory = monitor_gpu_memory()\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            print(f\"  실행 {i+1}/{num_runs}...\")\n",
    "            \n",
    "            # 실행 전 메모리 정리\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # 메모리 측정 시작\n",
    "            start_memory = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # 시간 측정\n",
    "            start_time = time.time()\n",
    "            corrected_text = self.correct_text(text)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # 메모리 측정 종료\n",
    "            end_memory = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "            memory_used = end_memory - start_memory\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            memory_usage.append(memory_used)\n",
    "            \n",
    "            # 각 실행 후 메모리 정리\n",
    "            clear_gpu_memory()\n",
    "        \n",
    "        # 최종 메모리 상태\n",
    "        final_memory = monitor_gpu_memory()\n",
    "        \n",
    "        result = {\n",
    "            \"original\": text,\n",
    "            \"corrected\": corrected_text,\n",
    "            \"performance\": {\n",
    "                \"avg_time\": np.mean(times),\n",
    "                \"min_time\": np.min(times),\n",
    "                \"max_time\": np.max(times),\n",
    "                \"std_time\": np.std(times),\n",
    "                \"avg_memory_mb\": np.mean(memory_usage) * 1024,\n",
    "                \"max_memory_mb\": np.max(memory_usage) * 1024\n",
    "            },\n",
    "            \"memory_info\": {\n",
    "                \"initial_allocated_gb\": initial_memory['allocated_gb'] if initial_memory else 0,\n",
    "                \"final_allocated_gb\": final_memory['allocated_gb'] if final_memory else 0,\n",
    "                \"memory_efficiency\": \"양자화된 모델 사용\" if hasattr(self.model, 'hf_quantizer') else \"일반 모델\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 벤치마크 완료:\")\n",
    "        print(f\"  - 평균 시간: {result['performance']['avg_time']:.3f}초\")\n",
    "        print(f\"  - 평균 메모리 사용: {result['performance']['avg_memory_mb']:.1f}MB\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ 메모리 최적화된 KoreanTextCorrector 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f457f1",
   "metadata": {},
   "source": [
    "## 🚀 양자화 모델 추론기 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ec5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화된 모델로 추론기 생성\n",
    "print(\"🤖 양자화 텍스트 교정기 생성 중...\")\n",
    "\n",
    "# 모델과 토크나이저가 로드되었는지 확인\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"❌ 모델이 로드되지 않았습니다. 먼저 위의 모델 로드 셀을 실행해주세요.\")\n",
    "else:\n",
    "    # 추론기 생성 전 메모리 상태 확인\n",
    "    print(\"\\n📊 추론기 생성 전 메모리 상태:\")\n",
    "    pre_corrector_memory = monitor_gpu_memory()\n",
    "    \n",
    "    # 텍스트 교정기 생성\n",
    "    corrector = KoreanTextCorrector(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=256  # 양자화 환경에서 적절한 길이\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📊 추론기 생성 후 메모리 상태:\")\n",
    "    post_corrector_memory = monitor_gpu_memory()\n",
    "    \n",
    "    # 메모리 절약 효과 계산\n",
    "    if pre_corrector_memory and post_corrector_memory:\n",
    "        memory_increase = post_corrector_memory['allocated_gb'] - pre_corrector_memory['allocated_gb']\n",
    "        print(f\"\\n💾 추론기 생성으로 인한 메모리 증가: {memory_increase:.3f}GB\")\n",
    "    \n",
    "    print(\"\\n🎉 양자화 텍스트 교정기 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d769d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 상태 검증\n",
    "print(\"🔍 양자화 상태 검증\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 모델의 양자화 상태 확인\n",
    "try:\n",
    "    # BitsAndBytesConfig 사용 여부 확인\n",
    "    is_quantized = False\n",
    "    quantization_info = {}\n",
    "    \n",
    "    if hasattr(model, 'hf_quantizer'):\n",
    "        is_quantized = True\n",
    "        quantization_info['quantizer_type'] = type(model.hf_quantizer).__name__\n",
    "        if hasattr(model.hf_quantizer, 'quantization_config'):\n",
    "            config = model.hf_quantizer.quantization_config\n",
    "            quantization_info['load_in_4bit'] = getattr(config, 'load_in_4bit', False)\n",
    "            quantization_info['compute_dtype'] = getattr(config, 'bnb_4bit_compute_dtype', None)\n",
    "            quantization_info['quant_type'] = getattr(config, 'bnb_4bit_quant_type', None)\n",
    "    \n",
    "    # 레이어별 양자화 상태 확인\n",
    "    quantized_layers = 0\n",
    "    total_layers = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            total_layers += 1\n",
    "            if hasattr(module.weight, 'dtype'):\n",
    "                if module.weight.dtype in [torch.uint8, torch.int8]:\n",
    "                    quantized_layers += 1\n",
    "    \n",
    "    print(f\"✅ 양자화 상태: {'활성화' if is_quantized else '비활성화'}\")\n",
    "    if quantization_info:\n",
    "        print(f\"📊 양자화 정보:\")\n",
    "        for key, value in quantization_info.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(f\"🔢 양자화된 레이어: {quantized_layers}/{total_layers}\")\n",
    "    \n",
    "    if quantized_layers > 0:\n",
    "        quantization_ratio = (quantized_layers / total_layers) * 100\n",
    "        print(f\"📈 양자화 비율: {quantization_ratio:.1f}%\")\n",
    "        \n",
    "        # 예상 메모리 절약량 계산\n",
    "        memory_saving = quantization_ratio * 0.75  # 대략적인 절약량\n",
    "        print(f\"💾 예상 메모리 절약: ~{memory_saving:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 양자화 상태 확인 중 오류: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d347ff",
   "metadata": {},
   "source": [
    "## 🧪 기본 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 테스트 케이스\n",
    "test_cases = [\n",
    "    \"안녕하셰요\",              # 안녕하세요\n",
    "    \"감사햐니다\",              # 감사합니다\n",
    "    \"잘 뫃겠습니다\",           # 잘 부탁드립니다\n",
    "    \"괜챠습니까\",              # 괜찮습니까\n",
    "    \"이거 어떻게 생간하세요\",   # 이거 어떻게 생각하세요\n",
    "    \"오늘 날시가 좋네요\",       # 오늘 날씨가 좋네요\n",
    "    \"회의는 몇시에 시작해나요\",  # 회의는 몇시에 시작합니까\n",
    "    \"점심 메뉴는 뭐가 조을까요\" # 점심 메뉴는 뭐가 좋을까요\n",
    "]\n",
    "\n",
    "print(\"🧪 기본 텍스트 교정 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_text in enumerate(test_cases, 1):\n",
    "    start_time = time.time()\n",
    "    corrected = corrector.correct_text(test_text)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"{i:2d}. 원본: {test_text}\")\n",
    "    print(f\"    교정: {corrected}\")\n",
    "    print(f\"    시간: {(end_time - start_time)*1000:.1f}ms\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a91c6",
   "metadata": {},
   "source": [
    "## ⚡ 성능 벤치마크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 벤치마크\n",
    "benchmark_texts = [\n",
    "    \"안녕하셰요\",\n",
    "    \"이거 정말 맛있게 생겼네요. 어디서 살 수 있나요?\",\n",
    "    \"회의는 내일 오전 10시에 시작할 예정입니다. 참석 가능하신지 확인 부탁드려요.\"\n",
    "]\n",
    "\n",
    "print(\"⚡ 성능 벤치마크 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(benchmark_texts, 1):\n",
    "    print(f\"\\n{i}. 벤치마크 텍스트: {text[:30]}...\")\n",
    "    \n",
    "    benchmark_result = corrector.benchmark_correction(text, num_runs=5)\n",
    "    \n",
    "    print(f\"   원본: {benchmark_result['original']}\")\n",
    "    print(f\"   교정: {benchmark_result['corrected']}\")\n",
    "    print(f\"   평균 시간: {benchmark_result['performance']['avg_time']*1000:.1f}ms\")\n",
    "    print(f\"   최소 시간: {benchmark_result['performance']['min_time']*1000:.1f}ms\")\n",
    "    print(f\"   최대 시간: {benchmark_result['performance']['max_time']*1000:.1f}ms\")\n",
    "    print(f\"   표준편차: {benchmark_result['performance']['std_time']*1000:.1f}ms\")\n",
    "    print(f\"   평균 메모리: {benchmark_result['performance']['avg_memory_mb']:.1f}MB\")\n",
    "    print(f\"   최대 메모리: {benchmark_result['performance']['max_memory_mb']:.1f}MB\")\n",
    "    print(f\"   효율성: {benchmark_result['memory_info']['memory_efficiency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b45811",
   "metadata": {},
   "source": [
    "## 📊 배치 처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 처리 테스트\n",
    "batch_texts = [\n",
    "    \"안녕하셰요\",\n",
    "    \"감사햐니다\", \n",
    "    \"잘 뫃겠습니다\",\n",
    "    \"괜챠습니까\",\n",
    "    \"이거 어떻게 생간하세요\",\n",
    "    \"오늘 날시가 좋네요\",\n",
    "    \"회의는 몇시에 시작해나요\",\n",
    "    \"점심 메뉴는 뭐가 조을까요\",\n",
    "    \"이 문서를 검토해 주세요\",\n",
    "    \"프로젝트 진행 상황은 어떤가요\"\n",
    "]\n",
    "\n",
    "print(\"📊 배치 처리 테스트 (양자화 모델)\")\n",
    "print(f\"총 {len(batch_texts)}개 텍스트 처리\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 배치 처리 전 메모리 상태\n",
    "print(\"\\n🔍 배치 처리 전 메모리 상태:\")\n",
    "pre_batch_memory = monitor_gpu_memory()\n",
    "\n",
    "start_time = time.time()\n",
    "batch_results = corrector.correct_batch(batch_texts, batch_size=1)  # 양자화 환경에서 안전한 배치 크기\n",
    "end_time = time.time()\n",
    "\n",
    "# 배치 처리 후 메모리 상태\n",
    "print(\"\\n🔍 배치 처리 후 메모리 상태:\")\n",
    "post_batch_memory = monitor_gpu_memory()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_text = total_time / len(batch_texts)\n",
    "\n",
    "print(f\"\\n📈 배치 처리 결과:\")\n",
    "for i, (original, corrected) in enumerate(zip(batch_texts, batch_results), 1):\n",
    "    print(f\"{i:2d}. {original} → {corrected}\")\n",
    "\n",
    "print(f\"\\n⏱️ 처리 시간:\")\n",
    "print(f\"전체 시간: {total_time:.2f}초\")\n",
    "print(f\"평균 시간: {avg_time_per_text*1000:.1f}ms/텍스트\")\n",
    "print(f\"처리 속도: {len(batch_texts)/total_time:.1f}텍스트/초\")\n",
    "\n",
    "# 메모리 효율성 분석\n",
    "if pre_batch_memory and post_batch_memory:\n",
    "    memory_change = post_batch_memory['allocated_gb'] - pre_batch_memory['allocated_gb']\n",
    "    print(f\"\\n💾 메모리 효율성:\")\n",
    "    print(f\"메모리 변화: {memory_change:+.3f}GB\")\n",
    "    print(f\"최대 메모리 사용률: {max(pre_batch_memory['used_percent'], post_batch_memory['used_percent']):.1f}%\")\n",
    "    \n",
    "    if memory_change < 0.1:  # 100MB 미만 증가\n",
    "        print(\"✅ 메모리 효율적 처리 성공!\")\n",
    "    else:\n",
    "        print(\"⚠️ 메모리 사용량 증가 감지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667015a",
   "metadata": {},
   "source": [
    "## 🖥️ 대화형 인터페이스 (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_correct_text(text, num_beams, temperature, do_sample):\n",
    "    \"\"\"Gradio용 텍스트 교정 함수\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"텍스트를 입력해주세요.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        corrected = corrector.correct_text(\n",
    "            text,\n",
    "            num_beams=int(num_beams),\n",
    "            temperature=float(temperature),\n",
    "            do_sample=bool(do_sample)\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = f\"처리 시간: {(end_time - start_time)*1000:.1f}ms\"\n",
    "        \n",
    "        return corrected, processing_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"오류 발생: {str(e)}\", \"\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "with gr.Blocks(title=\"한국어 텍스트 교정기\") as demo:\n",
    "    gr.Markdown(\"# 🇰🇷 한국어 텍스트 교정기\")\n",
    "    gr.Markdown(\"mT5 기반 한국어 텍스트 교정 모델로 맞춤법과 문법을 교정합니다.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(\n",
    "                label=\"교정할 텍스트\",\n",
    "                placeholder=\"교정하고 싶은 한국어 텍스트를 입력하세요...\",\n",
    "                lines=5\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                num_beams = gr.Slider(\n",
    "                    minimum=1, maximum=8, value=4, step=1,\n",
    "                    label=\"Beam Search 크기\"\n",
    "                )\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.1, maximum=2.0, value=0.7, step=0.1,\n",
    "                    label=\"Temperature\"\n",
    "                )\n",
    "                do_sample = gr.Checkbox(value=True, label=\"Sampling 사용\")\n",
    "            \n",
    "            correct_btn = gr.Button(\"교정하기\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(\n",
    "                label=\"교정된 텍스트\",\n",
    "                lines=5\n",
    "            )\n",
    "            processing_info = gr.Textbox(\n",
    "                label=\"처리 정보\",\n",
    "                lines=1\n",
    "            )\n",
    "    \n",
    "    # 예시 텍스트\n",
    "    gr.Markdown(\"### 📝 예시 텍스트\")\n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            [\"안녕하셰요\"],\n",
    "            [\"감사햐니다\"],\n",
    "            [\"잘 뫃겠습니다\"],\n",
    "            [\"이거 어떻게 생간하세요\"],\n",
    "            [\"오늘 날시가 정말 좋네요\"]\n",
    "        ],\n",
    "        inputs=[input_text]\n",
    "    )\n",
    "    \n",
    "    # 이벤트 연결\n",
    "    correct_btn.click(\n",
    "        fn=gradio_correct_text,\n",
    "        inputs=[input_text, num_beams, temperature, do_sample],\n",
    "        outputs=[output_text, processing_info]\n",
    "    )\n",
    "\n",
    "# 인터페이스 실행\n",
    "print(\"🖥️ Gradio 인터페이스를 시작합니다...\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c3188",
   "metadata": {},
   "source": [
    "## 📈 모델 평가 (선택사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e081f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가용 데이터 로드 (Google Drive에서)\n",
    "def evaluate_on_test_data():\n",
    "    \"\"\"테스트 데이터로 모델 평가\"\"\"\n",
    "    \n",
    "    # 테스트 데이터 파일 경로 (예시)\n",
    "    test_file_path = \"/content/drive/MyDrive/구어체_대화체_16878_sample_난독화결과.csv\"\n",
    "    \n",
    "    if not os.path.exists(test_file_path):\n",
    "        print(\"❌ 테스트 데이터 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(\"📊 테스트 데이터로 모델 평가 중...\")\n",
    "    \n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(test_file_path, encoding='utf-8')\n",
    "    \n",
    "    # 작은 샘플로 테스트\n",
    "    test_df = df.sample(n=50, random_state=42)\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"총 {len(test_df)}개 샘플 평가 중...\")\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        original = row['original']\n",
    "        error_text = row['obfuscated']\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted = corrector.correct_text(error_text)\n",
    "        \n",
    "        predictions.append(predicted)\n",
    "        references.append(original)\n",
    "        \n",
    "        if len(predictions) % 10 == 0:\n",
    "            print(f\"진행률: {len(predictions)}/{len(test_df)}\")\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_scores = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📈 평가 결과:\")\n",
    "    print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "    \n",
    "    # 몇 가지 예시 출력\n",
    "    print(\"\\n📝 예시 결과:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"\\n{i+1}.\")\n",
    "        print(f\"오류문: {test_df.iloc[i]['obfuscated']}\")\n",
    "        print(f\"정답: {references[i]}\")\n",
    "        print(f\"예측: {predictions[i]}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# 평가 실행 (선택사항)\n",
    "# evaluate_on_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cee4e",
   "metadata": {},
   "source": [
    "## 🧹 정리\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 양자화 모델 최종 테스트\n",
    "\n",
    "print(\"🔬 양자화 모델 최종 성능 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. 단일 텍스트 교정 테스트\n",
    "print(\"\\n1️⃣ 단일 텍스트 교정 테스트:\")\n",
    "test_text = \"안녕하셰요! 오늘 날시가 정말 좋네요.\"\n",
    "result = corrector.correct_text(test_text)\n",
    "print(f\"원본: {test_text}\")\n",
    "print(f\"교정: {result}\")\n",
    "\n",
    "# 2. 메모리 효율성 테스트\n",
    "print(\"\\n2️⃣ 메모리 효율성 테스트:\")\n",
    "pre_memory = monitor_gpu_memory()\n",
    "\n",
    "# 여러 번의 추론으로 메모리 누수 확인\n",
    "for i in range(5):\n",
    "    _ = corrector.correct_text(f\"테스트 {i+1}: 안녕하셰요\")\n",
    "    \n",
    "print(\"\\n5회 추론 후 메모리 상태:\")\n",
    "post_memory = monitor_gpu_memory()\n",
    "\n",
    "if pre_memory and post_memory:\n",
    "    memory_change = post_memory['allocated_gb'] - pre_memory['allocated_gb']\n",
    "    print(f\"\\n메모리 변화: {memory_change:+.3f}GB\")\n",
    "    if abs(memory_change) < 0.1:\n",
    "        print(\"✅ 메모리 누수 없음 - 안정적!\")\n",
    "    else:\n",
    "        print(\"⚠️ 메모리 사용량 변화 감지\")\n",
    "\n",
    "# 3. 양자화 상태 최종 확인\n",
    "print(\"\\n3️⃣ 양자화 상태 최종 확인:\")\n",
    "is_quantized = hasattr(model, 'hf_quantizer')\n",
    "print(f\"양자화 활성화: {'✅ 예' if is_quantized else '❌ 아니오'}\")\n",
    "\n",
    "if is_quantized:\n",
    "    print(\"🎉 4-bit 양자화가 성공적으로 적용되었습니다!\")\n",
    "    print(\"💾 예상 메모리 절약: ~75%\")\n",
    "    print(\"⚡ GPU 메모리 부족 문제가 해결되었습니다!\")\n",
    "else:\n",
    "    print(\"⚠️ 양자화가 적용되지 않았습니다.\")\n",
    "    print(\"🔧 Fallback 모드에서 실행 중입니다.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎊 양자화 기반 한국어 텍스트 교정 시스템 준비 완료!\")\n",
    "print(\"이제 메모리 부족 없이 안전하게 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717b481",
   "metadata": {},
   "source": [
    "## 📖 사용 가이드\n",
    "\n",
    "### 🚀 빠른 시작:\n",
    "\n",
    "**1. 단일 텍스트 교정:**\n",
    "```python\n",
    "result = corrector.correct_text(\"안녕하셰요\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**2. 배치 텍스트 교정:**\n",
    "```python\n",
    "texts = [\"안녕하셰요\", \"감사햐니다\", \"잘 뫃겠습니다\"]\n",
    "results = corrector.correct_batch(texts, batch_size=1)\n",
    "for original, corrected in zip(texts, results):\n",
    "    print(f\"{original} → {corrected}\")\n",
    "```\n",
    "\n",
    "**3. 성능 벤치마크:**\n",
    "```python\n",
    "benchmark = corrector.benchmark_correction(\"테스트 텍스트\", num_runs=3)\n",
    "print(f\"평균 시간: {benchmark['performance']['avg_time']*1000:.1f}ms\")\n",
    "```\n",
    "\n",
    "**4. 메모리 모니터링:**\n",
    "```python\n",
    "monitor_gpu_memory()  # 현재 메모리 상태 확인\n",
    "clear_gpu_memory()    # 메모리 정리\n",
    "```\n",
    "\n",
    "### 🔧 문제 해결:\n",
    "\n",
    "**메모리 부족 시:**\n",
    "1. `clear_gpu_memory()` 실행\n",
    "2. 배치 크기를 1로 감소\n",
    "3. 텍스트 길이 축소 (256자 이하 권장)\n",
    "\n",
    "**성능 향상 팁:**\n",
    "- Beam search 크기: 1-2 (기본값: 2)\n",
    "- Temperature: 0.7 (기본값)\n",
    "- 배치 크기: 1 (양자화 환경 권장)\n",
    "\n",
    "### 🎯 양자화 효과:\n",
    "- ✅ GPU 메모리 사용량 ~75% 감소\n",
    "- ✅ OutOfMemoryError 해결\n",
    "- ✅ 안정적인 장시간 사용 가능\n",
    "- ⚠️ 약간의 품질 저하 가능 (일반적으로 미미함)\n",
    "\n",
    "이제 **메모리 효율적인 한국어 텍스트 교정**을 즐기세요! 🎉\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
