{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af584fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ë°˜ë“œì‹œ ìµœìƒë‹¨ì— ì‹¤í–‰)\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(\"âš™ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True,max_split_size_mb:512\")\n",
    "print(\"TOKENIZERS_PARALLELISM: false\")\n",
    "print(\"CUDA_LAUNCH_BLOCKING: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7b977",
   "metadata": {},
   "source": [
    "# ğŸ” í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Google Colab)\n",
    "\n",
    "í›ˆë ¨ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡  ë° ì„±ëŠ¥ í‰ê°€\n",
    "- Google Driveì—ì„œ ëª¨ë¸ ë¡œë“œ\n",
    "- ëŒ€í™”í˜• í…ìŠ¤íŠ¸ êµì • ì¸í„°í˜ì´ìŠ¤\n",
    "- ë°°ì¹˜ í…ìŠ¤íŠ¸ êµì •\n",
    "- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffcf41",
   "metadata": {},
   "source": [
    "## ğŸš€ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a21798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi\n",
    "print(\"\\nPyTorch CUDA ì§€ì› ì—¬ë¶€:\", torch.cuda.is_available() if 'torch' in globals() else 'torch not imported yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "print(f\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì–‘ìí™” ì§€ì› í¬í•¨)\n",
    "!pip install -q transformers datasets peft accelerate\n",
    "!pip install -q gradio sentencepiece protobuf\n",
    "!pip install -q evaluate rouge-score sacrebleu\n",
    "!pip install -q bitsandbytes  # ì–‘ìí™”ë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ (ì–‘ìí™” ì§€ì› í¬í•¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee60544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from typing import List, Dict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    BitsAndBytesConfig,  # ì–‘ìí™” ì§€ì›\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used_percent = (allocated / total) * 100\n",
    "        \n",
    "        print(f\"ğŸ” GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "        print(f\"  - í• ë‹¹ë¨: {allocated:.2f}GB ({used_percent:.1f}%)\")\n",
    "        print(f\"  - ì˜ˆì•½ë¨: {reserved:.2f}GB\")\n",
    "        print(f\"  - ì‚¬ìš©ê°€ëŠ¥: {free:.2f}GB\")\n",
    "        print(f\"  - ì „ì²´: {total:.2f}GB\")\n",
    "        \n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'free_gb': free,\n",
    "            'total_gb': total,\n",
    "            'used_percent': used_percent\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"ì™„ì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "    # ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë³€ìˆ˜ ì‚­ì œ\n",
    "    for var_name in ['model', 'base_model', 'tokenizer']:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    print(\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "# ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()\n",
    "\n",
    "# GPU ì •ë³´ ì¶œë ¥\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "    print(f\"í• ë‹¹ëœ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB\")\n",
    "    print(f\"ì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬: {reserved:.2f}GB\")\n",
    "    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {free:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb2511",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (Google Driveì—ì„œ)\n",
    "MODEL_PATH = \"/content/drive/MyDrive/korean-text-correction-model\"\n",
    "BASE_MODEL = \"google/mt5-small\"\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "print(f\"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}\")\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ“‹ ëª¨ë¸ íŒŒì¼ë“¤:\")\n",
    "    !ls -la {MODEL_PATH}\n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ë¨¼ì € colab_training.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2422a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit ì–‘ìí™” ì„¤ì •\n",
    "print(\"âš™ï¸ ì–‘ìí™” ì„¤ì • ì¤€ë¹„...\")\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~75% ê°ì†Œ)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š ì–‘ìí™” ì„¤ì •:\")\n",
    "print(f\"- 4-bit ì–‘ìí™”: {quantization_config.load_in_4bit}\")\n",
    "print(f\"- ê³„ì‚° íƒ€ì…: {quantization_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"- ì–‘ìí™” íƒ€ì…: {quantization_config.bnb_4bit_quant_type}\")\n",
    "print(f\"- ì´ì¤‘ ì–‘ìí™”: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"\\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”©...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ì„ 4-bit ì–‘ìí™”ë¡œ ë¡œë“œ\n",
    "print(\"ğŸ¤– ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (4-bit ì–‘ìí™”)...\")\n",
    "try:\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"âœ… ì–‘ìí™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì–‘ìí™” ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ fallback: {e}\")\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "# PEFT ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ”§ PEFT ì–´ëŒ‘í„° ë¡œë”©...\")\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "model.eval()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nğŸ“Š ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\n",
    "    print(f\"- í• ë‹¹ëœ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB\")\n",
    "    print(f\"- ì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0397949",
   "metadata": {},
   "source": [
    "## ğŸ” ì¶”ë¡  í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextCorrector:\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ìµœì í™”ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • í´ë˜ìŠ¤ (ì–‘ìí™” ì§€ì›)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device, max_length=256):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •\n",
    "        self.model.eval()\n",
    "        \n",
    "        # ì–‘ìí™” ìƒíƒœ í™•ì¸\n",
    "        is_quantized = hasattr(self.model, 'hf_quantizer') or any(\n",
    "            hasattr(getattr(self.model, name), 'weight') and \n",
    "            getattr(getattr(self.model, name), 'weight').dtype == torch.uint8\n",
    "            for name, _ in self.model.named_modules()\n",
    "            if hasattr(getattr(self.model, name), 'weight')\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… KoreanTextCorrector ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"- ë””ë°”ì´ìŠ¤: {device}\")\n",
    "        print(f\"- ìµœëŒ€ ê¸¸ì´: {max_length}\")\n",
    "        print(f\"- ì–‘ìí™” ìƒíƒœ: {'í™œì„±í™”' if is_quantized else 'ë¹„í™œì„±í™”'}\")\n",
    "        \n",
    "        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    def correct_text(self, text: str, **generation_kwargs) -> str:\n",
    "        \"\"\"ë‹¨ì¼ í…ìŠ¤íŠ¸ êµì • (ë©”ëª¨ë¦¬ ìµœì í™”)\"\"\"\n",
    "        if not text.strip():\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "            input_text = f\"êµì •: {text.strip()}\"\n",
    "            \n",
    "            # í† í¬ë‚˜ì´ì œì´ì…˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=False  # ë‹¨ì¼ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ padding ë¹„í™œì„±í™”\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # ìƒì„± ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "            default_kwargs = {\n",
    "                \"max_length\": self.max_length,\n",
    "                \"num_beams\": 2,  # beam ìˆ˜ ê°ì†Œë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "                \"temperature\": 0.7,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"early_stopping\": True,  # ì¡°ê¸° ì¢…ë£Œë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "                \"use_cache\": False  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "            }\n",
    "            default_kwargs.update(generation_kwargs)\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **default_kwargs)\n",
    "            \n",
    "            # ê²°ê³¼ ë””ì½”ë”©\n",
    "            corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del inputs, outputs\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            return corrected.strip()\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}\")\n",
    "            clear_gpu_memory()\n",
    "            # ë” ì‘ì€ beam sizeë¡œ ì¬ì‹œë„\n",
    "            return self.correct_text_fallback(text)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì¶”ë¡  ì˜¤ë¥˜: {e}\")\n",
    "            clear_gpu_memory()\n",
    "            return text\n",
    "    \n",
    "    def correct_text_fallback(self, text: str) -> str:\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í´ë°± ë°©ë²• (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ”„ ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œë¡œ ì¬ì‹œë„...\")\n",
    "            \n",
    "            # ë” ì‘ì€ ìµœëŒ€ ê¸¸ì´ë¡œ ì„¤ì •\n",
    "            fallback_max_length = min(96, self.max_length // 2)\n",
    "            \n",
    "            input_text = f\"êµì •: {text.strip()[:200]}\"  # ì…ë ¥ í…ìŠ¤íŠ¸ë„ ì¤„ì„\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=fallback_max_length,\n",
    "                truncation=True,\n",
    "                padding=False\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ìƒì„± (ì–‘ìí™” ëª¨ë¸ì— ìµœì í™”)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=fallback_max_length,\n",
    "                    num_beams=1,  # greedy decodingë§Œ ì‚¬ìš©\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=False,\n",
    "                    return_dict_in_generate=False  # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ë¹„í™œì„±í™”\n",
    "                )\n",
    "            \n",
    "            corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del inputs, outputs\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            print(\"âœ… ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ ì„±ê³µ\")\n",
    "            return corrected.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í´ë°± ë°©ë²•ë„ ì‹¤íŒ¨: {e}\")\n",
    "            print(\"ğŸ”„ CPU ëª¨ë“œë¡œ ìµœì¢… ì‹œë„...\")\n",
    "            \n",
    "            try:\n",
    "                # CPUë¡œ ì´ë™í•˜ì—¬ ìµœì¢… ì‹œë„\n",
    "                cpu_inputs = self.tokenizer(\n",
    "                    f\"êµì •: {text.strip()[:100]}\",\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=64,\n",
    "                    truncation=True,\n",
    "                    padding=False\n",
    "                )\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    cpu_outputs = self.model.cpu().generate(\n",
    "                        **cpu_inputs,\n",
    "                        max_length=64,\n",
    "                        num_beams=1,\n",
    "                        do_sample=False,\n",
    "                        use_cache=False\n",
    "                    )\n",
    "                \n",
    "                result = self.tokenizer.decode(cpu_outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # ëª¨ë¸ì„ ë‹¤ì‹œ GPUë¡œ ì´ë™\n",
    "                self.model.to(self.device)\n",
    "                clear_gpu_memory()\n",
    "                \n",
    "                return result.strip()\n",
    "                \n",
    "            except Exception as cpu_e:\n",
    "                print(f\"âŒ CPU ëª¨ë“œë„ ì‹¤íŒ¨: {cpu_e}\")\n",
    "                clear_gpu_memory()\n",
    "                return text\n",
    "    \n",
    "    def correct_batch(self, texts: List[str], batch_size: int = 1, **generation_kwargs) -> List[str]:\n",
    "        \"\"\"ë°°ì¹˜ í…ìŠ¤íŠ¸ êµì • (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\"\"\"\n",
    "        results = []\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"ğŸ“ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "        \n",
    "        # ì–‘ìí™” ëª¨ë¸ì—ì„œëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì‘ê²Œ ì„¤ì •\n",
    "        safe_batch_size = min(batch_size, 2)  # ì–‘ìí™” í™˜ê²½ì—ì„œëŠ” ìµœëŒ€ 2ê°œ\n",
    "        \n",
    "        for i in range(0, len(texts), safe_batch_size):\n",
    "            batch_texts = texts[i:i + safe_batch_size]\n",
    "            batch_num = i // safe_batch_size + 1\n",
    "            \n",
    "            print(f\"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...\")\n",
    "            \n",
    "            # ë°°ì¹˜ ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "            if torch.cuda.is_available():\n",
    "                memory_info = monitor_gpu_memory()\n",
    "                if memory_info and memory_info['used_percent'] > 85:\n",
    "                    print(\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì •ë¦¬ ì¤‘...\")\n",
    "                    clear_gpu_memory()\n",
    "            \n",
    "            # ê° í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)\n",
    "            batch_results = []\n",
    "            for j, text in enumerate(batch_texts):\n",
    "                try:\n",
    "                    print(f\"  - í…ìŠ¤íŠ¸ {j+1}/{len(batch_texts)} ì²˜ë¦¬ ì¤‘...\")\n",
    "                    corrected = self.correct_text(text, **generation_kwargs)\n",
    "                    batch_results.append(corrected)\n",
    "                    \n",
    "                    # ê° í…ìŠ¤íŠ¸ ì²˜ë¦¬ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "                    clear_gpu_memory()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                    batch_results.append(text)  # ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "                    clear_gpu_memory()\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            print(f\"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ ({len(batch_results)}ê°œ ì²˜ë¦¬)\")\n",
    "        \n",
    "        print(f\"ğŸ‰ ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼\")\n",
    "        return results\n",
    "    \n",
    "    def benchmark_correction(self, text: str, num_runs: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"ì–‘ìí™” ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\"\"\"\n",
    "        print(f\"ğŸ”¬ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {num_runs}íšŒ ì‹¤í–‰\")\n",
    "        \n",
    "        times = []\n",
    "        memory_usage = []\n",
    "        corrected_text = \"\"\n",
    "        \n",
    "        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        initial_memory = monitor_gpu_memory()\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            print(f\"  ì‹¤í–‰ {i+1}/{num_runs}...\")\n",
    "            \n",
    "            # ì‹¤í–‰ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘\n",
    "            start_memory = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # ì‹œê°„ ì¸¡ì •\n",
    "            start_time = time.time()\n",
    "            corrected_text = self.correct_text(text)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ\n",
    "            end_memory = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "            memory_used = end_memory - start_memory\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            memory_usage.append(memory_used)\n",
    "            \n",
    "            # ê° ì‹¤í–‰ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            clear_gpu_memory()\n",
    "        \n",
    "        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        final_memory = monitor_gpu_memory()\n",
    "        \n",
    "        result = {\n",
    "            \"original\": text,\n",
    "            \"corrected\": corrected_text,\n",
    "            \"performance\": {\n",
    "                \"avg_time\": np.mean(times),\n",
    "                \"min_time\": np.min(times),\n",
    "                \"max_time\": np.max(times),\n",
    "                \"std_time\": np.std(times),\n",
    "                \"avg_memory_mb\": np.mean(memory_usage) * 1024,\n",
    "                \"max_memory_mb\": np.max(memory_usage) * 1024\n",
    "            },\n",
    "            \"memory_info\": {\n",
    "                \"initial_allocated_gb\": initial_memory['allocated_gb'] if initial_memory else 0,\n",
    "                \"final_allocated_gb\": final_memory['allocated_gb'] if final_memory else 0,\n",
    "                \"memory_efficiency\": \"ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©\" if hasattr(self.model, 'hf_quantizer') else \"ì¼ë°˜ ëª¨ë¸\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ:\")\n",
    "        print(f\"  - í‰ê·  ì‹œê°„: {result['performance']['avg_time']:.3f}ì´ˆ\")\n",
    "        print(f\"  - í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {result['performance']['avg_memory_mb']:.1f}MB\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… ë©”ëª¨ë¦¬ ìµœì í™”ëœ KoreanTextCorrector í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f457f1",
   "metadata": {},
   "source": [
    "## ğŸš€ ì–‘ìí™” ëª¨ë¸ ì¶”ë¡ ê¸° ìƒì„± ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ec5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–‘ìí™”ëœ ëª¨ë¸ë¡œ ì¶”ë¡ ê¸° ìƒì„±\n",
    "print(\"ğŸ¤– ì–‘ìí™” í…ìŠ¤íŠ¸ êµì •ê¸° ìƒì„± ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ì˜ ëª¨ë¸ ë¡œë“œ ì…€ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    # ì¶”ë¡ ê¸° ìƒì„± ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "    print(\"\\nğŸ“Š ì¶”ë¡ ê¸° ìƒì„± ì „ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    pre_corrector_memory = monitor_gpu_memory()\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ êµì •ê¸° ìƒì„±\n",
    "    corrector = KoreanTextCorrector(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=256  # ì–‘ìí™” í™˜ê²½ì—ì„œ ì ì ˆí•œ ê¸¸ì´\n",
    "    )\n",
    "    \n",
    "    print(\"\\nğŸ“Š ì¶”ë¡ ê¸° ìƒì„± í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    post_corrector_memory = monitor_gpu_memory()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ ê³„ì‚°\n",
    "    if pre_corrector_memory and post_corrector_memory:\n",
    "        memory_increase = post_corrector_memory['allocated_gb'] - pre_corrector_memory['allocated_gb']\n",
    "        print(f\"\\nğŸ’¾ ì¶”ë¡ ê¸° ìƒì„±ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.3f}GB\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ ì–‘ìí™” í…ìŠ¤íŠ¸ êµì •ê¸° ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d769d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–‘ìí™” ìƒíƒœ ê²€ì¦\n",
    "print(\"ğŸ” ì–‘ìí™” ìƒíƒœ ê²€ì¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ëª¨ë¸ì˜ ì–‘ìí™” ìƒíƒœ í™•ì¸\n",
    "try:\n",
    "    # BitsAndBytesConfig ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    is_quantized = False\n",
    "    quantization_info = {}\n",
    "    \n",
    "    if hasattr(model, 'hf_quantizer'):\n",
    "        is_quantized = True\n",
    "        quantization_info['quantizer_type'] = type(model.hf_quantizer).__name__\n",
    "        if hasattr(model.hf_quantizer, 'quantization_config'):\n",
    "            config = model.hf_quantizer.quantization_config\n",
    "            quantization_info['load_in_4bit'] = getattr(config, 'load_in_4bit', False)\n",
    "            quantization_info['compute_dtype'] = getattr(config, 'bnb_4bit_compute_dtype', None)\n",
    "            quantization_info['quant_type'] = getattr(config, 'bnb_4bit_quant_type', None)\n",
    "    \n",
    "    # ë ˆì´ì–´ë³„ ì–‘ìí™” ìƒíƒœ í™•ì¸\n",
    "    quantized_layers = 0\n",
    "    total_layers = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            total_layers += 1\n",
    "            if hasattr(module.weight, 'dtype'):\n",
    "                if module.weight.dtype in [torch.uint8, torch.int8]:\n",
    "                    quantized_layers += 1\n",
    "    \n",
    "    print(f\"âœ… ì–‘ìí™” ìƒíƒœ: {'í™œì„±í™”' if is_quantized else 'ë¹„í™œì„±í™”'}\")\n",
    "    if quantization_info:\n",
    "        print(f\"ğŸ“Š ì–‘ìí™” ì •ë³´:\")\n",
    "        for key, value in quantization_info.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(f\"ğŸ”¢ ì–‘ìí™”ëœ ë ˆì´ì–´: {quantized_layers}/{total_layers}\")\n",
    "    \n",
    "    if quantized_layers > 0:\n",
    "        quantization_ratio = (quantized_layers / total_layers) * 100\n",
    "        print(f\"ğŸ“ˆ ì–‘ìí™” ë¹„ìœ¨: {quantization_ratio:.1f}%\")\n",
    "        \n",
    "        # ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½ëŸ‰ ê³„ì‚°\n",
    "        memory_saving = quantization_ratio * 0.75  # ëŒ€ëµì ì¸ ì ˆì•½ëŸ‰\n",
    "        print(f\"ğŸ’¾ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½: ~{memory_saving:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì–‘ìí™” ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d347ff",
   "metadata": {},
   "source": [
    "## ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    \"ì•ˆë…•í•˜ì…°ìš”\",              # ì•ˆë…•í•˜ì„¸ìš”\n",
    "    \"ê°ì‚¬í–ë‹ˆë‹¤\",              # ê°ì‚¬í•©ë‹ˆë‹¤\n",
    "    \"ì˜ ë«ƒê² ìŠµë‹ˆë‹¤\",           # ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤\n",
    "    \"ê´œì± ìŠµë‹ˆê¹Œ\",              # ê´œì°®ìŠµë‹ˆê¹Œ\n",
    "    \"ì´ê±° ì–´ë–»ê²Œ ìƒê°„í•˜ì„¸ìš”\",   # ì´ê±° ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì‹œê°€ ì¢‹ë„¤ìš”\",       # ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”\n",
    "    \"íšŒì˜ëŠ” ëª‡ì‹œì— ì‹œì‘í•´ë‚˜ìš”\",  # íšŒì˜ëŠ” ëª‡ì‹œì— ì‹œì‘í•©ë‹ˆê¹Œ\n",
    "    \"ì ì‹¬ ë©”ë‰´ëŠ” ë­ê°€ ì¡°ì„ê¹Œìš”\" # ì ì‹¬ ë©”ë‰´ëŠ” ë­ê°€ ì¢‹ì„ê¹Œìš”\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª ê¸°ë³¸ í…ìŠ¤íŠ¸ êµì • í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_text in enumerate(test_cases, 1):\n",
    "    start_time = time.time()\n",
    "    corrected = corrector.correct_text(test_text)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"{i:2d}. ì›ë³¸: {test_text}\")\n",
    "    print(f\"    êµì •: {corrected}\")\n",
    "    print(f\"    ì‹œê°„: {(end_time - start_time)*1000:.1f}ms\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a91c6",
   "metadata": {},
   "source": [
    "## âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "benchmark_texts = [\n",
    "    \"ì•ˆë…•í•˜ì…°ìš”\",\n",
    "    \"ì´ê±° ì •ë§ ë§›ìˆê²Œ ìƒê²¼ë„¤ìš”. ì–´ë””ì„œ ì‚´ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"íšŒì˜ëŠ” ë‚´ì¼ ì˜¤ì „ 10ì‹œì— ì‹œì‘í•  ì˜ˆì •ì…ë‹ˆë‹¤. ì°¸ì„ ê°€ëŠ¥í•˜ì‹ ì§€ í™•ì¸ ë¶€íƒë“œë ¤ìš”.\"\n",
    "]\n",
    "\n",
    "print(\"âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(benchmark_texts, 1):\n",
    "    print(f\"\\n{i}. ë²¤ì¹˜ë§ˆí¬ í…ìŠ¤íŠ¸: {text[:30]}...\")\n",
    "    \n",
    "    benchmark_result = corrector.benchmark_correction(text, num_runs=5)\n",
    "    \n",
    "    print(f\"   ì›ë³¸: {benchmark_result['original']}\")\n",
    "    print(f\"   êµì •: {benchmark_result['corrected']}\")\n",
    "    print(f\"   í‰ê·  ì‹œê°„: {benchmark_result['performance']['avg_time']*1000:.1f}ms\")\n",
    "    print(f\"   ìµœì†Œ ì‹œê°„: {benchmark_result['performance']['min_time']*1000:.1f}ms\")\n",
    "    print(f\"   ìµœëŒ€ ì‹œê°„: {benchmark_result['performance']['max_time']*1000:.1f}ms\")\n",
    "    print(f\"   í‘œì¤€í¸ì°¨: {benchmark_result['performance']['std_time']*1000:.1f}ms\")\n",
    "    print(f\"   í‰ê·  ë©”ëª¨ë¦¬: {benchmark_result['performance']['avg_memory_mb']:.1f}MB\")\n",
    "    print(f\"   ìµœëŒ€ ë©”ëª¨ë¦¬: {benchmark_result['performance']['max_memory_mb']:.1f}MB\")\n",
    "    print(f\"   íš¨ìœ¨ì„±: {benchmark_result['memory_info']['memory_efficiency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b45811",
   "metadata": {},
   "source": [
    "## ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "batch_texts = [\n",
    "    \"ì•ˆë…•í•˜ì…°ìš”\",\n",
    "    \"ê°ì‚¬í–ë‹ˆë‹¤\", \n",
    "    \"ì˜ ë«ƒê² ìŠµë‹ˆë‹¤\",\n",
    "    \"ê´œì± ìŠµë‹ˆê¹Œ\",\n",
    "    \"ì´ê±° ì–´ë–»ê²Œ ìƒê°„í•˜ì„¸ìš”\",\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì‹œê°€ ì¢‹ë„¤ìš”\",\n",
    "    \"íšŒì˜ëŠ” ëª‡ì‹œì— ì‹œì‘í•´ë‚˜ìš”\",\n",
    "    \"ì ì‹¬ ë©”ë‰´ëŠ” ë­ê°€ ì¡°ì„ê¹Œìš”\",\n",
    "    \"ì´ ë¬¸ì„œë¥¼ ê²€í† í•´ ì£¼ì„¸ìš”\",\n",
    "    \"í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì€ ì–´ë–¤ê°€ìš”\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ì–‘ìí™” ëª¨ë¸)\")\n",
    "print(f\"ì´ {len(batch_texts)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë°°ì¹˜ ì²˜ë¦¬ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "print(\"\\nğŸ” ë°°ì¹˜ ì²˜ë¦¬ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "pre_batch_memory = monitor_gpu_memory()\n",
    "\n",
    "start_time = time.time()\n",
    "batch_results = corrector.correct_batch(batch_texts, batch_size=1)  # ì–‘ìí™” í™˜ê²½ì—ì„œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°\n",
    "end_time = time.time()\n",
    "\n",
    "# ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "print(\"\\nğŸ” ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "post_batch_memory = monitor_gpu_memory()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_text = total_time / len(batch_texts)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "for i, (original, corrected) in enumerate(zip(batch_texts, batch_results), 1):\n",
    "    print(f\"{i:2d}. {original} â†’ {corrected}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„:\")\n",
    "print(f\"ì „ì²´ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {avg_time_per_text*1000:.1f}ms/í…ìŠ¤íŠ¸\")\n",
    "print(f\"ì²˜ë¦¬ ì†ë„: {len(batch_texts)/total_time:.1f}í…ìŠ¤íŠ¸/ì´ˆ\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„\n",
    "if pre_batch_memory and post_batch_memory:\n",
    "    memory_change = post_batch_memory['allocated_gb'] - pre_batch_memory['allocated_gb']\n",
    "    print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:\")\n",
    "    print(f\"ë©”ëª¨ë¦¬ ë³€í™”: {memory_change:+.3f}GB\")\n",
    "    print(f\"ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {max(pre_batch_memory['used_percent'], post_batch_memory['used_percent']):.1f}%\")\n",
    "    \n",
    "    if memory_change < 0.1:  # 100MB ë¯¸ë§Œ ì¦ê°€\n",
    "        print(\"âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ì„±ê³µ!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ê°ì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667015a",
   "metadata": {},
   "source": [
    "## ğŸ–¥ï¸ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_correct_text(text, num_beams, temperature, do_sample):\n",
    "    \"\"\"Gradioìš© í…ìŠ¤íŠ¸ êµì • í•¨ìˆ˜\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        corrected = corrector.correct_text(\n",
    "            text,\n",
    "            num_beams=int(num_beams),\n",
    "            temperature=float(temperature),\n",
    "            do_sample=bool(do_sample)\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = f\"ì²˜ë¦¬ ì‹œê°„: {(end_time - start_time)*1000:.1f}ms\"\n",
    "        \n",
    "        return corrected, processing_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\", \"\"\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "with gr.Blocks(title=\"í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì •ê¸°\") as demo:\n",
    "    gr.Markdown(\"# ğŸ‡°ğŸ‡· í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì •ê¸°\")\n",
    "    gr.Markdown(\"mT5 ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ë¡œ ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ êµì •í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(\n",
    "                label=\"êµì •í•  í…ìŠ¤íŠ¸\",\n",
    "                placeholder=\"êµì •í•˜ê³  ì‹¶ì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "                lines=5\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                num_beams = gr.Slider(\n",
    "                    minimum=1, maximum=8, value=4, step=1,\n",
    "                    label=\"Beam Search í¬ê¸°\"\n",
    "                )\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.1, maximum=2.0, value=0.7, step=0.1,\n",
    "                    label=\"Temperature\"\n",
    "                )\n",
    "                do_sample = gr.Checkbox(value=True, label=\"Sampling ì‚¬ìš©\")\n",
    "            \n",
    "            correct_btn = gr.Button(\"êµì •í•˜ê¸°\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(\n",
    "                label=\"êµì •ëœ í…ìŠ¤íŠ¸\",\n",
    "                lines=5\n",
    "            )\n",
    "            processing_info = gr.Textbox(\n",
    "                label=\"ì²˜ë¦¬ ì •ë³´\",\n",
    "                lines=1\n",
    "            )\n",
    "    \n",
    "    # ì˜ˆì‹œ í…ìŠ¤íŠ¸\n",
    "    gr.Markdown(\"### ğŸ“ ì˜ˆì‹œ í…ìŠ¤íŠ¸\")\n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            [\"ì•ˆë…•í•˜ì…°ìš”\"],\n",
    "            [\"ê°ì‚¬í–ë‹ˆë‹¤\"],\n",
    "            [\"ì˜ ë«ƒê² ìŠµë‹ˆë‹¤\"],\n",
    "            [\"ì´ê±° ì–´ë–»ê²Œ ìƒê°„í•˜ì„¸ìš”\"],\n",
    "            [\"ì˜¤ëŠ˜ ë‚ ì‹œê°€ ì •ë§ ì¢‹ë„¤ìš”\"]\n",
    "        ],\n",
    "        inputs=[input_text]\n",
    "    )\n",
    "    \n",
    "    # ì´ë²¤íŠ¸ ì—°ê²°\n",
    "    correct_btn.click(\n",
    "        fn=gradio_correct_text,\n",
    "        inputs=[input_text, num_beams, temperature, do_sample],\n",
    "        outputs=[output_text, processing_info]\n",
    "    )\n",
    "\n",
    "# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "print(\"ğŸ–¥ï¸ Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c3188",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ ëª¨ë¸ í‰ê°€ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e081f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ìš© ë°ì´í„° ë¡œë“œ (Google Driveì—ì„œ)\n",
    "def evaluate_on_test_data():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€\"\"\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì˜ˆì‹œ)\n",
    "    test_file_path = \"/content/drive/MyDrive/êµ¬ì–´ì²´_ëŒ€í™”ì²´_16878_sample_ë‚œë…í™”ê²°ê³¼.csv\"\n",
    "    \n",
    "    if not os.path.exists(test_file_path):\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    df = pd.read_csv(test_file_path, encoding='utf-8')\n",
    "    \n",
    "    # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸\n",
    "    test_df = df.sample(n=50, random_state=42)\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"ì´ {len(test_df)}ê°œ ìƒ˜í”Œ í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        original = row['original']\n",
    "        error_text = row['obfuscated']\n",
    "        \n",
    "        # ëª¨ë¸ ì˜ˆì¸¡\n",
    "        predicted = corrector.correct_text(error_text)\n",
    "        \n",
    "        predictions.append(predicted)\n",
    "        references.append(original)\n",
    "        \n",
    "        if len(predictions) % 10 == 0:\n",
    "            print(f\"ì§„í–‰ë¥ : {len(predictions)}/{len(test_df)}\")\n",
    "    \n",
    "    # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_scores = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references\n",
    "    )\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ í‰ê°€ ê²°ê³¼:\")\n",
    "    print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "    \n",
    "    # ëª‡ ê°€ì§€ ì˜ˆì‹œ ì¶œë ¥\n",
    "    print(\"\\nğŸ“ ì˜ˆì‹œ ê²°ê³¼:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"\\n{i+1}.\")\n",
    "        print(f\"ì˜¤ë¥˜ë¬¸: {test_df.iloc[i]['obfuscated']}\")\n",
    "        print(f\"ì •ë‹µ: {references[i]}\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[i]}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰ (ì„ íƒì‚¬í•­)\n",
    "# evaluate_on_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cee4e",
   "metadata": {},
   "source": [
    "## ğŸ§¹ ì •ë¦¬\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ ì–‘ìí™” ëª¨ë¸ ìµœì¢… í…ŒìŠ¤íŠ¸\n",
    "\n",
    "print(\"ğŸ”¬ ì–‘ìí™” ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ë‹¨ì¼ í…ìŠ¤íŠ¸ êµì • í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n1ï¸âƒ£ ë‹¨ì¼ í…ìŠ¤íŠ¸ êµì • í…ŒìŠ¤íŠ¸:\")\n",
    "test_text = \"ì•ˆë…•í•˜ì…°ìš”! ì˜¤ëŠ˜ ë‚ ì‹œê°€ ì •ë§ ì¢‹ë„¤ìš”.\"\n",
    "result = corrector.correct_text(test_text)\n",
    "print(f\"ì›ë³¸: {test_text}\")\n",
    "print(f\"êµì •: {result}\")\n",
    "\n",
    "# 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n2ï¸âƒ£ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸:\")\n",
    "pre_memory = monitor_gpu_memory()\n",
    "\n",
    "# ì—¬ëŸ¬ ë²ˆì˜ ì¶”ë¡ ìœ¼ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸\n",
    "for i in range(5):\n",
    "    _ = corrector.correct_text(f\"í…ŒìŠ¤íŠ¸ {i+1}: ì•ˆë…•í•˜ì…°ìš”\")\n",
    "    \n",
    "print(\"\\n5íšŒ ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "post_memory = monitor_gpu_memory()\n",
    "\n",
    "if pre_memory and post_memory:\n",
    "    memory_change = post_memory['allocated_gb'] - pre_memory['allocated_gb']\n",
    "    print(f\"\\në©”ëª¨ë¦¬ ë³€í™”: {memory_change:+.3f}GB\")\n",
    "    if abs(memory_change) < 0.1:\n",
    "        print(\"âœ… ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ - ì•ˆì •ì !\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™” ê°ì§€\")\n",
    "\n",
    "# 3. ì–‘ìí™” ìƒíƒœ ìµœì¢… í™•ì¸\n",
    "print(\"\\n3ï¸âƒ£ ì–‘ìí™” ìƒíƒœ ìµœì¢… í™•ì¸:\")\n",
    "is_quantized = hasattr(model, 'hf_quantizer')\n",
    "print(f\"ì–‘ìí™” í™œì„±í™”: {'âœ… ì˜ˆ' if is_quantized else 'âŒ ì•„ë‹ˆì˜¤'}\")\n",
    "\n",
    "if is_quantized:\n",
    "    print(\"ğŸ‰ 4-bit ì–‘ìí™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"ğŸ’¾ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½: ~75%\")\n",
    "    print(\"âš¡ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ”§ Fallback ëª¨ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸŠ ì–‘ìí™” ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ì´ì œ ë©”ëª¨ë¦¬ ë¶€ì¡± ì—†ì´ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717b481",
   "metadata": {},
   "source": [
    "## ğŸ“– ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "### ğŸš€ ë¹ ë¥¸ ì‹œì‘:\n",
    "\n",
    "**1. ë‹¨ì¼ í…ìŠ¤íŠ¸ êµì •:**\n",
    "```python\n",
    "result = corrector.correct_text(\"ì•ˆë…•í•˜ì…°ìš”\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**2. ë°°ì¹˜ í…ìŠ¤íŠ¸ êµì •:**\n",
    "```python\n",
    "texts = [\"ì•ˆë…•í•˜ì…°ìš”\", \"ê°ì‚¬í–ë‹ˆë‹¤\", \"ì˜ ë«ƒê² ìŠµë‹ˆë‹¤\"]\n",
    "results = corrector.correct_batch(texts, batch_size=1)\n",
    "for original, corrected in zip(texts, results):\n",
    "    print(f\"{original} â†’ {corrected}\")\n",
    "```\n",
    "\n",
    "**3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:**\n",
    "```python\n",
    "benchmark = corrector.benchmark_correction(\"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\", num_runs=3)\n",
    "print(f\"í‰ê·  ì‹œê°„: {benchmark['performance']['avg_time']*1000:.1f}ms\")\n",
    "```\n",
    "\n",
    "**4. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§:**\n",
    "```python\n",
    "monitor_gpu_memory()  # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "clear_gpu_memory()    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "```\n",
    "\n",
    "### ğŸ”§ ë¬¸ì œ í•´ê²°:\n",
    "\n",
    "**ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:**\n",
    "1. `clear_gpu_memory()` ì‹¤í–‰\n",
    "2. ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ê°ì†Œ\n",
    "3. í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶•ì†Œ (256ì ì´í•˜ ê¶Œì¥)\n",
    "\n",
    "**ì„±ëŠ¥ í–¥ìƒ íŒ:**\n",
    "- Beam search í¬ê¸°: 1-2 (ê¸°ë³¸ê°’: 2)\n",
    "- Temperature: 0.7 (ê¸°ë³¸ê°’)\n",
    "- ë°°ì¹˜ í¬ê¸°: 1 (ì–‘ìí™” í™˜ê²½ ê¶Œì¥)\n",
    "\n",
    "### ğŸ¯ ì–‘ìí™” íš¨ê³¼:\n",
    "- âœ… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~75% ê°ì†Œ\n",
    "- âœ… OutOfMemoryError í•´ê²°\n",
    "- âœ… ì•ˆì •ì ì¸ ì¥ì‹œê°„ ì‚¬ìš© ê°€ëŠ¥\n",
    "- âš ï¸ ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥ (ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë¯¸í•¨)\n",
    "\n",
    "ì´ì œ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì •**ì„ ì¦ê¸°ì„¸ìš”! ğŸ‰\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
