{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a36b6b",
   "metadata": {},
   "source": [
    "# 🇰🇷 한국어 텍스트 교정 모델 파인튜닝 (Google Colab T4)\n",
    "\n",
    "Google Colab T4 GPU를 사용한 한국어 텍스트 교정 모델 훈련\n",
    "- 모델: mT5-small\n",
    "- 기법: LoRA (Parameter-Efficient Fine-tuning)\n",
    "- 데이터: 구글 드라이브의 CSV 파일들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546008a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 최적화 설정 (반드시 최상단에 실행)\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(\"⚙️ 메모리 최적화 설정 완료\")\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True,max_split_size_mb:512\")\n",
    "print(\"TOKENIZERS_PARALLELISM: false\")\n",
    "print(\"CUDA_LAUNCH_BLOCKING: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252f51e",
   "metadata": {},
   "source": [
    "## 🚀 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df015baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 관리 및 모니터링 함수들\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"GPU 메모리 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"GPU 메모리 사용량 모니터링\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used_percent = (allocated / total) * 100\n",
    "        \n",
    "        print(f\"🔍 GPU 메모리 상태:\")\n",
    "        print(f\"  - 할당됨: {allocated:.2f}GB ({used_percent:.1f}%)\")\n",
    "        print(f\"  - 예약됨: {reserved:.2f}GB\")\n",
    "        print(f\"  - 사용가능: {free:.2f}GB\")\n",
    "        print(f\"  - 전체: {total:.2f}GB\")\n",
    "        \n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'free_gb': free,\n",
    "            'total_gb': total,\n",
    "            'used_percent': used_percent\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def get_optimal_batch_size(base_batch_size=8):\n",
    "    \"\"\"메모리 상황에 따른 최적 배치 사이즈 계산\"\"\"\n",
    "    memory_info = monitor_gpu_memory()\n",
    "    if memory_info:\n",
    "        if memory_info['used_percent'] > 80:\n",
    "            return max(1, base_batch_size // 4)\n",
    "        elif memory_info['used_percent'] > 60:\n",
    "            return max(2, base_batch_size // 2)\n",
    "        else:\n",
    "            return base_batch_size\n",
    "    return base_batch_size\n",
    "\n",
    "# 초기 메모리 정리\n",
    "clear_gpu_memory()\n",
    "print(\"✅ 메모리 관리 함수들 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi\n",
    "print(\"\\nPyTorch CUDA 지원 여부:\", torch.cuda.is_available() if 'torch' in globals() else 'torch not imported yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 작업 디렉토리 설정\n",
    "import os\n",
    "os.chdir('/content')\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치 (양자화 지원 포함)\n",
    "!pip install -q transformers datasets peft trl accelerate\n",
    "!pip install -q evaluate rouge-score sacrebleu scikit-learn\n",
    "!pip install -q gradio sentencepiece protobuf\n",
    "!pip install -q bitsandbytes  # 양자화를 위한 패키지\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"✅ 패키지 설치 완료 (양자화 지원 포함)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 효율성을 위한 환경 변수 설정\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig  # 양자화 지원\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# GPU 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    # 메모리 사용량 출력\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "    print(f\"할당된 GPU 메모리: {allocated:.2f}GB\")\n",
    "    print(f\"예약된 GPU 메모리: {reserved:.2f}GB\")\n",
    "    print(f\"사용 가능한 GPU 메모리: {free:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b381f88",
   "metadata": {},
   "source": [
    "## ⚙️ 설정 및 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654582a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 훈련 설정 (T4 GPU 양자화 최적화)\n",
    "CONFIG = {\n",
    "    \"model_name\": \"google/mt5-small\",\n",
    "    \"max_length\": 256,\n",
    "    \"batch_size\": 4,  # 양자화 환경에서 안전한 크기\n",
    "    \"gradient_accumulation_steps\": 8,  # 유효 배치 사이즈 32 유지\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"output_dir\": \"./korean-text-correction-colab\",\n",
    "}\n",
    "\n",
    "# LoRA 설정 (양자화 환경 최적화)\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
    "}\n",
    "\n",
    "# 데이터 파일 경로 (Google Drive)\n",
    "DATA_FILES = {\n",
    "    \"구어체\": \"/content/drive/MyDrive/구어체_대화체_16878_sample_난독화결과.csv\",\n",
    "    \"뉴스\": \"/content/drive/MyDrive/뉴스문어체_281932_sample_난독화결과.csv\",\n",
    "    \"문화\": \"/content/drive/MyDrive/문화문어체_25628_sample_난독화결과.csv\",\n",
    "    \"전문분야\": \"/content/drive/MyDrive/전문분야 문어체_306542_sample_난독화결과.csv\",\n",
    "    \"조례\": \"/content/drive/MyDrive/조례문어체_36339_sample_난독화결과.csv\",\n",
    "    \"지자체웹사이트\": \"/content/drive/MyDrive/지자체웹사이트 문어체_28705_sample_난독화결과.csv\"\n",
    "}\n",
    "\n",
    "# 메모리 상황에 따른 동적 배치 사이즈 조정\n",
    "optimal_batch_size = get_optimal_batch_size(CONFIG['batch_size'])\n",
    "if optimal_batch_size != CONFIG['batch_size']:\n",
    "    print(f\"⚠️ 메모리 부족으로 배치 사이즈 조정: {CONFIG['batch_size']} → {optimal_batch_size}\")\n",
    "    CONFIG['batch_size'] = optimal_batch_size\n",
    "    # Gradient accumulation 조정으로 유효 배치 사이즈 유지\n",
    "    CONFIG['gradient_accumulation_steps'] = max(1, 32 // optimal_batch_size)\n",
    "\n",
    "print(\"✅ 설정 완료 (양자화 최적화)\")\n",
    "print(f\"출력 디렉토리: {CONFIG['output_dir']}\")\n",
    "print(f\"배치 사이즈: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient accumulation steps: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"유효 배치 사이즈: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9a9a6",
   "metadata": {},
   "source": [
    "## 📊 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8854d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabDataPreprocessor:\n",
    "    \"\"\"Google Colab용 데이터 전처리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 256):\n",
    "        self.max_length = max_length\n",
    "        self.text_cleaning_patterns = [\n",
    "            (r'\\s+', ' '),  # 연속된 공백 정리\n",
    "            (r'^\\s+|\\s+$', ''),  # 앞뒤 공백 제거\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"텍스트 정리\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        text = str(text)\n",
    "        for pattern, replacement in self.text_cleaning_patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def load_and_process_csv(self, file_path: str, sample_size: int = 5000) -> pd.DataFrame:\n",
    "        \"\"\"CSV 파일 로드 및 전처리\"\"\"\n",
    "        print(f\"로딩 중: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            # CSV 파일 로드\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            print(f\"원본 데이터 크기: {len(df)}\")\n",
    "            print(f\"컬럼명: {df.columns.tolist()}\")\n",
    "            \n",
    "            # 필요한 컬럼 확인 및 매핑\n",
    "            if 'original' not in df.columns or 'obfuscated' not in df.columns:\n",
    "                # 한국어 컬럼명이 있는지 확인\n",
    "                if '원문' in df.columns and '오류문' in df.columns:\n",
    "                    df = df.rename(columns={'원문': 'original', '오류문': 'obfuscated'})\n",
    "                    print(\"✅ 한국어 컬럼명을 영어로 변환했습니다.\")\n",
    "                else:\n",
    "                    print(f\"⚠️ 필요한 컬럼이 없습니다: {df.columns.tolist()}\")\n",
    "                    print(\"필요한 컬럼: 'original', 'obfuscated' 또는 '원문', '오류문'\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            # 텍스트 정리\n",
    "            df['original'] = df['original'].apply(self.clean_text)\n",
    "            df['obfuscated'] = df['obfuscated'].apply(self.clean_text)\n",
    "            \n",
    "            # 빈 텍스트 제거\n",
    "            df = df[(df['original'] != '') & (df['obfuscated'] != '')]\n",
    "            \n",
    "            # 길이 제한\n",
    "            df = df[\n",
    "                (df['original'].str.len() <= self.max_length) & \n",
    "                (df['obfuscated'].str.len() <= self.max_length)\n",
    "            ]\n",
    "            \n",
    "            # 샘플링\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"전처리 후 데이터 크기: {len(df)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 로딩 오류: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_all_data(self, data_files: Dict[str, str], sample_per_file: int = 3000) -> pd.DataFrame:\n",
    "        \"\"\"모든 데이터 파일 로드 및 통합\"\"\"\n",
    "        all_dfs = []\n",
    "        \n",
    "        for name, file_path in data_files.items():\n",
    "            df = self.load_and_process_csv(file_path, sample_per_file)\n",
    "            if not df.empty:\n",
    "                df['source'] = name\n",
    "                all_dfs.append(df)\n",
    "        \n",
    "        if not all_dfs:\n",
    "            raise ValueError(\"로드된 데이터가 없습니다.\")\n",
    "        \n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n📊 통합 데이터셋 정보:\")\n",
    "        print(f\"전체 샘플 수: {len(combined_df)}\")\n",
    "        print(f\"소스별 분포:\")\n",
    "        print(combined_df['source'].value_counts())\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "print(\"✅ 데이터 전처리 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 전처리 (양자화 환경 최적화)\n",
    "preprocessor = ColabDataPreprocessor(max_length=CONFIG['max_length'])\n",
    "\n",
    "# 메모리 상태 확인 대이터 로드\n",
    "print(\"🔍 데이터 로드 전 메모리 상태:\")\n",
    "memory_info = monitor_gpu_memory()\n",
    "\n",
    "# 메모리 사용량에 따른 샘플 사이즈 동적 조정\n",
    "if memory_info and memory_info['used_percent'] > 70:\n",
    "    sample_per_file = 1500  # 메모리 부족 시 작은 샘플\n",
    "    print(f\"⚠️ 메모리 사용량 높음 ({memory_info['used_percent']:.1f}%), 샘플 사이즈 감소: {sample_per_file}\")\n",
    "else:\n",
    "    sample_per_file = 2000  # 정상 샘플 사이즈\n",
    "    print(f\"✅ 메모리 상태 양호, 기본 샘플 사이즈 사용: {sample_per_file}\")\n",
    "\n",
    "# 데이터 로드 시작\n",
    "df = preprocessor.load_all_data(DATA_FILES, sample_per_file=sample_per_file)\n",
    "\n",
    "# 훈련/검증 분할\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "print(f\"\\n📊 데이터 분할 (양자화 최적화):\")\n",
    "print(f\"훈련 데이터: {len(train_df)}\")\n",
    "print(f\"검증 데이터: {len(val_df)}\")\n",
    "\n",
    "# 데이터 로드 후 메모리 상태 확인\n",
    "print(f\"\\n🔍 데이터 로드 후 메모리 상태:\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n📝 샘플 데이터:\")\n",
    "for i in range(3):\n",
    "    print(f\"오류문 (obfuscated): {train_df.iloc[i]['obfuscated']}\")\n",
    "    print(f\"원문 (original): {train_df.iloc[i]['original']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 메모리 정리\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149a83",
   "metadata": {},
   "source": [
    "## 🤖 모델 설정 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화 설정\n",
    "print(\"⚙️ 양자화 설정 준비...\")\n",
    "\n",
    "# 4-bit 양자화 설정 (메모리 사용량 ~75% 감소)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"📊 양자화 설정:\")\n",
    "print(f\"- 4-bit 양자화: {quantization_config.load_in_4bit}\")\n",
    "print(f\"- 계산 타입: {quantization_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"- 양자화 타입: {quantization_config.bnb_4bit_quant_type}\")\n",
    "print(f\"- 이중 양자화: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"\\n🔤 토크나이저 로딩...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], use_fast=False)\n",
    "\n",
    "# 베이스 모델을 4-bit 양자화로 로드\n",
    "print(\"🤖 모델 로딩 (4-bit 양자화)...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 양자화된 모델 로드 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 양자화 로드 실패, 기본 모드로 fallback: {e}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "# 모델을 kbit 훈련용으로 준비\n",
    "print(\"🔧 kbit 훈련용 모델 준비...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 설정\n",
    "print(\"🔧 LoRA 설정 적용...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=LORA_CONFIG['r'],\n",
    "    lora_alpha=LORA_CONFIG['lora_alpha'],\n",
    "    lora_dropout=LORA_CONFIG['lora_dropout'],\n",
    "    target_modules=LORA_CONFIG['target_modules']\n",
    ")\n",
    "\n",
    "# PEFT 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 훈련 가능한 파라미터 확인\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 메모리 사용량 출력\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\n📊 모델 로드 후 메모리 사용량:\")\n",
    "    print(f\"- 할당된 GPU 메모리: {allocated:.2f}GB\")\n",
    "    print(f\"- 예약된 GPU 메모리: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"\\n✅ 양자화 모델 설정 완료!\")\n",
    "print(f\"모델 파라미터 수: {model.num_parameters():,}\")\n",
    "\n",
    "# 메모리 정리\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7b66",
   "metadata": {},
   "source": [
    "## 🔤 토크나이제이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ba557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"토크나이제이션 함수\"\"\"\n",
    "    # 입력: 오류문(obfuscated), 출력: 원문(original)\n",
    "    inputs = [f\"교정: {text}\" for text in examples['obfuscated']]\n",
    "    targets = examples['original']\n",
    "    \n",
    "    # 입력 토크나이제이션\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 타겟 토크나이제이션\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=CONFIG['max_length'],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# 토크나이제이션 적용\n",
    "print(\"🔤 토크나이제이션 진행...\")\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"✅ 토크나이제이션 완료\")\n",
    "print(f\"훈련 토큰화 데이터: {len(train_tokenized)}\")\n",
    "print(f\"검증 토큰화 데이터: {len(val_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ce1f",
   "metadata": {},
   "source": [
    "## 🚀 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0051463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭 로드\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"평가 메트릭 계산\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 라벨에서 -100을 제거\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "# 데이터 콜레이터\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"✅ 훈련 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c64981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 인수 설정 (양자화 환경 최적화)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,  # wandb 등 비활성화\n",
    "    dataloader_pin_memory=False,  # 메모리 효율성을 위해 비활성화\n",
    "    fp16=True,  # T4 GPU에서 메모리 효율성을 위해 mixed precision 사용\n",
    "    gradient_checkpointing=True,  # 메모리 절약을 위한 gradient checkpointing 활성화\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=1.0,  # Gradient clipping으로 안정성 향상\n",
    "    save_total_limit=2,  # 디스크 공간 절약\n",
    "    optim=\"adamw_torch\",  # 메모리 효율적인 optimizer\n",
    "    dataloader_num_workers=0,  # 메모리 안전성을 위해 비활성화\n",
    ")\n",
    "\n",
    "print(\"📋 훈련 인수 (양자화 최적화):\")\n",
    "print(f\"에포크: {CONFIG['num_epochs']}\")\n",
    "print(f\"배치 사이즈: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"유효 배치 사이즈: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"학습률: {CONFIG['learning_rate']}\")\n",
    "print(f\"출력 디렉토리: {CONFIG['output_dir']}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "print(f\"Gradient Checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이너 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✅ 트레이너 설정 완료\")\n",
    "print(\"훈련을 시작합니다...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작 (양자화 환경 최적화)\n",
    "print(\"🚀 훈련 시작! (양자화 모델)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 훈련 전 메모리 상태 확인\n",
    "print(\"🔍 훈련 시작 전 메모리 상태:\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n✅ 훈련 완료!\")\n",
    "    \n",
    "    # 마지막 메모리 상태 확인\n",
    "    print(\"\\n🔍 훈련 완료 후 메모리 상태:\")\n",
    "    monitor_gpu_memory()\n",
    "    \n",
    "    # 최종 모델 저장\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "    \n",
    "    print(f\"\\n📁 모델 저장 완료: {CONFIG['output_dir']}\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\n⚠️ GPU 메모리 부족 오류: {e}\")\n",
    "    print(\"🔧 메모리 정리 후 더 작은 배치 사이즈로 재시도해주세요.\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # 더 작은 배치 사이즈로 재시도\n",
    "    reduced_batch_size = max(1, CONFIG['batch_size'] // 2)\n",
    "    print(f\"🔄 배치 사이즈를 {CONFIG['batch_size']}에서 {reduced_batch_size}로 감소하여 재시도\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 훈련 중 오류 발생: {e}\")\n",
    "    print(\"🔍 오류 세부 사항:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # 메모리 정리\n",
    "    clear_gpu_memory()\n",
    "    print(\"🧙 메모리 정리 완료\")\n",
    "\n",
    "finally:\n",
    "    # 항상 마지막에 메모리 정리\n",
    "    clear_gpu_memory()\n",
    "    print(\"🏁 훈련 세션 종료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c199b",
   "metadata": {},
   "source": [
    "## 📊 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 평가\n",
    "print(\"📊 최종 모델 평가\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n📈 평가 결과:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e96dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 추론 테스트\n",
    "def test_correction(text: str) -> str:\n",
    "    \"\"\"텍스트 교정 테스트\"\"\"\n",
    "    input_text = f\"교정: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected\n",
    "\n",
    "# 테스트 케이스\n",
    "test_cases = [\n",
    "    \"안녕하셰요\",\n",
    "    \"감사햐니다\", \n",
    "    \"잘 뫃겠습니다\",\n",
    "    \"괜챠습니까\",\n",
    "    \"이거 어떻게 생간하세요\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 텍스트 교정 테스트:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    corrected = test_correction(test_text)\n",
    "    print(f\"원본: {test_text}\")\n",
    "    print(f\"교정: {corrected}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42634b2",
   "metadata": {},
   "source": [
    "## 💾 Google Drive에 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive에 모델 백업 (양자화 정보 포함)\n",
    "drive_save_path = \"/content/drive/MyDrive/korean-text-correction-model\"\n",
    "\n",
    "print(f\"📁 Google Drive에 모델 저장: {drive_save_path}\")\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "# 양자화 정보 저장\n",
    "quantization_info = {\n",
    "    \"quantization_type\": \"4-bit\",\n",
    "    \"compute_dtype\": \"float16\",\n",
    "    \"quant_type\": \"nf4\",\n",
    "    \"double_quant\": True,\n",
    "    \"lora_config\": LORA_CONFIG,\n",
    "    \"training_config\": CONFIG\n",
    "}\n",
    "\n",
    "# 양자화 정보를 JSON 파일로 저장\n",
    "import json\n",
    "with open(f\"{drive_save_path}/quantization_info.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(quantization_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    # 모델 파일 복사 (안전하게)\n",
    "    !cp -r {CONFIG['output_dir']}/* {drive_save_path}/\n",
    "    \n",
    "    print(\"✅ Google Drive 저장 완료\")\n",
    "    print(f\"저장된 파일들:\")\n",
    "    !ls -la {drive_save_path}\n",
    "    \n",
    "    # 파일 사이즈 체크\n",
    "    !du -sh {drive_save_path}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Google Drive 저장 오류: {e}\")\n",
    "    print(\"수동으로 파일을 복사해주세요.\")\n",
    "\n",
    "print(\"\\n🎆 양자화 기반 한국어 텍스트 교정 모델 훈련 완료!\")\n",
    "print(\"🚀 메모리 효율성이 대폭 향상되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd290f1",
   "metadata": {},
   "source": [
    "## 🧹 메모리 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2497aa",
   "metadata": {},
   "source": [
    "## 🎆 양자화 모델 훈련 요약\n",
    "\n",
    "### 📊 메모리 최적화 성과\n",
    "- **4-bit 양자화**: ~75% 메모리 절약\n",
    "- **LoRA 기법**: 파라미터 효율적 훈련\n",
    "- **Gradient Checkpointing**: 추가 메모리 절약\n",
    "- **동적 배치 사이즈**: 메모리 상황에 따른 자동 조정\n",
    "\n",
    "### 🚀 다음 단계\n",
    "1. **사용법**: `colab_inference.ipynb`로 모델 테스트\n",
    "2. **모델 위치**: `/content/drive/MyDrive/korean-text-correction-model`\n",
    "3. **양자화 정보**: `quantization_info.json` 파일 참조\n",
    "\n",
    "### ⚠️ 메모리 관리 팁\n",
    "- 훈련 중 OOM 오류 시 배치 사이즈 감소\n",
    "- `clear_gpu_memory()` 함수로 정기적 메모리 정리\n",
    "- `monitor_gpu_memory()`로 실시간 메모리 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 메모리 정리 및 운력 모니터링\n",
    "print(\"🗑️ 전체 메모리 정리 시작...\")\n",
    "\n",
    "# 마지막 메모리 상태 확인\n",
    "print(\"\\n🔍 정리 전 메모리 상태:\")\n",
    "final_memory_before = monitor_gpu_memory()\n",
    "\n",
    "# 모델과 트레이너 객체 삭제\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "\n",
    "# 전체 메모리 정리\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"\\n🔍 정리 후 메모리 상태:\")\n",
    "final_memory_after = monitor_gpu_memory()\n",
    "\n",
    "# 메모리 절약 효과 계산\n",
    "if final_memory_before and final_memory_after:\n",
    "    memory_freed = final_memory_before['allocated_gb'] - final_memory_after['allocated_gb']\n",
    "    print(f\"\\n💾 메모리 절약 효과: {memory_freed:.2f}GB 해제\")\n",
    "\n",
    "print(\"\\n✅ 메모리 정리 완료\")\n",
    "print(\"\\n🎉 훈련이 성공적으로 완료되었습니다!\")\n",
    "print(f\"📁 훈련된 모델은 다음 위치에 저장되었습니다:\")\n",
    "print(f\"- Colab: {CONFIG['output_dir']}\")\n",
    "print(f\"- Google Drive: /content/drive/MyDrive/korean-text-correction-model\")\n",
    "print(\"\\n🚀 이제 colab_inference.ipynb를 사용하여 모델을 테스트해보세요!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
