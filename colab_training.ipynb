{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a36b6b",
   "metadata": {},
   "source": [
    "# ğŸ‡°ğŸ‡· í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ íŒŒì¸íŠœë‹ (Google Colab T4)\n",
    "\n",
    "Google Colab T4 GPUë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ í›ˆë ¨\n",
    "- ëª¨ë¸: mT5-small\n",
    "- ê¸°ë²•: LoRA (Parameter-Efficient Fine-tuning)\n",
    "- ë°ì´í„°: êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ CSV íŒŒì¼ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546008a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ë°˜ë“œì‹œ ìµœìƒë‹¨ì— ì‹¤í–‰)\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(\"âš™ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True,max_split_size_mb:512\")\n",
    "print(\"TOKENIZERS_PARALLELISM: false\")\n",
    "print(\"CUDA_LAUNCH_BLOCKING: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252f51e",
   "metadata": {},
   "source": [
    "## ğŸš€ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df015baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ë“¤\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used_percent = (allocated / total) * 100\n",
    "        \n",
    "        print(f\"ğŸ” GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "        print(f\"  - í• ë‹¹ë¨: {allocated:.2f}GB ({used_percent:.1f}%)\")\n",
    "        print(f\"  - ì˜ˆì•½ë¨: {reserved:.2f}GB\")\n",
    "        print(f\"  - ì‚¬ìš©ê°€ëŠ¥: {free:.2f}GB\")\n",
    "        print(f\"  - ì „ì²´: {total:.2f}GB\")\n",
    "        \n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'free_gb': free,\n",
    "            'total_gb': total,\n",
    "            'used_percent': used_percent\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def get_optimal_batch_size(base_batch_size=8):\n",
    "    \"\"\"ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°\"\"\"\n",
    "    memory_info = monitor_gpu_memory()\n",
    "    if memory_info:\n",
    "        if memory_info['used_percent'] > 80:\n",
    "            return max(1, base_batch_size // 4)\n",
    "        elif memory_info['used_percent'] > 60:\n",
    "            return max(2, base_batch_size // 2)\n",
    "        else:\n",
    "            return base_batch_size\n",
    "    return base_batch_size\n",
    "\n",
    "# ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()\n",
    "print(\"âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi\n",
    "print(\"\\nPyTorch CUDA ì§€ì› ì—¬ë¶€:\", torch.cuda.is_available() if 'torch' in globals() else 'torch not imported yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "import os\n",
    "os.chdir('/content')\n",
    "print(f\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì–‘ìí™” ì§€ì› í¬í•¨)\n",
    "!pip install -q transformers datasets peft trl accelerate\n",
    "!pip install -q evaluate rouge-score sacrebleu scikit-learn\n",
    "!pip install -q gradio sentencepiece protobuf\n",
    "!pip install -q bitsandbytes  # ì–‘ìí™”ë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ (ì–‘ìí™” ì§€ì› í¬í•¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig  # ì–‘ìí™” ì§€ì›\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "    print(f\"í• ë‹¹ëœ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB\")\n",
    "    print(f\"ì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬: {reserved:.2f}GB\")\n",
    "    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {free:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b381f88",
   "metadata": {},
   "source": [
    "## âš™ï¸ ì„¤ì • ë° êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654582a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì • (T4 GPU ì–‘ìí™” ìµœì í™”)\n",
    "CONFIG = {\n",
    "    \"model_name\": \"google/mt5-small\",\n",
    "    \"max_length\": 256,\n",
    "    \"batch_size\": 4,  # ì–‘ìí™” í™˜ê²½ì—ì„œ ì•ˆì „í•œ í¬ê¸°\n",
    "    \"gradient_accumulation_steps\": 8,  # ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 32 ìœ ì§€\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"output_dir\": \"./korean-text-correction-colab\",\n",
    "}\n",
    "\n",
    "# LoRA ì„¤ì • (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
    "}\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ (Google Drive)\n",
    "DATA_FILES = {\n",
    "    \"êµ¬ì–´ì²´\": \"/content/drive/MyDrive/êµ¬ì–´ì²´_ëŒ€í™”ì²´_16878_sample_ë‚œë…í™”ê²°ê³¼.csv\",\n",
    "    \"ë‰´ìŠ¤\": \"/content/drive/MyDrive/ë‰´ìŠ¤ë¬¸ì–´ì²´_281932_sample_ë‚œë…í™”ê²°ê³¼.csv\",\n",
    "    \"ë¬¸í™”\": \"/content/drive/MyDrive/ë¬¸í™”ë¬¸ì–´ì²´_25628_sample_ë‚œë…í™”ê²°ê³¼.csv\",\n",
    "    \"ì „ë¬¸ë¶„ì•¼\": \"/content/drive/MyDrive/ì „ë¬¸ë¶„ì•¼ ë¬¸ì–´ì²´_306542_sample_ë‚œë…í™”ê²°ê³¼.csv\",\n",
    "    \"ì¡°ë¡€\": \"/content/drive/MyDrive/ì¡°ë¡€ë¬¸ì–´ì²´_36339_sample_ë‚œë…í™”ê²°ê³¼.csv\",\n",
    "    \"ì§€ìì²´ì›¹ì‚¬ì´íŠ¸\": \"/content/drive/MyDrive/ì§€ìì²´ì›¹ì‚¬ì´íŠ¸ ë¬¸ì–´ì²´_28705_sample_ë‚œë…í™”ê²°ê³¼.csv\"\n",
    "}\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ë™ì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "optimal_batch_size = get_optimal_batch_size(CONFIG['batch_size'])\n",
    "if optimal_batch_size != CONFIG['batch_size']:\n",
    "    print(f\"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •: {CONFIG['batch_size']} â†’ {optimal_batch_size}\")\n",
    "    CONFIG['batch_size'] = optimal_batch_size\n",
    "    # Gradient accumulation ì¡°ì •ìœ¼ë¡œ ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìœ ì§€\n",
    "    CONFIG['gradient_accumulation_steps'] = max(1, 32 // optimal_batch_size)\n",
    "\n",
    "print(\"âœ… ì„¤ì • ì™„ë£Œ (ì–‘ìí™” ìµœì í™”)\")\n",
    "print(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬: {CONFIG['output_dir']}\")\n",
    "print(f\"ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient accumulation steps: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9a9a6",
   "metadata": {},
   "source": [
    "## ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8854d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabDataPreprocessor:\n",
    "    \"\"\"Google Colabìš© ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 256):\n",
    "        self.max_length = max_length\n",
    "        self.text_cleaning_patterns = [\n",
    "            (r'\\s+', ' '),  # ì—°ì†ëœ ê³µë°± ì •ë¦¬\n",
    "            (r'^\\s+|\\s+$', ''),  # ì•ë’¤ ê³µë°± ì œê±°\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ë¦¬\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        text = str(text)\n",
    "        for pattern, replacement in self.text_cleaning_patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def load_and_process_csv(self, file_path: str, sample_size: int = 5000) -> pd.DataFrame:\n",
    "        \"\"\"CSV íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(f\"ë¡œë”© ì¤‘: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            # CSV íŒŒì¼ ë¡œë“œ\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            print(f\"ì›ë³¸ ë°ì´í„° í¬ê¸°: {len(df)}\")\n",
    "            print(f\"ì»¬ëŸ¼ëª…: {df.columns.tolist()}\")\n",
    "            \n",
    "            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ë§¤í•‘\n",
    "            if 'original' not in df.columns or 'obfuscated' not in df.columns:\n",
    "                # í•œêµ­ì–´ ì»¬ëŸ¼ëª…ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "                if 'ì›ë¬¸' in df.columns and 'ì˜¤ë¥˜ë¬¸' in df.columns:\n",
    "                    df = df.rename(columns={'ì›ë¬¸': 'original', 'ì˜¤ë¥˜ë¬¸': 'obfuscated'})\n",
    "                    print(\"âœ… í•œêµ­ì–´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {df.columns.tolist()}\")\n",
    "                    print(\"í•„ìš”í•œ ì»¬ëŸ¼: 'original', 'obfuscated' ë˜ëŠ” 'ì›ë¬¸', 'ì˜¤ë¥˜ë¬¸'\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "            df['original'] = df['original'].apply(self.clean_text)\n",
    "            df['obfuscated'] = df['obfuscated'].apply(self.clean_text)\n",
    "            \n",
    "            # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°\n",
    "            df = df[(df['original'] != '') & (df['obfuscated'] != '')]\n",
    "            \n",
    "            # ê¸¸ì´ ì œí•œ\n",
    "            df = df[\n",
    "                (df['original'].str.len() <= self.max_length) & \n",
    "                (df['obfuscated'].str.len() <= self.max_length)\n",
    "            ]\n",
    "            \n",
    "            # ìƒ˜í”Œë§\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {len(df)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_all_data(self, data_files: Dict[str, str], sample_per_file: int = 3000) -> pd.DataFrame:\n",
    "        \"\"\"ëª¨ë“  ë°ì´í„° íŒŒì¼ ë¡œë“œ ë° í†µí•©\"\"\"\n",
    "        all_dfs = []\n",
    "        \n",
    "        for name, file_path in data_files.items():\n",
    "            df = self.load_and_process_csv(file_path, sample_per_file)\n",
    "            if not df.empty:\n",
    "                df['source'] = name\n",
    "                all_dfs.append(df)\n",
    "        \n",
    "        if not all_dfs:\n",
    "            raise ValueError(\"ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í†µí•© ë°ì´í„°ì…‹ ì •ë³´:\")\n",
    "        print(f\"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(combined_df)}\")\n",
    "        print(f\"ì†ŒìŠ¤ë³„ ë¶„í¬:\")\n",
    "        print(combined_df['source'].value_counts())\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\n",
    "preprocessor = ColabDataPreprocessor(max_length=CONFIG['max_length'])\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ëŒ€ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ” ë°ì´í„° ë¡œë“œ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "memory_info = monitor_gpu_memory()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ ë™ì  ì¡°ì •\n",
    "if memory_info and memory_info['used_percent'] > 70:\n",
    "    sample_per_file = 1500  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‘ì€ ìƒ˜í”Œ\n",
    "    print(f\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ ({memory_info['used_percent']:.1f}%), ìƒ˜í”Œ ì‚¬ì´ì¦ˆ ê°ì†Œ: {sample_per_file}\")\n",
    "else:\n",
    "    sample_per_file = 2000  # ì •ìƒ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ\n",
    "    print(f\"âœ… ë©”ëª¨ë¦¬ ìƒíƒœ ì–‘í˜¸, ê¸°ë³¸ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ ì‚¬ìš©: {sample_per_file}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ì‹œì‘\n",
    "df = preprocessor.load_all_data(DATA_FILES, sample_per_file=sample_per_file)\n",
    "\n",
    "# í›ˆë ¨/ê²€ì¦ ë¶„í• \n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ë¶„í•  (ì–‘ìí™” ìµœì í™”):\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_df)}\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_df)}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "print(f\"\\nğŸ” ë°ì´í„° ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nğŸ“ ìƒ˜í”Œ ë°ì´í„°:\")\n",
    "for i in range(3):\n",
    "    print(f\"ì˜¤ë¥˜ë¬¸ (obfuscated): {train_df.iloc[i]['obfuscated']}\")\n",
    "    print(f\"ì›ë¬¸ (original): {train_df.iloc[i]['original']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149a83",
   "metadata": {},
   "source": [
    "## ğŸ¤– ëª¨ë¸ ì„¤ì • ë° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit ì–‘ìí™” ì„¤ì •\n",
    "print(\"âš™ï¸ ì–‘ìí™” ì„¤ì • ì¤€ë¹„...\")\n",
    "\n",
    "# 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~75% ê°ì†Œ)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š ì–‘ìí™” ì„¤ì •:\")\n",
    "print(f\"- 4-bit ì–‘ìí™”: {quantization_config.load_in_4bit}\")\n",
    "print(f\"- ê³„ì‚° íƒ€ì…: {quantization_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"- ì–‘ìí™” íƒ€ì…: {quantization_config.bnb_4bit_quant_type}\")\n",
    "print(f\"- ì´ì¤‘ ì–‘ìí™”: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"\\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”©...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], use_fast=False)\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ì„ 4-bit ì–‘ìí™”ë¡œ ë¡œë“œ\n",
    "print(\"ğŸ¤– ëª¨ë¸ ë¡œë”© (4-bit ì–‘ìí™”)...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"âœ… ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì–‘ìí™” ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ fallback: {e}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "# ëª¨ë¸ì„ kbit í›ˆë ¨ìš©ìœ¼ë¡œ ì¤€ë¹„\n",
    "print(\"ğŸ”§ kbit í›ˆë ¨ìš© ëª¨ë¸ ì¤€ë¹„...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA ì„¤ì •\n",
    "print(\"ğŸ”§ LoRA ì„¤ì • ì ìš©...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=LORA_CONFIG['r'],\n",
    "    lora_alpha=LORA_CONFIG['lora_alpha'],\n",
    "    lora_dropout=LORA_CONFIG['lora_dropout'],\n",
    "    target_modules=LORA_CONFIG['target_modules']\n",
    ")\n",
    "\n",
    "# PEFT ëª¨ë¸ ìƒì„±\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nğŸ“Š ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\n",
    "    print(f\"- í• ë‹¹ëœ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB\")\n",
    "    print(f\"- ì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬: {reserved:.2f}GB\")\n",
    "\n",
    "print(\"\\nâœ… ì–‘ìí™” ëª¨ë¸ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7b66",
   "metadata": {},
   "source": [
    "## ğŸ”¤ í† í¬ë‚˜ì´ì œì´ì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ba557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"í† í¬ë‚˜ì´ì œì´ì…˜ í•¨ìˆ˜\"\"\"\n",
    "    # ì…ë ¥: ì˜¤ë¥˜ë¬¸(obfuscated), ì¶œë ¥: ì›ë¬¸(original)\n",
    "    inputs = [f\"êµì •: {text}\" for text in examples['obfuscated']]\n",
    "    targets = examples['original']\n",
    "    \n",
    "    # ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì œì´ì…˜\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=CONFIG['max_length'],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì œì´ì…˜ ì ìš©\n",
    "print(\"ğŸ”¤ í† í¬ë‚˜ì´ì œì´ì…˜ ì§„í–‰...\")\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ\")\n",
    "print(f\"í›ˆë ¨ í† í°í™” ë°ì´í„°: {len(train_tokenized)}\")\n",
    "print(f\"ê²€ì¦ í† í°í™” ë°ì´í„°: {len(val_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ce1f",
   "metadata": {},
   "source": [
    "## ğŸš€ ëª¨ë¸ í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0051463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íŠ¸ë¦­ ë¡œë“œ\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # ë¼ë²¨ì—ì„œ -100ì„ ì œê±°\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„°\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c64981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì¸ìˆ˜ ì„¤ì • (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,  # wandb ë“± ë¹„í™œì„±í™”\n",
    "    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”\n",
    "    fp16=True,  # T4 GPUì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ mixed precision ì‚¬ìš©\n",
    "    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ gradient checkpointing í™œì„±í™”\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=1.0,  # Gradient clippingìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
    "    save_total_limit=2,  # ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½\n",
    "    optim=\"adamw_torch\",  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ optimizer\n",
    "    dataloader_num_workers=0,  # ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‹ í›ˆë ¨ ì¸ìˆ˜ (ì–‘ìí™” ìµœì í™”):\")\n",
    "print(f\"ì—í¬í¬: {CONFIG['num_epochs']}\")\n",
    "print(f\"ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"í•™ìŠµë¥ : {CONFIG['learning_rate']}\")\n",
    "print(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬: {CONFIG['output_dir']}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "print(f\"Gradient Checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"âœ… íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì‹œì‘ (ì–‘ìí™” í™˜ê²½ ìµœì í™”)\n",
    "print(\"ğŸš€ í›ˆë ¨ ì‹œì‘! (ì–‘ìí™” ëª¨ë¸)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# í›ˆë ¨ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "print(\"ğŸ” í›ˆë ¨ ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "monitor_gpu_memory()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\nâœ… í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "    print(\"\\nğŸ” í›ˆë ¨ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "    monitor_gpu_memory()\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "    \n",
    "    print(f\"\\nğŸ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {CONFIG['output_dir']}\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\nâš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ”§ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ë” ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ì¬ì‹œë„í•´ì£¼ì„¸ìš”.\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # ë” ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ì¬ì‹œë„\n",
    "    reduced_batch_size = max(1, CONFIG['batch_size'] // 2)\n",
    "    print(f\"ğŸ”„ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ {CONFIG['batch_size']}ì—ì„œ {reduced_batch_size}ë¡œ ê°ì†Œí•˜ì—¬ ì¬ì‹œë„\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ğŸ” ì˜¤ë¥˜ ì„¸ë¶€ ì‚¬í•­:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    clear_gpu_memory()\n",
    "    print(\"ğŸ§™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "finally:\n",
    "    # í•­ìƒ ë§ˆì§€ë§‰ì— ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    clear_gpu_memory()\n",
    "    print(\"ğŸ í›ˆë ¨ ì„¸ì…˜ ì¢…ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c199b",
   "metadata": {},
   "source": [
    "## ğŸ“Š ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… í‰ê°€\n",
    "print(\"ğŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nğŸ“ˆ í‰ê°€ ê²°ê³¼:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e96dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "def test_correction(text: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ êµì • í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    input_text = f\"êµì •: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    \"ì•ˆë…•í•˜ì…°ìš”\",\n",
    "    \"ê°ì‚¬í–ë‹ˆë‹¤\", \n",
    "    \"ì˜ ë«ƒê² ìŠµë‹ˆë‹¤\",\n",
    "    \"ê´œì± ìŠµë‹ˆê¹Œ\",\n",
    "    \"ì´ê±° ì–´ë–»ê²Œ ìƒê°„í•˜ì„¸ìš”\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ§ª í…ìŠ¤íŠ¸ êµì • í…ŒìŠ¤íŠ¸:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    corrected = test_correction(test_text)\n",
    "    print(f\"ì›ë³¸: {test_text}\")\n",
    "    print(f\"êµì •: {corrected}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42634b2",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Google Driveì— ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveì— ëª¨ë¸ ë°±ì—… (ì–‘ìí™” ì •ë³´ í¬í•¨)\n",
    "drive_save_path = \"/content/drive/MyDrive/korean-text-correction-model\"\n",
    "\n",
    "print(f\"ğŸ“ Google Driveì— ëª¨ë¸ ì €ì¥: {drive_save_path}\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "# ì–‘ìí™” ì •ë³´ ì €ì¥\n",
    "quantization_info = {\n",
    "    \"quantization_type\": \"4-bit\",\n",
    "    \"compute_dtype\": \"float16\",\n",
    "    \"quant_type\": \"nf4\",\n",
    "    \"double_quant\": True,\n",
    "    \"lora_config\": LORA_CONFIG,\n",
    "    \"training_config\": CONFIG\n",
    "}\n",
    "\n",
    "# ì–‘ìí™” ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "import json\n",
    "with open(f\"{drive_save_path}/quantization_info.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(quantization_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    # ëª¨ë¸ íŒŒì¼ ë³µì‚¬ (ì•ˆì „í•˜ê²Œ)\n",
    "    !cp -r {CONFIG['output_dir']}/* {drive_save_path}/\n",
    "    \n",
    "    print(\"âœ… Google Drive ì €ì¥ ì™„ë£Œ\")\n",
    "    print(f\"ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "    !ls -la {drive_save_path}\n",
    "    \n",
    "    # íŒŒì¼ ì‚¬ì´ì¦ˆ ì²´í¬\n",
    "    !du -sh {drive_save_path}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Google Drive ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(\"\\nğŸ† ì–‘ìí™” ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(\"ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ëŒ€í­ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd290f1",
   "metadata": {},
   "source": [
    "## ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2497aa",
   "metadata": {},
   "source": [
    "## ğŸ† ì–‘ìí™” ëª¨ë¸ í›ˆë ¨ ìš”ì•½\n",
    "\n",
    "### ğŸ“Š ë©”ëª¨ë¦¬ ìµœì í™” ì„±ê³¼\n",
    "- **4-bit ì–‘ìí™”**: ~75% ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "- **LoRA ê¸°ë²•**: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  í›ˆë ¨\n",
    "- **Gradient Checkpointing**: ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "- **ë™ì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ**: ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ìë™ ì¡°ì •\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "1. **ì‚¬ìš©ë²•**: `colab_inference.ipynb`ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "2. **ëª¨ë¸ ìœ„ì¹˜**: `/content/drive/MyDrive/korean-text-correction-model`\n",
    "3. **ì–‘ìí™” ì •ë³´**: `quantization_info.json` íŒŒì¼ ì°¸ì¡°\n",
    "\n",
    "### âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ íŒ\n",
    "- í›ˆë ¨ ì¤‘ OOM ì˜¤ë¥˜ ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ\n",
    "- `clear_gpu_memory()` í•¨ìˆ˜ë¡œ ì •ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "- `monitor_gpu_memory()`ë¡œ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìš´ë ¥ ëª¨ë‹ˆí„°ë§\n",
    "print(\"ğŸ—‘ï¸ ì „ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# ë§ˆì§€ë§‰ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "print(\"\\nğŸ” ì •ë¦¬ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "final_memory_before = monitor_gpu_memory()\n",
    "\n",
    "# ëª¨ë¸ê³¼ íŠ¸ë ˆì´ë„ˆ ê°ì²´ ì‚­ì œ\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "\n",
    "# ì „ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"\\nğŸ” ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "final_memory_after = monitor_gpu_memory()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ ê³„ì‚°\n",
    "if final_memory_before and final_memory_after:\n",
    "    memory_freed = final_memory_before['allocated_gb'] - final_memory_after['allocated_gb']\n",
    "    print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼: {memory_freed:.2f}GB í•´ì œ\")\n",
    "\n",
    "print(\"\\nâœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "print(\"\\nğŸ‰ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ í›ˆë ¨ëœ ëª¨ë¸ì€ ë‹¤ìŒ ìœ„ì¹˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "print(f\"- Colab: {CONFIG['output_dir']}\")\n",
    "print(f\"- Google Drive: /content/drive/MyDrive/korean-text-correction-model\")\n",
    "print(\"\\nğŸš€ ì´ì œ colab_inference.ipynbë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
